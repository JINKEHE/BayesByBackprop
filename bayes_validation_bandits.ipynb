{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bayes_validation_bandits.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bxPeVnpksepA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f82b5e9a-b88f-48d4-a852-411c20658e3c"
      },
      "cell_type": "code",
      "source": [
        "from core import *\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data as Data\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "from bandits import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use GPU: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "appsi0oRMy5K",
        "colab_type": "code",
        "outputId": "b628b0c3-72c2-4ef7-a6d2-42b6213a958a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# hypers that do not need to be tuned\n",
        "N_Steps = 4000 # in actual training, we use 600\n",
        "\n",
        "# Load the UCI Mushroom Dataset: 8124 datapoints, each with 22 categorical\n",
        "# features and one label - edible/poisonous. The features are transformed to a\n",
        "# one-hot encoding. \n",
        "# The missing values (marked with ?) are treated as a different class for now.\n",
        "\n",
        "mushroom_dataset = pd.read_csv('mushrooms.csv')\n",
        "train_labels = mushroom_dataset['class']\n",
        "train_labels = train_labels.replace(['p', 'e'],\n",
        "                                    [POISONOUS_CONSTANT, EDIBLE_CONSTANT])\n",
        "train_features = pd.get_dummies(mushroom_dataset.drop(['class'], axis=1))\n",
        "\n",
        "train_features = torch.tensor(train_features.values, dtype=torch.float)\n",
        "train_labels = torch.tensor(train_labels.values)\n",
        "\n",
        "trainset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
        "                                          shuffle=True, num_workers=1)\n",
        "\n",
        "# TODO: cannot specify the hyper for priors outside the network\n",
        "\n",
        "# need N_epochs * 4 * 2 * 3 * 3 * 3 = 100 * 4 * 54= 21600 epochs = 648000 seconds = 180 hours\n",
        "N_Samples_Testing_candidates = [1,2,5,10]\n",
        "LearningRate_candidates = [1e-4, 1e-3]\n",
        "mixture_PI_candidates = [0.25, 0.5, 0.75]\n",
        "mixture_sigma1_candidates = [math.exp(-0), math.exp(-1), math.exp(-2)]\n",
        "mixture_sigma2_candidates = [math.exp(-6), math.exp(-7), math.exp(-8)]\n",
        "Epsilon_candidates = [0.1, 0.01, 0.001]\n",
        "\n",
        "hyper_val_error_dict = {}\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # could may have more\n",
        "    hyper_list = itertools.product(LearningRate_candidates, Epsilon_candidates)\n",
        "    \n",
        "    for LearningRate, epsilon in hyper_list:\n",
        "      \n",
        "      print(\"*\"*50)\n",
        "      \n",
        "      print(\"Learning rate: {}\".format(LearningRate))\n",
        "      print(\"Epsilon: {}\".format(epsilon))\n",
        "      \n",
        "      # Initialize network\n",
        "      optimizer_constructor = torch.optim.Adam\n",
        "      optimizer_params = {'lr': LearningRate, 'eps': epsilon}\n",
        "      \n",
        "      eg_agent = EGreedyNNAgent(epsilon=.05, \n",
        "                                optimizer_constructor=optimizer_constructor,\n",
        "                                optim_params=optimizer_params)\n",
        "      eg_env = Environment(eg_agent, trainloader)\n",
        "\n",
        "      loss = []\n",
        "      regret = []\n",
        "        \n",
        "\n",
        "      for i_step in range(N_Steps):\n",
        "\n",
        "          # Training\n",
        "          loss.append(eg_env.play_round())\n",
        "          regret.append(eg_env.cumulative_regret)\n",
        "          \n",
        "          if (i_step + 1) % 100 == 0:\n",
        "            print('Step {}. Regret {}'.format(i_step, eg_env.cumulative_regret))\n",
        "\n",
        "      plt.plot(np.array(loss))\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show()\n",
        "    \n",
        "      plt.plot(np.array(regret))\n",
        "      plt.ylabel('Cumulative Regret')\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Learning rate: 0.0001\n",
            "Epsilon: 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xsYlKmc8sV5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}