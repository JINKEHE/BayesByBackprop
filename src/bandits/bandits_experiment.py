# -*- coding: utf-8 -*-
"""Bandits.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ryV2BfuI6A-J3g7UzzGreDlS6onh3HPw
"""

import queue
import random
import torch
import copy
import os
import sys
import argparse
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch.utils.data
from torch import optim
from torch import nn
from torch import distributions as dist

sys.path.append(os.path.abspath('src'))
from core import *
from bandits import *

print(torch.__version__)

use_cuda = torch.cuda.is_available()

CONTEXT_SIZE = 117 + 1
SAMPLE_COUNT = 2
NUM_ACTIONS = 2
AGENT_MEMORY_LEN = 4096

EDIBLE_REWARD = 5.0
POISONOUS_REWARD = -35.0

EDIBLE_CONSTANT = 1.0
POISONOUS_CONSTANT = -1.0

"""The environment sends the agent the features of a mushroom.

The agent can pick one of two actions:
1. eat
2. pass

If the agent picks pass, the regret is always 0.

If the agent picks eat:
1. if the mushroom is edible, the reward is always +5.0.
2. if the mushroom is poisonous, with prob 1/2 the reward is -35.0, with prob 1/2 the reward is +5.0

The system is represented by two entities: an **agent** and an **environment**.  They interact in rounds, in the following manner:

1. The environment randomly selects a mushroom from the dataset and presents its features (the current context) to the agent.

2. The agent selects an action that it deems optimal given the context.

3. The environment computes the reward for the selected action and sends it back to the agent, which updates its predictions.
"""

def read_args(args=None):
  parser = argparse.ArgumentParser()
  parser.add_argument('--experiment_name', type=str, default='Experiment')
  parser.add_argument('--optimizer_type', type=str, default='Adam')
  parser.add_argument('--eg_learning_rate', type=float, default=1e-3)
  parser.add_argument('--eg_epsilon', type=float, default=1e-3)
  parser.add_argument('--eg_momentum', type=float, default=0)
  parser.add_argument('--bnn_learning_rate', type=float, default=1e-3)
  parser.add_argument('--bnn_epsilon', type=float, default=1e-3)
  parser.add_argument('--bnn_momentum', type=float, default=0)
  parser.add_argument('--bnn_lr_scheduler_step_size', type=int, default=32)
  parser.add_argument('--bnn_lr_scheduler_gamma', type=float, default=0.1)
  parser.add_argument('--bnn_pi', type=float, default=0.75)
  parser.add_argument('--bnn_log_sigma1', type=float, default=math.exp(-2))
  parser.add_argument('--bnn_log_sigma2', type=float, default=math.exp(-7))
  parser.add_argument('--averaged_weights', dest='averaged_weights', action='store_true')
  parser.add_argument('--avg_weights_count', type=int, default=2)
  parser.add_argument('--initial_mu_weights_range', type=float, nargs=2, default=[-0.5, 0.5])
  parser.add_argument('--initial_mu_bias_range', type=float, nargs=2, default=[-0.5, 0.5])
  parser.add_argument('--initial_rho_weights_range', type=float, nargs=2, default=[-4, -2])
  parser.add_argument('--initial_rho_bias_range', type=float, nargs=2, default=[-4, -2])
  parser.add_argument('--number_of_runs', type=int, default=1)
  args = parser.parse_args()
  return args
  

if __name__ == '__main__':

	# Load the UCI Mushroom Dataset: 8124 datapoints, each with 22 categorical
	# features and one label - edible/poisonous. The features are transformed to a
	# one-hot encoding. 
	# The missing values (marked with ?) are treated as a different class for now.

  mushroom_dataset = pd.read_csv('src/bandits/mushrooms.csv')
  train_labels = mushroom_dataset['class']
  train_labels = train_labels.replace(['p', 'e'],
                                [POISONOUS_CONSTANT, EDIBLE_CONSTANT])
  train_features = pd.get_dummies(mushroom_dataset.drop(['class'], axis=1))

  train_features = torch.tensor(train_features.values, dtype=torch.float)
  train_labels = torch.tensor(train_labels.values)

  trainset = torch.utils.data.TensorDataset(train_features, train_labels)
  trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,
                                            shuffle=True, num_workers=0)
    
	# Parameters for bnn and eg agents
  args = read_args(sys.argv)
  
  if args.optimizer_type == 'Adam':
    optimizer_constructor = torch.optim.Adam
    eg_optimizer_params = {'lr': args.eg_learning_rate,
                           'eps': args.eg_epsilon}
    bnn_optimizer_params = {'lr': args.bnn_learning_rate,
                            'eps': args.bnn_epsilon}
  elif args.optimizer_type == 'SGD':
    optimizer_constructor = torch.optim.SGD
    eg_optimizer_params = {'lr': args.eg_learning_rate,
                           'momentum': args.eg_momentum}
    bnn_optimizer_params = {'lr': args.bnn_learning_rate, 
                            'momentum': args.bnn_momentum}
  
  sigma1 = math.exp(args.bnn_log_sigma1)
  sigma2 = math.exp(args.bnn_log_sigma2)
  prior_params = {'pi': args.bnn_pi, 
                  'sigma1': sigma1,
                  'sigma2': sigma2}

  bnn_envs = []
  eg5_envs = []
  eg1_envs = []
  eg0_envs = []
  envs = []

  for j in range(args.number_of_runs):

    bnn_agent = BNNAgent(optimizer_constructor=optimizer_constructor,
                  optim_params=bnn_optimizer_params,
                  prior_params=prior_params,
                  lr_scheduler_step_size=args.bnn_lr_scheduler_step_size,
                  lr_scheduler_gamma=args.bnn_lr_scheduler_gamma,
                  averaged_weights=args.averaged_weights,
                  avg_weights_count=args.avg_weights_count,
                  initial_mu_weights=args.initial_mu_weights_range,
                  initial_mu_bias=args.initial_mu_bias_range,
                  initial_rho_weights=args.initial_rho_weights_range,
                  initial_rho_bias=args.initial_rho_bias_range)
    bnn_env = Environment(bnn_agent, trainloader,
                          'Bayes by Backprop')

    eg5_agent = EGreedyNNAgent(epsilon=.05, 
                        optimizer_constructor=optimizer_constructor,
                        optim_params=eg_optimizer_params)
    eg5_env = Environment(eg5_agent, copy.deepcopy(trainloader),
                          'Epsilon Greedy 5%')

    eg1_agent = EGreedyNNAgent(epsilon=.01, 
                        optimizer_constructor=optimizer_constructor,
                        optim_params=eg_optimizer_params)
    eg1_env = Environment(eg1_agent, copy.deepcopy(trainloader),
                          'Epsilon Greedy 1%')

    eg0_agent = EGreedyNNAgent(epsilon=.00, 
                        optimizer_constructor=optimizer_constructor,
                        optim_params=eg_optimizer_params)
    eg0_env = Environment(eg0_agent, copy.deepcopy(trainloader),
                          'Greedy')

    bnn_envs.append(bnn_env)
    eg5_envs.append(eg5_env)
    eg1_envs.append(eg1_env)
    eg0_envs.append(eg0_env)

  bnn_loss = []
  bnn_kl_divergence = []
  bnn_nll = []

  eg5_regret = []
  eg1_regret = []
  eg0_regret = []
  bnn_regret = []

  eg5_mushrooms_eaten = []
  eg1_mushrooms_eaten = []
  eg0_mushrooms_eaten = []
  bnn_mushrooms_eaten = []

  eg5_regret_from_passing = []
  eg1_regret_from_passing = []
  eg0_regret_from_passing = []
  bnn_regret_from_passing = []

  eg5_regret_from_eating = []
  eg1_regret_from_eating = []
  eg0_regret_from_eating = []
  bnn_regret_from_eating = []

  envs = [(eg5_envs, eg5_regret, eg5_regret_from_eating, 
           eg5_regret_from_passing, eg5_mushrooms_eaten),
          (eg1_envs, eg1_regret, eg1_regret_from_eating, 
           eg1_regret_from_passing, eg1_mushrooms_eaten),
          (eg0_envs, eg0_regret, eg0_regret_from_eating, 
           eg0_regret_from_passing, eg0_mushrooms_eaten),
          (bnn_envs, bnn_regret, bnn_regret_from_eating, 
           bnn_regret_from_passing, bnn_mushrooms_eaten)]

  # If necessary, create directory for graph outputs
  if not os.path.isdir('results'):
    os.makedirs('results')  

  if not os.path.isdir('results/{}'.format(args.experiment_name)):
    os.makedirs('results/{}'.format(args.experiment_name))

  if not os.path.isdir('results/{}/graphs'.format(args.experiment_name)):
    os.makedirs('results/{}/graphs'.format(args.experiment_name))

  for i in range(50000):

    logs = False
    if (i+1) % 100 == 0:
      logs = True
      print('{}.'.format(i))

    for env_set, regret, regret_eat, regret_pass, mushrooms_eaten in envs:
      if logs:
        print(env_set[0].name)
      is_bnn = env_set[0].name == 'Bayes by Backprop'
      avg_regret = 0
      avg_regret_from_eating = 0
      avg_regret_from_passing = 0
      avg_mushrooms_eaten = 0
      if is_bnn:
        avg_loss = 0
        avg_kl_divergence = 0
        avg_nll = 0
      for env in env_set:
        if is_bnn:
          loss, kl_divergence, nll = env.play_round(logs=logs)
        else:
          env.play_round(logs=logs)
        avg_regret += env.cumulative_regret
        avg_mushrooms_eaten += env.mushrooms_eaten
        avg_regret_from_eating += env.regret_from_eating
        avg_regret_from_passing += env.regret_from_passing
        if is_bnn:
          avg_loss += loss
          avg_kl_divergence += kl_divergence
          avg_nll += nll
      avg_regret /= args.number_of_runs
      avg_regret_from_eating /= args.number_of_runs
      avg_regret_from_passing /= args.number_of_runs
      avg_mushrooms_eaten /= args.number_of_runs
      if is_bnn:
        avg_loss /= args.number_of_runs
        avg_kl_divergence /= args.number_of_runs
        avg_nll /= args.number_of_runs 
        bnn_loss.append(avg_loss)
        bnn_kl_divergence.append(avg_kl_divergence)
        bnn_nll.append(avg_nll)
      regret.append(avg_regret)
      regret_eat.append(avg_regret_from_eating)
      regret_pass.append(avg_regret_from_passing)
      mushrooms_eaten.append(avg_mushrooms_eaten)
    
    if (i+1) % 500 == 0:
      plt.plot(np.array(bnn_loss), label='BNN loss')
      plt.legend()
      plt.ylabel('Loss')
      plt.savefig('results/{}/graphs/bnn_loss_{}'.format(args.experiment_name, i+1))
      plt.clf()
      
      plt.plot(np.array(bnn_kl_divergence), label='BNN KL Divergence')
      plt.legend()
      plt.ylabel('KL Divergence')
      plt.savefig('results/{}/graphs/bnn_kl_divergence_{}'.format(args.experiment_name, i+1))
      plt.clf()

      plt.plot(np.array(bnn_nll), label='BNN NLL')
      plt.legend()
      plt.ylabel('Negative Log Likelihood')
      plt.savefig('results/{}/graphs/bnn_nll_{}'.format(args.experiment_name, i+1))
      plt.clf()
      
      for env_set, regret, _, _, _ in envs:
        plt.plot(np.array(regret), label=env_set[0].name)
      plt.legend()
      plt.ylabel('Cumulative Regret')
      plt.savefig('results/{}/graphs/regret_{}'.format(args.experiment_name, i+1))
      plt.clf()

      for env_set, _, regret_eat, _, _ in envs:
        plt.plot(np.array(regret_eat), label=env_set[0].name)
      plt.legend()
      plt.ylabel('Cumulative Regret from Eating')
      plt.savefig('results/{}/graphs/regret_eating_{}'.format(args.experiment_name, i+1))
      plt.clf()

      for env_set, _, _, regret_pass, _ in envs:
        plt.plot(np.array(regret_pass), label=env_set[0].name)
      plt.legend()
      plt.ylabel('Cumulative Regret from Passing')
      plt.savefig('results/{}/graphs/regret_passing_{}'.format(args.experiment_name, i+1))
      plt.clf()

      for env_set, _, _, _, mushrooms_eaten in envs:
        plt.plot(np.array(mushrooms_eaten), label=env_set[0].name)
      plt.legend()
      plt.ylabel('Mushrooms Eaten')
      plt.savefig('results/{}/graphs/mushrooms_eaten_{}'.format(args.experiment_name, i+1))
      plt.clf()
