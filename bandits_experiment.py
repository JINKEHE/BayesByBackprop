# -*- coding: utf-8 -*-
"""Bandits.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ryV2BfuI6A-J3g7UzzGreDlS6onh3HPw
"""

import queue
import random
import torch
import copy
import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch.utils.data
from torch import optim
from torch import nn
from torch import distributions as dist

from core import *
from bandits import *

print(torch.__version__)

use_cuda = torch.cuda.is_available()

CONTEXT_SIZE = 117 + 1
SAMPLE_COUNT = 2
NUM_ACTIONS = 2
AGENT_MEMORY_LEN = 4096

EDIBLE_REWARD = 5.0
POISONOUS_REWARD = -35.0

EDIBLE_CONSTANT = 1.0
POISONOUS_CONSTANT = -1.0

"""The environment sends the agent the features of a mushroom.

The agent can pick one of two actions:
1. eat
2. pass

If the agent picks pass, the regret is always 0.

If the agent picks eat:
1. if the mushroom is edible, the reward is always +5.0.
2. if the mushroom is poisonous, with prob 1/2 the reward is -35.0, with prob 1/2 the reward is +5.0
"""

# Load the UCI Mushroom Dataset: 8124 datapoints, each with 22 categorical
# features and one label - edible/poisonous. The features are transformed to a
# one-hot encoding. 
# The missing values (marked with ?) are treated as a different class for now.

mushroom_dataset = pd.read_csv('mushrooms.csv')
train_labels = mushroom_dataset['class']
train_labels = train_labels.replace(['p', 'e'],
                                    [POISONOUS_CONSTANT, EDIBLE_CONSTANT])
train_features = pd.get_dummies(mushroom_dataset.drop(['class'], axis=1))

train_features = torch.tensor(train_features.values, dtype=torch.float)
train_labels = torch.tensor(train_labels.values)

trainset = torch.utils.data.TensorDataset(train_features, train_labels)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,
                                          shuffle=True, num_workers=0)

"""The system is represented by two entities: an **agent** and an **environment**.  They interact in rounds, in the following manner:

1. The environment randomly selects a mushroom from the dataset and presents its features (the current context) to the agent.

2. The agent selects an action that it deems optimal given the context.

3. The environment computes the reward for the selected action and sends it back to the agent, which updates its predictions.
"""
    
# Parameters for bnn and eg agents

LEARNING_RATE = 0.001
EPSILON = 0.001
PI = 0.75
SIGMA1 = math.exp(-2)
SIGMA2 = math.exp(-7)
BNN_LR_SCHEDULER_STEP_SIZE = 32

optimizer_constructor = torch.optim.Adam
optimizer_params = {'lr': LEARNING_RATE} #, 'eps': EPSILON,}
prior_params = {'pi': PI, 'sigma1': SIGMA1, 'sigma2': SIGMA2}
      
bnn_agent = BNNAgent(optimizer_constructor=optimizer_constructor,
                     optim_params=optimizer_params,
                     prior_params=prior_params,
                     lr_scheduler_step_size=BNN_LR_SCHEDULER_STEP_SIZE)
bnn_env = Environment(bnn_agent, trainloader)

eg_agent = EGreedyNNAgent(epsilon=.05, 
                          optimizer_constructor=optimizer_constructor,
                          optim_params=optimizer_params)
eg_env = Environment(eg_agent, copy.deepcopy(trainloader))

eg_loss = []  # Loss in last 100
eg_regret = []
bnn_loss = []
bnn_regret = []

# If necessary, create directory for graph outputs
if not os.path.isdir('graphs'):
  os.makedirs('graphs')

for i in range(4000):

  logs = False
  if (i+1) % 100 == 0:
    logs = True
    print('{}.'.format(i))

  current_eg_loss = eg_env.play_round(logs=logs)
  current_bnn_loss = bnn_env.play_round(logs=logs)

  if (i+1) % 50 == 0:
    eg_loss.append(current_eg_loss)
    bnn_loss.append(current_bnn_loss)
  eg_regret.append(eg_env.cumulative_regret)
  bnn_regret.append(bnn_env.cumulative_regret)

  if (i+1) % 500 == 0:
    plt.plot(np.array(bnn_loss), label='BNN loss')
    plt.legend()
    plt.ylabel('Loss')
    plt.savefig('graphs/bnn_loss_{}'.format(i+1))
    plt.clf()
    
    plt.plot(np.array(eg_regret), label='EG Regret')
    plt.plot(np.array(bnn_regret), label='BNN Regret')
    plt.legend()
    plt.ylabel('Cumulative Regret')
    plt.savefig('graphs/regret_{}'.format(i+1))
    plt.clf()
    bnn_loss = []
    eg_loss = []
