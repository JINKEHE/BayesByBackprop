Use GPU: False
1.0.1.post2
99.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.6768]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.7590]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 385.0
1. Loss: 2.689642906188965
Action 0 - predicted reward: tensor([[-0.3293]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3597]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 330.0
1. Loss: 0.531099259853363
Action 0 - predicted reward: tensor([[-0.6347]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6506]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 310.0
1. Loss: 0.39858195185661316
Action 0 - predicted reward: tensor([[-2.8374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.6391]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 560.0
1. Loss: 2.190303325653076
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.9214]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.9412]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 315.0
1. Loss: 0.9554077386856079
Action 0 - predicted reward: tensor([[-4.8939]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.8929]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 510.0
1. Loss: 1.4646916389465332
Action 0 - predicted reward: tensor([[-0.7540]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.8603]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 345.0
1. Loss: 0.7662249803543091
Action 0 - predicted reward: tensor([[0.0134]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.9746e-05]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 265.0
1. Loss: 0.003925381228327751
Greedy
Action 0 - predicted reward: tensor([[-0.0817]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0774]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 375.0
1. Loss: 1.130333423614502
Action 0 - predicted reward: tensor([[-0.8370]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.8911]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 360.0
1. Loss: 1.4080727100372314
Action 0 - predicted reward: tensor([[-5.6947]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.3786]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 735.0
1. Loss: 2.9111924171447754
Action 0 - predicted reward: tensor([[-0.1148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1237]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 420.0
1. Loss: 1.8509979248046875
Bayes by Backprop
Action 0 - predicted reward: tensor([[-3.6041]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-4.0063]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 735.0
1. Loss: 157328.6875
Action 0 - predicted reward: tensor([[-2.0101]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.1222]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 545.0
1. Loss: 97806.3828125
Action 0 - predicted reward: tensor([[-1.4656]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.5902]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 430.0
1. Loss: 62989.2109375
Action 0 - predicted reward: tensor([[-4.3910]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-4.3520]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 695.0
1. Loss: 147703.3125
199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[6.0158]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.9989]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 630.0
3. Loss: 1.0118670463562012
Action 0 - predicted reward: tensor([[-0.4903]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5678]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 605.0
3. Loss: 0.3047218918800354
Action 0 - predicted reward: tensor([[-0.3272]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3705]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 555.0
3. Loss: 0.19351907074451447
Action 0 - predicted reward: tensor([[0.5563]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.5790]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 880.0
3. Loss: 0.5190019011497498
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1867]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 575.0
3. Loss: 0.37605199217796326
Action 0 - predicted reward: tensor([[0.2882]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0253]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 770.0
3. Loss: 0.39675602316856384
Action 0 - predicted reward: tensor([[0.9428]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.8619]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 575.0
3. Loss: 0.2790525257587433
Action 0 - predicted reward: tensor([[0.0795]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0587]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 550.0
3. Loss: 0.0038740679156035185
Greedy
Action 0 - predicted reward: tensor([[-0.3893]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4280]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 570.0
3. Loss: 0.40918123722076416
Action 0 - predicted reward: tensor([[0.9746]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.9272]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 585.0
3. Loss: 1.043418526649475
Action 0 - predicted reward: tensor([[0.7523]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.1205]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 940.0
3. Loss: 0.600677490234375
Action 0 - predicted reward: tensor([[0.5446]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 665.0
3. Loss: 0.6255742311477661
Bayes by Backprop
Action 0 - predicted reward: tensor([[-2.2493]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.9876]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1025.0
3. Loss: 116051.3515625
Action 0 - predicted reward: tensor([[-1.8384]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.8016]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1025.0
3. Loss: 119794.0234375
Action 0 - predicted reward: tensor([[-0.3787]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3267]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 670.0
3. Loss: 48746.63671875
Action 0 - predicted reward: tensor([[-4.3385]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-4.3624]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1515.0
3. Loss: 176943.5625
299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.4333]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.5677]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 845.0
4. Loss: 0.2853412330150604
Action 0 - predicted reward: tensor([[-0.0016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1367]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 835.0
4. Loss: 0.19314968585968018
Action 0 - predicted reward: tensor([[-0.1115]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2215]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 865.0
4. Loss: 0.1888987421989441
Action 0 - predicted reward: tensor([[0.3339]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.3379]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1140.0
4. Loss: 0.09722332656383514
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.6387]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.5209]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 815.0
4. Loss: 0.1916692554950714
Action 0 - predicted reward: tensor([[0.1898]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0189]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1055.0
4. Loss: 0.07796779274940491
Action 0 - predicted reward: tensor([[0.4447]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3291]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 835.0
4. Loss: 0.03042609617114067
Action 0 - predicted reward: tensor([[0.0520]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0446]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 810.0
4. Loss: 0.002576307626441121
Greedy
Action 0 - predicted reward: tensor([[0.2036]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1183]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 855.0
4. Loss: 0.2911371886730194
Action 0 - predicted reward: tensor([[0.4656]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.1302]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 860.0
4. Loss: 0.2452576905488968
Action 0 - predicted reward: tensor([[1.5706]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.2837]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1095.0
4. Loss: 0.07534556090831757
Action 0 - predicted reward: tensor([[0.8965]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.7246]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 905.0
4. Loss: 0.19334658980369568
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.9332]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.9511]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1545.0
4. Loss: 117118.015625
Action 0 - predicted reward: tensor([[-1.0659]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0431]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1335.0
4. Loss: 100653.53125
Action 0 - predicted reward: tensor([[-0.8364]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8491]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1160.0
4. Loss: 64301.33203125
Action 0 - predicted reward: tensor([[-4.0361]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-4.0360]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2075.0
4. Loss: 159181.171875
399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[2.5263]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.9514]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1220.0
6. Loss: 0.4584391415119171
Action 0 - predicted reward: tensor([[0.2080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0797]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1105.0
6. Loss: 0.31029385328292847
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1105.0
6. Loss: 0.12854766845703125
Action 0 - predicted reward: tensor([[0.5100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.7190]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1470.0
6. Loss: 0.34438174962997437
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2753]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2210]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1025.0
6. Loss: 0.0506635382771492
Action 0 - predicted reward: tensor([[-0.1883]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.0199]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1305.0
6. Loss: 0.02245667390525341
Action 0 - predicted reward: tensor([[0.3420]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1939]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1100.0
6. Loss: 0.07419060170650482
Action 0 - predicted reward: tensor([[0.0126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0005]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1070.0
6. Loss: 0.009480801410973072
Greedy
Action 0 - predicted reward: tensor([[0.1609]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2651]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1080.0
6. Loss: 0.1843980848789215
Action 0 - predicted reward: tensor([[0.8569]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.8948]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1065.0
6. Loss: 0.051253948360681534
Action 0 - predicted reward: tensor([[0.2375]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2830]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1210.0
6. Loss: 0.037042226642370224
Action 0 - predicted reward: tensor([[0.0579]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2497]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1155.0
6. Loss: 0.19685472548007965
Bayes by Backprop
Action 0 - predicted reward: tensor([[-2.2540]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.2365]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2115.0
6. Loss: 125649.96875
Action 0 - predicted reward: tensor([[-0.7155]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7135]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1705.0
6. Loss: 98841.359375
Action 0 - predicted reward: tensor([[-1.3127]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.3144]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1715.0
6. Loss: 78332.9296875
Action 0 - predicted reward: tensor([[-3.4986]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-3.4984]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2575.0
6. Loss: 155137.5
499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[4.0612]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[8.9550]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1610.0
7. Loss: 0.8804492950439453
Action 0 - predicted reward: tensor([[0.0108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8331]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1365.0
7. Loss: 0.16672058403491974
Action 0 - predicted reward: tensor([[0.2322]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1490]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1400.0
7. Loss: 0.07695937901735306
Action 0 - predicted reward: tensor([[-0.1215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8152]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1825.0
7. Loss: 0.4093652367591858
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1307]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.3553]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1130.0
7. Loss: 0.028628358617424965
Action 0 - predicted reward: tensor([[0.0969]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.7789]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1575.0
7. Loss: 0.018401337787508965
Action 0 - predicted reward: tensor([[0.8752]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.4432]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1420.0
7. Loss: 0.39894914627075195
Action 0 - predicted reward: tensor([[0.1479]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1410]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1235.0
7. Loss: 0.02090693637728691
Greedy
Action 0 - predicted reward: tensor([[1.0032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0696]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1335.0
7. Loss: 0.11393372714519501
Action 0 - predicted reward: tensor([[0.1100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.8528]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1190.0
7. Loss: 0.031351592391729355
Action 0 - predicted reward: tensor([[-0.0432]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7959]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1370.0
7. Loss: 0.08125513046979904
Action 0 - predicted reward: tensor([[0.0169]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.0230]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1275.0
7. Loss: 0.068047434091568
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.8830]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.8796]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2550.0
7. Loss: 122209.375
Action 0 - predicted reward: tensor([[-1.1549]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1550]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2230.0
7. Loss: 107951.5625
Action 0 - predicted reward: tensor([[-0.9410]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9849]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1975.0
7. Loss: 82130.78125
Action 0 - predicted reward: tensor([[-2.7611]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.7608]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2870.0
7. Loss: 134516.828125
599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.9551]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.0597]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1660.0
9. Loss: 0.1404276341199875
Action 0 - predicted reward: tensor([[0.3699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1588]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1690.0
9. Loss: 0.10848754644393921
Action 0 - predicted reward: tensor([[0.0917]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.2439]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1675.0
9. Loss: 0.016084041446447372
Action 0 - predicted reward: tensor([[0.4186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.3299]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1930.0
9. Loss: 0.1908463090658188
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7571]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1305.0
9. Loss: 0.07251182943582535
Action 0 - predicted reward: tensor([[-0.0165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0411]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1980.0
9. Loss: 0.2716268002986908
Action 0 - predicted reward: tensor([[0.1147]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.1995]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1595.0
9. Loss: 0.1088840588927269
Action 0 - predicted reward: tensor([[-0.3751]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4286]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1765.0
9. Loss: 0.5122172832489014
Greedy
Action 0 - predicted reward: tensor([[0.4840]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.4167]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1565.0
9. Loss: 0.08018314838409424
Action 0 - predicted reward: tensor([[0.0137]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.0704]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1510.0
9. Loss: 0.04539293423295021
Action 0 - predicted reward: tensor([[0.1637]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.6765]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1525.0
9. Loss: 0.09505666047334671
Action 0 - predicted reward: tensor([[-0.4891]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.5646]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1495.0
9. Loss: 0.1694457232952118
Bayes by Backprop
Action 0 - predicted reward: tensor([[-2.3962]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.3962]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2975.0
9. Loss: 116022.46875
Action 0 - predicted reward: tensor([[-0.7509]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7517]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2600.0
9. Loss: 105997.40625
Action 0 - predicted reward: tensor([[-1.0158]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9692]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2460.0
9. Loss: 87826.7265625
Action 0 - predicted reward: tensor([[-2.8468]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.9507]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3350.0
9. Loss: 132374.28125
699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-2.0406]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.9210]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1775.0
10. Loss: 0.07069412618875504
Action 0 - predicted reward: tensor([[0.1018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5706]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1905.0
10. Loss: 0.03445560112595558
Action 0 - predicted reward: tensor([[0.0582]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0433]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1865.0
10. Loss: 0.04487514868378639
Action 0 - predicted reward: tensor([[1.0672]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0097]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2210.0
10. Loss: 0.6548064351081848
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2390]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8108]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1445.0
10. Loss: 0.04905333369970322
Action 0 - predicted reward: tensor([[0.1011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2705]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2215.0
10. Loss: 0.2490435540676117
Action 0 - predicted reward: tensor([[-0.0536]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.1479]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1785.0
10. Loss: 0.05580878630280495
Action 0 - predicted reward: tensor([[0.0172]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1643]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1925.0
10. Loss: 0.3676249086856842
Greedy
Action 0 - predicted reward: tensor([[-0.3743]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.2231]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1760.0
10. Loss: 0.08322358131408691
Action 0 - predicted reward: tensor([[0.1250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.6716]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1720.0
10. Loss: 0.08634475618600845
Action 0 - predicted reward: tensor([[-1.4308]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.3060]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1610.0
10. Loss: 0.1336291879415512
Action 0 - predicted reward: tensor([[1.4993]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.9884]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1750.0
10. Loss: 0.2586419880390167
Bayes by Backprop
Action 0 - predicted reward: tensor([[-2.1741]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.1680]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 3290.0
10. Loss: 110163.6796875
Action 0 - predicted reward: tensor([[-1.5243]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.5245]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3035.0
10. Loss: 107706.25
Action 0 - predicted reward: tensor([[-0.4931]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5436]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2685.0
10. Loss: 82716.0859375
Action 0 - predicted reward: tensor([[-2.3265]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.3266]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3925.0
10. Loss: 131828.890625
799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4918]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7289]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2120.0
12. Loss: 0.07291751354932785
Action 0 - predicted reward: tensor([[0.3484]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.9952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2085.0
12. Loss: 0.020547132939100266
Action 0 - predicted reward: tensor([[0.3011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.9582]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2090.0
12. Loss: 0.12905563414096832
Action 0 - predicted reward: tensor([[-0.3800]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.6943]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2255.0
12. Loss: 0.20029638707637787
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7681]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1580.0
12. Loss: 0.0788438692688942
Action 0 - predicted reward: tensor([[0.2348]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.6544]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2535.0
12. Loss: 0.11235222220420837
Action 0 - predicted reward: tensor([[-0.5905]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.7025]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1950.0
12. Loss: 0.07469743490219116
Action 0 - predicted reward: tensor([[-0.5219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.8517]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2050.0
12. Loss: 0.1824169009923935
Greedy
Action 0 - predicted reward: tensor([[0.1798]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.0345]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1855.0
12. Loss: 0.01253387052565813
Action 0 - predicted reward: tensor([[-0.5736]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.4127]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1830.0
12. Loss: 0.046877793967723846
Action 0 - predicted reward: tensor([[-0.1120]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.6194]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1705.0
12. Loss: 0.1508665680885315
Action 0 - predicted reward: tensor([[0.5159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.5194]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1940.0
12. Loss: 0.1441783457994461
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.1697]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.2227]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3545.0
12. Loss: 103760.2578125
Action 0 - predicted reward: tensor([[-1.4066]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.4214]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3330.0
12. Loss: 101605.1015625
Action 0 - predicted reward: tensor([[-1.1529]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.2452]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2960.0
12. Loss: 78827.953125
Action 0 - predicted reward: tensor([[-2.1939]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.3138]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4220.0
12. Loss: 121072.609375
899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3294]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.8913]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2235.0
14. Loss: 0.027491383254528046
Action 0 - predicted reward: tensor([[0.4242]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.0696]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2270.0
14. Loss: 0.041732363402843475
Action 0 - predicted reward: tensor([[0.5150]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2828]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2340.0
14. Loss: 0.10280771553516388
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2335.0
14. Loss: 0.0747758075594902
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.9337]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.7219]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1685.0
14. Loss: 0.09541849046945572
Action 0 - predicted reward: tensor([[0.2294]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.5957]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2780.0
14. Loss: 0.13890422880649567
Action 0 - predicted reward: tensor([[-0.0034]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4521]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2060.0
14. Loss: 0.2420208603143692
Action 0 - predicted reward: tensor([[0.0323]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3611]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2280.0
14. Loss: 0.19106534123420715
Greedy
Action 0 - predicted reward: tensor([[0.3199]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.5696]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1925.0
14. Loss: 0.019068490713834763
Action 0 - predicted reward: tensor([[-0.1953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.3327]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2020.0
14. Loss: 0.0918089970946312
Action 0 - predicted reward: tensor([[-0.1099]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.5960]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1815.0
14. Loss: 0.2711811363697052
Action 0 - predicted reward: tensor([[1.9105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.1769]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1980.0
14. Loss: 0.10267091542482376
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.1933]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.3044]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3900.0
14. Loss: 102380.15625
Action 0 - predicted reward: tensor([[-0.8593]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8551]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3765.0
14. Loss: 102296.3359375
Action 0 - predicted reward: tensor([[-1.1160]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1418]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3275.0
14. Loss: 79526.9765625
Action 0 - predicted reward: tensor([[-2.1283]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.4334]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4585.0
14. Loss: 117060.2734375
999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.8837]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.5066]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2460.0
15. Loss: 0.05550815537571907
Action 0 - predicted reward: tensor([[-0.1754]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.6843]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2390.0
15. Loss: 0.04415082186460495
Action 0 - predicted reward: tensor([[-0.0437]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2636]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2740.0
15. Loss: 0.19668355584144592
Action 0 - predicted reward: tensor([[0.1692]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.6194]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2430.0
15. Loss: 0.05728132277727127
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.7529]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1715.0
15. Loss: 0.05297357216477394
Action 0 - predicted reward: tensor([[0.2377]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2945]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2910.0
15. Loss: 0.09611444920301437
Action 0 - predicted reward: tensor([[-0.1936]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.4657]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2215.0
15. Loss: 0.0669441744685173
Action 0 - predicted reward: tensor([[0.1020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.1714]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2400.0
15. Loss: 0.031029969453811646
Greedy
Action 0 - predicted reward: tensor([[0.1065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9478]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1965.0
15. Loss: 0.002283753827214241
Action 0 - predicted reward: tensor([[0.2927]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.8538]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2065.0
15. Loss: 0.04642877355217934
Action 0 - predicted reward: tensor([[-0.3657]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.5820]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1875.0
15. Loss: 0.08825203776359558
Action 0 - predicted reward: tensor([[2.4428]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2176]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2070.0
15. Loss: 0.08039715886116028
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.4428]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.3967]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4115.0
15. Loss: 94915.15625
Action 0 - predicted reward: tensor([[-1.6230]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.5474]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4155.0
15. Loss: 100336.0703125
Action 0 - predicted reward: tensor([[-0.4488]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4529]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3400.0
15. Loss: 73695.8203125
Action 0 - predicted reward: tensor([[-2.1014]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.1011]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4990.0
15. Loss: 114872.828125
1099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.4363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.7681]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2615.0
17. Loss: 0.7222354412078857
Action 0 - predicted reward: tensor([[0.1382]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2395]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2495.0
17. Loss: 0.048551734536886215
Action 0 - predicted reward: tensor([[-0.2401]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.3459]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2820.0
17. Loss: 0.08656536787748337
Action 0 - predicted reward: tensor([[-0.1948]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5364]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2570.0
17. Loss: 0.05432181432843208
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1675]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.8046]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1810.0
17. Loss: 0.03647631034255028
Action 0 - predicted reward: tensor([[0.0140]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.3867]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3115.0
17. Loss: 0.09120995551347733
Action 0 - predicted reward: tensor([[-0.3918]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3722]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2255.0
17. Loss: 0.05279671773314476
Action 0 - predicted reward: tensor([[-0.3238]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.7830]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2555.0
17. Loss: 0.02664612978696823
Greedy
Action 0 - predicted reward: tensor([[-0.1326]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.6792]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2110.0
17. Loss: 0.024879436939954758
Action 0 - predicted reward: tensor([[-0.0231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1193]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2105.0
17. Loss: 0.037052322179079056
Action 0 - predicted reward: tensor([[0.2773]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.5927]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2055.0
17. Loss: 0.09211662411689758
Action 0 - predicted reward: tensor([[2.6170]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.7222]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2160.0
17. Loss: 0.06579526513814926
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.3953]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.4758]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4310.0
17. Loss: 89693.5859375
Action 0 - predicted reward: tensor([[-1.0511]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0513]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4530.0
17. Loss: 99489.9453125
Action 0 - predicted reward: tensor([[-0.4973]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7589]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3590.0
17. Loss: 71091.3828125
Action 0 - predicted reward: tensor([[-1.1931]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.2864]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5265.0
17. Loss: 111915.9453125
1199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.6761]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2685.0
18. Loss: 0.05781964585185051
Action 0 - predicted reward: tensor([[0.2899]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.8613]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2590.0
18. Loss: 0.05867652967572212
Action 0 - predicted reward: tensor([[0.8877]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.9062]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2905.0
18. Loss: 0.04207596927881241
Action 0 - predicted reward: tensor([[0.4705]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.9230]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2760.0
18. Loss: 0.0426587350666523
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.2996]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1920.0
18. Loss: 0.027881721034646034
Action 0 - predicted reward: tensor([[-0.3594]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.8807]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3235.0
18. Loss: 0.04979611188173294
Action 0 - predicted reward: tensor([[-0.2223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.0982]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2300.0
18. Loss: 0.03930830955505371
Action 0 - predicted reward: tensor([[0.1482]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.9647]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2635.0
18. Loss: 0.010808385908603668
Greedy
Action 0 - predicted reward: tensor([[-0.0616]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.5811]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2195.0
18. Loss: 0.005406328476965427
Action 0 - predicted reward: tensor([[-0.1355]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.2218]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2290.0
18. Loss: 0.05151143670082092
Action 0 - predicted reward: tensor([[-0.1909]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.3884]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2125.0
18. Loss: 0.027322378009557724
Action 0 - predicted reward: tensor([[0.2769]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9619]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2210.0
18. Loss: 0.045128289610147476
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.9286]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9927]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4505.0
18. Loss: 86077.3359375
Action 0 - predicted reward: tensor([[-1.0434]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0715]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4750.0
18. Loss: 96297.9609375
Action 0 - predicted reward: tensor([[-0.9524]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0278]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3885.0
18. Loss: 70884.0546875
Action 0 - predicted reward: tensor([[-1.1098]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1060]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5420.0
18. Loss: 104163.9609375
1299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2290]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.7591]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2830.0
20. Loss: 0.0649968609213829
Action 0 - predicted reward: tensor([[0.4148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7261]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2765.0
20. Loss: 0.03284842148423195
Action 0 - predicted reward: tensor([[-0.0658]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2905]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2995.0
20. Loss: 0.02002668008208275
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2790.0
20. Loss: 0.06798297166824341
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3735]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.1482]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1955.0
20. Loss: 0.012134699150919914
Action 0 - predicted reward: tensor([[0.1837]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8877]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3445.0
20. Loss: 0.04352762550115585
Action 0 - predicted reward: tensor([[-0.0261]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.8399]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2445.0
20. Loss: 0.03174666687846184
Action 0 - predicted reward: tensor([[1.4116]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0380]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2720.0
20. Loss: 0.004710453096777201
Greedy
Action 0 - predicted reward: tensor([[-0.0562]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5678]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2235.0
20. Loss: 0.0006509749800898135
Action 0 - predicted reward: tensor([[0.1218]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.0097]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2380.0
20. Loss: 0.041782207787036896
Action 0 - predicted reward: tensor([[-0.7196]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.3756]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2245.0
20. Loss: 0.030325528234243393
Action 0 - predicted reward: tensor([[-0.6287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.4911]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2280.0
20. Loss: 0.04240770637989044
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.8085]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1911]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4715.0
20. Loss: 84100.8359375
Action 0 - predicted reward: tensor([[-0.5009]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5008]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5075.0
20. Loss: 94391.1484375
Action 0 - predicted reward: tensor([[-0.2687]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4184]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4050.0
20. Loss: 69554.59375
Action 0 - predicted reward: tensor([[-0.9300]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8951]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5575.0
20. Loss: 98164.828125
1399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3298]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3020.0
21. Loss: 0.09307625144720078
Action 0 - predicted reward: tensor([[-0.1965]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1921]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2900.0
21. Loss: 0.030441278591752052
Action 0 - predicted reward: tensor([[0.1920]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.6227]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3080.0
21. Loss: 0.030759917572140694
Action 0 - predicted reward: tensor([[-1.5508]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.7604]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2875.0
21. Loss: 0.042717333883047104
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-1.6102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.5844]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2075.0
21. Loss: 0.014876436442136765
Action 0 - predicted reward: tensor([[-0.0609]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.2071]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3480.0
21. Loss: 0.017945684492588043
Action 0 - predicted reward: tensor([[0.2327]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.8671]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2555.0
21. Loss: 0.02511848509311676
Action 0 - predicted reward: tensor([[-0.0467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.9778]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2755.0
21. Loss: 0.014811009168624878
Greedy
Action 0 - predicted reward: tensor([[0.0095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9261]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2240.0
21. Loss: 0.00032140759867616
Action 0 - predicted reward: tensor([[-0.0493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.6386]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2425.0
21. Loss: 0.03118198923766613
Action 0 - predicted reward: tensor([[0.3388]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.0893]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2350.0
21. Loss: 0.02477336674928665
Action 0 - predicted reward: tensor([[-0.0165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.3523]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2405.0
21. Loss: 0.02746882103383541
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.6003]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5911]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5020.0
21. Loss: 82976.359375
Action 0 - predicted reward: tensor([[-0.6202]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6203]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5260.0
21. Loss: 90929.3515625
Action 0 - predicted reward: tensor([[0.0181]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0173]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4160.0
21. Loss: 65762.3359375
Action 0 - predicted reward: tensor([[-0.9031]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.3238]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5780.0
21. Loss: 92984.2421875
1499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1815]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9346]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3060.0
23. Loss: 0.09018091857433319
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2965.0
23. Loss: 0.03339201211929321
Action 0 - predicted reward: tensor([[-0.0300]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.9352]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3105.0
23. Loss: 0.012139307335019112
Action 0 - predicted reward: tensor([[0.6895]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.9350]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2975.0
23. Loss: 0.041087426245212555
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0118]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.1157]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2145.0
23. Loss: 0.01359468698501587
Action 0 - predicted reward: tensor([[-0.1057]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2424]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3555.0
23. Loss: 0.014922505244612694
Action 0 - predicted reward: tensor([[0.1334]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.0400]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2625.0
23. Loss: 0.016742294654250145
Action 0 - predicted reward: tensor([[-0.9497]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5232]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2940.0
23. Loss: 0.030411843210458755
Greedy
Action 0 - predicted reward: tensor([[0.1567]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.4119]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2350.0
23. Loss: 0.012630256824195385
Action 0 - predicted reward: tensor([[-0.1068]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8788]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2470.0
23. Loss: 0.043187133967876434
Action 0 - predicted reward: tensor([[-0.2671]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.3561]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2415.0
23. Loss: 0.018508752807974815
Action 0 - predicted reward: tensor([[-0.2176]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.9041]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2585.0
23. Loss: 0.05303790792822838
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.0514]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0559]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5100.0
23. Loss: 78770.6015625
Action 0 - predicted reward: tensor([[-0.1433]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1432]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5420.0
23. Loss: 87176.015625
Action 0 - predicted reward: tensor([[-0.1728]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1726]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4325.0
23. Loss: 63031.97265625
Action 0 - predicted reward: tensor([[-1.2742]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.3870]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5995.0
23. Loss: 88836.4296875
1599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.6036]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.8039]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3250.0
24. Loss: 0.09035485237836838
Action 0 - predicted reward: tensor([[-0.0243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4255]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2990.0
24. Loss: 0.020961346104741096
Action 0 - predicted reward: tensor([[-0.9769]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.5164]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3255.0
24. Loss: 0.02327997423708439
Action 0 - predicted reward: tensor([[-0.0669]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2637]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3020.0
24. Loss: 0.03700266778469086
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.8329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8884]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2195.0
24. Loss: 0.008615588769316673
Action 0 - predicted reward: tensor([[-0.1314]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8976]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3660.0
24. Loss: 0.01079389825463295
Action 0 - predicted reward: tensor([[0.1193]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.8290]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2770.0
24. Loss: 0.05369419977068901
Action 0 - predicted reward: tensor([[-1.2464]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.7935]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3125.0
24. Loss: 0.02986736223101616
Greedy
Action 0 - predicted reward: tensor([[-0.0386]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9319]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2385.0
24. Loss: 0.010649454779922962
Action 0 - predicted reward: tensor([[0.0722]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2258]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2655.0
24. Loss: 0.04306712746620178
Action 0 - predicted reward: tensor([[0.1252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-73.5592]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2495.0
24. Loss: 0.029692865908145905
Action 0 - predicted reward: tensor([[0.0321]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2740.0
24. Loss: 0.06268727034330368
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.5735]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5723]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5275.0
24. Loss: 76539.2109375
Action 0 - predicted reward: tensor([[-0.0469]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0440]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5630.0
24. Loss: 85077.7265625
Action 0 - predicted reward: tensor([[0.4023]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0769]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4470.0
24. Loss: 62356.8046875
Action 0 - predicted reward: tensor([[-0.6494]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7135]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6165.0
24. Loss: 85657.0703125
1699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1485]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9502]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3375.0
26. Loss: 0.09395889937877655
Action 0 - predicted reward: tensor([[-0.4970]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.5141]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3150.0
26. Loss: 0.04924404248595238
Action 0 - predicted reward: tensor([[0.2610]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.8137]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3350.0
26. Loss: 0.022470299154520035
Action 0 - predicted reward: tensor([[-0.1008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.3264]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3100.0
26. Loss: 0.03781982511281967
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4338]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2953]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2305.0
26. Loss: 0.03229890018701553
Action 0 - predicted reward: tensor([[0.0705]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1468]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3770.0
26. Loss: 0.03291737288236618
Action 0 - predicted reward: tensor([[-1.6444]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.3994]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2955.0
26. Loss: 0.5221739411354065
Action 0 - predicted reward: tensor([[-0.0902]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6710]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3270.0
26. Loss: 0.05477619543671608
Greedy
Action 0 - predicted reward: tensor([[0.2186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.7385]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2525.0
26. Loss: 0.04237576946616173
Action 0 - predicted reward: tensor([[-0.1127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.2222]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2670.0
26. Loss: 0.03419974073767662
Action 0 - predicted reward: tensor([[-0.1555]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8295]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2550.0
26. Loss: 0.0118613475933671
Action 0 - predicted reward: tensor([[0.5614]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.9822]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2945.0
26. Loss: 0.06629203259944916
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.4789]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8345]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5505.0
26. Loss: 74488.5234375
Action 0 - predicted reward: tensor([[-0.0038]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0270]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5805.0
26. Loss: 81910.375
Action 0 - predicted reward: tensor([[-0.0677]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0381]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4655.0
26. Loss: 60092.984375
Action 0 - predicted reward: tensor([[-0.9438]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9431]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6415.0
26. Loss: 82628.15625
1799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5300]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.3738]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3560.0
28. Loss: 0.08274184167385101
Action 0 - predicted reward: tensor([[0.4924]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9794]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3375.0
28. Loss: 0.02912045828998089
Action 0 - predicted reward: tensor([[0.0059]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.9657]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3375.0
28. Loss: 0.0012690270086750388
Action 0 - predicted reward: tensor([[0.9438]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[12.2352]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3145.0
28. Loss: 0.03434392809867859
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5193]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5659]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2350.0
28. Loss: 0.023930398747324944
Action 0 - predicted reward: tensor([[-0.1899]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9214]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3850.0
28. Loss: 0.044131532311439514
Action 0 - predicted reward: tensor([[-0.3776]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.9109]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2970.0
28. Loss: 0.20328480005264282
Action 0 - predicted reward: tensor([[0.3549]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9050]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3345.0
28. Loss: 0.05178967863321304
Greedy
Action 0 - predicted reward: tensor([[-0.0751]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0444]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2560.0
28. Loss: 0.033261995762586594
Action 0 - predicted reward: tensor([[-0.0707]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1881]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2780.0
28. Loss: 0.053515005856752396
Action 0 - predicted reward: tensor([[0.2505]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.4161]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2605.0
28. Loss: 0.009264752268791199
Action 0 - predicted reward: tensor([[0.1676]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.8799]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3025.0
28. Loss: 0.0621284618973732
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.2139]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2251]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5595.0
28. Loss: 72121.578125
Action 0 - predicted reward: tensor([[0.0542]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0541]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5925.0
28. Loss: 79647.5703125
Action 0 - predicted reward: tensor([[0.2406]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2422]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4835.0
28. Loss: 59840.98828125
Action 0 - predicted reward: tensor([[-0.7935]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7982]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6570.0
28. Loss: 79410.84375
1899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.1906]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3850.0
29. Loss: 0.12081871181726456
Action 0 - predicted reward: tensor([[0.0459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0080]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3460.0
29. Loss: 0.022207137197256088
Action 0 - predicted reward: tensor([[0.2165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8554]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3490.0
29. Loss: 0.01279730536043644
Action 0 - predicted reward: tensor([[-4.2692]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7731]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3335.0
29. Loss: 0.056738127022981644
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0123]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.0699]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2390.0
29. Loss: 0.015683505684137344
Action 0 - predicted reward: tensor([[0.0626]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.0588]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3995.0
29. Loss: 0.07159167528152466
Action 0 - predicted reward: tensor([[0.0617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.4298]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3080.0
29. Loss: 0.12773169577121735
Action 0 - predicted reward: tensor([[0.0600]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9473]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3425.0
29. Loss: 0.047523122280836105
Greedy
Action 0 - predicted reward: tensor([[0.3468]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5222]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2560.0
29. Loss: 0.02406230755150318
Action 0 - predicted reward: tensor([[0.0978]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2670]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2930.0
29. Loss: 0.035026662051677704
Action 0 - predicted reward: tensor([[-0.3212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4068]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2650.0
29. Loss: 0.016566013917326927
Action 0 - predicted reward: tensor([[0.1732]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9610]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3180.0
29. Loss: 0.05693504959344864
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.6107]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6107]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5705.0
29. Loss: 69495.765625
Action 0 - predicted reward: tensor([[-0.2025]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1987]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6165.0
29. Loss: 77693.484375
Action 0 - predicted reward: tensor([[0.4832]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3109]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4890.0
29. Loss: 57068.4765625
Action 0 - predicted reward: tensor([[-0.6803]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6802]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6915.0
29. Loss: 78938.3984375
1999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1683]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6668]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4025.0
31. Loss: 0.11926038563251495
Action 0 - predicted reward: tensor([[0.1963]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.8865]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3550.0
31. Loss: 0.028442198410630226
Action 0 - predicted reward: tensor([[0.0433]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.1758]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3610.0
31. Loss: 0.008867240510880947
Action 0 - predicted reward: tensor([[1.0412]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.3003]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3425.0
31. Loss: 0.04249187558889389
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5465]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7287]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2390.0
31. Loss: 0.010699770413339138
Action 0 - predicted reward: tensor([[-0.7433]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.9047]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4080.0
31. Loss: 0.08888895809650421
Action 0 - predicted reward: tensor([[0.3452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3776]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3160.0
31. Loss: 0.04826859384775162
Action 0 - predicted reward: tensor([[-0.0999]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3528]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3640.0
31. Loss: 0.033828865736722946
Greedy
Action 0 - predicted reward: tensor([[0.0951]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9411]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2600.0
31. Loss: 0.015442581847310066
Action 0 - predicted reward: tensor([[-0.0743]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.3438]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2970.0
31. Loss: 0.020817426964640617
Action 0 - predicted reward: tensor([[0.5398]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8217]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2650.0
31. Loss: 0.012473128736019135
Action 0 - predicted reward: tensor([[0.1865]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0844]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3360.0
31. Loss: 0.073188915848732
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.1473]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1516]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5805.0
31. Loss: 66097.8046875
Action 0 - predicted reward: tensor([[-0.1949]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2060]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6295.0
31. Loss: 75471.34375
Action 0 - predicted reward: tensor([[0.8179]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8179]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4975.0
31. Loss: 55311.88671875
Action 0 - predicted reward: tensor([[-0.9538]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1342]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7090.0
31. Loss: 77163.3125
2099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2377]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4558]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4170.0
32. Loss: 0.09798821061849594
Action 0 - predicted reward: tensor([[-0.2129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2806]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3715.0
32. Loss: 0.03091677650809288
Action 0 - predicted reward: tensor([[-0.0194]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5226]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3685.0
32. Loss: 0.008149690926074982
Action 0 - predicted reward: tensor([[0.7162]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9126]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3545.0
32. Loss: 0.03454380854964256
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1257]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0405]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2445.0
32. Loss: 0.008753757923841476
Action 0 - predicted reward: tensor([[0.2532]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9981]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4195.0
32. Loss: 0.06813061982393265
Action 0 - predicted reward: tensor([[0.7709]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.8705]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3275.0
32. Loss: 0.03054029494524002
Action 0 - predicted reward: tensor([[0.0572]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2587]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3750.0
32. Loss: 0.031590212136507034
Greedy
Action 0 - predicted reward: tensor([[0.1666]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0348]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2600.0
32. Loss: 0.014409288763999939
Action 0 - predicted reward: tensor([[0.1407]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3205]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3045.0
32. Loss: 0.019362855702638626
Action 0 - predicted reward: tensor([[-0.4964]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2381]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2690.0
32. Loss: 0.018087560310959816
Action 0 - predicted reward: tensor([[-1.0175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.9933]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3395.0
32. Loss: 0.04087190330028534
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.5090]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6004]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5900.0
32. Loss: 64041.203125
Action 0 - predicted reward: tensor([[0.2181]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1874]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6475.0
32. Loss: 74565.46875
Action 0 - predicted reward: tensor([[0.7387]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6434]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5190.0
32. Loss: 55510.85546875
Action 0 - predicted reward: tensor([[-0.7166]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7203]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7225.0
32. Loss: 74791.375
2199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.0345]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.5690]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4360.0
34. Loss: 0.5097306966781616
Action 0 - predicted reward: tensor([[0.3192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.2240]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3840.0
34. Loss: 0.0323282815515995
Action 0 - predicted reward: tensor([[-0.0198]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2617]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3735.0
34. Loss: 0.007257306016981602
Action 0 - predicted reward: tensor([[0.0461]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.8976]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3625.0
34. Loss: 0.03108125925064087
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4831]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.8606]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2450.0
34. Loss: 0.007270175963640213
Action 0 - predicted reward: tensor([[0.0601]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2438]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4235.0
34. Loss: 0.05554620549082756
Action 0 - predicted reward: tensor([[-0.3248]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.5604]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3315.0
34. Loss: 0.019364237785339355
Action 0 - predicted reward: tensor([[0.5281]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.4423]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3900.0
34. Loss: 0.06351190060377121
Greedy
Action 0 - predicted reward: tensor([[-0.1829]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9418]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2635.0
34. Loss: 0.01830887421965599
Action 0 - predicted reward: tensor([[0.0963]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.2003]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3125.0
34. Loss: 0.028808360919356346
Action 0 - predicted reward: tensor([[0.2214]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9075]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2815.0
34. Loss: 0.04768364503979683
Action 0 - predicted reward: tensor([[-0.0091]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.4211]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3405.0
34. Loss: 0.03615593910217285
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.0707]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2609]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6035.0
34. Loss: 62972.26171875
Action 0 - predicted reward: tensor([[0.2268]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2268]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6715.0
34. Loss: 73570.9921875
Action 0 - predicted reward: tensor([[-0.3550]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7090]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5290.0
34. Loss: 54223.5234375
Action 0 - predicted reward: tensor([[-0.4777]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4773]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7325.0
34. Loss: 71015.0
2299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[1.7402]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.4172]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4400.0
35. Loss: 0.1300262212753296
Action 0 - predicted reward: tensor([[-0.0296]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.1472]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3980.0
35. Loss: 0.0456366203725338
Action 0 - predicted reward: tensor([[0.0203]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.6919]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3785.0
35. Loss: 0.01041503343731165
Action 0 - predicted reward: tensor([[0.2074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.8760]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3770.0
35. Loss: 0.03704918920993805
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2197]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9247]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2495.0
35. Loss: 0.017756273970007896
Action 0 - predicted reward: tensor([[0.4959]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7013]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4320.0
35. Loss: 0.04158531501889229
Action 0 - predicted reward: tensor([[0.5938]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.3301]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3430.0
35. Loss: 0.03222446143627167
Action 0 - predicted reward: tensor([[0.1106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3118]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4010.0
35. Loss: 0.04072095453739166
Greedy
Action 0 - predicted reward: tensor([[0.0260]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2381]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2635.0
35. Loss: 0.017617451027035713
Action 0 - predicted reward: tensor([[0.0515]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6478]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3345.0
35. Loss: 0.040939394384622574
Action 0 - predicted reward: tensor([[-0.1138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9350]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2870.0
35. Loss: 0.027547704055905342
Action 0 - predicted reward: tensor([[-1.8374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.9504]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3485.0
35. Loss: 0.03823363780975342
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7119]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7155]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6140.0
35. Loss: 61896.3671875
Action 0 - predicted reward: tensor([[0.5002]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2281]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6800.0
35. Loss: 71490.171875
Action 0 - predicted reward: tensor([[0.6611]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3554]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5370.0
35. Loss: 52966.41796875
Action 0 - predicted reward: tensor([[-1.1009]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1008]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7450.0
35. Loss: 69971.7265625
2399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.2275]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5614]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4580.0
37. Loss: 0.09298776835203171
Action 0 - predicted reward: tensor([[0.0451]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0083]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4140.0
37. Loss: 0.05317593365907669
Action 0 - predicted reward: tensor([[0.0591]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6288]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3945.0
37. Loss: 0.017394259572029114
Action 0 - predicted reward: tensor([[0.9114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.2373]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3920.0
37. Loss: 0.05030473321676254
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2545]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9936]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2640.0
37. Loss: 0.021554991602897644
Action 0 - predicted reward: tensor([[-0.8880]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.2775]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4435.0
37. Loss: 0.054804638028144836
Action 0 - predicted reward: tensor([[-0.2166]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.1185]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 3510.0
37. Loss: 0.03639097511768341
Action 0 - predicted reward: tensor([[0.1278]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5879]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4150.0
37. Loss: 0.04300587996840477
Greedy
Action 0 - predicted reward: tensor([[0.1203]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.3618]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2675.0
37. Loss: 0.017976080998778343
Action 0 - predicted reward: tensor([[-0.0627]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9939]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3455.0
37. Loss: 0.04284501075744629
Action 0 - predicted reward: tensor([[0.2024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.7599]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2915.0
37. Loss: 0.026843678206205368
Action 0 - predicted reward: tensor([[0.2188]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8488]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3530.0
37. Loss: 0.037324484437704086
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.0027]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0023]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6205.0
37. Loss: 60136.8515625
Action 0 - predicted reward: tensor([[0.4191]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4191]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6965.0
37. Loss: 69724.4296875
Action 0 - predicted reward: tensor([[0.0873]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0822]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5470.0
37. Loss: 51421.22265625
Action 0 - predicted reward: tensor([[-0.3078]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8339]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7560.0
37. Loss: 66934.625
2499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3558]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.2527]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4755.0
39. Loss: 0.08688511699438095
Action 0 - predicted reward: tensor([[0.1090]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1440]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4325.0
39. Loss: 0.07436398416757584
Action 0 - predicted reward: tensor([[0.1124]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6585]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4015.0
39. Loss: 0.007767366711050272
Action 0 - predicted reward: tensor([[-0.2471]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1759]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3945.0
39. Loss: 0.05046923831105232
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1783]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1695]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2645.0
39. Loss: 0.019233709201216698
Action 0 - predicted reward: tensor([[0.0512]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3464]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4570.0
39. Loss: 0.05917520076036453
Action 0 - predicted reward: tensor([[-0.0008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1871]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 3755.0
39. Loss: 0.273937851190567
Action 0 - predicted reward: tensor([[-0.1352]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4226]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4155.0
39. Loss: 0.026752687990665436
Greedy
Action 0 - predicted reward: tensor([[0.0459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.1228]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2675.0
39. Loss: 0.016197608783841133
Action 0 - predicted reward: tensor([[-0.0759]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.7753]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 3530.0
39. Loss: 0.07569370418787003
Action 0 - predicted reward: tensor([[-0.6041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.5497]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2930.0
39. Loss: 0.019089139997959137
Action 0 - predicted reward: tensor([[-0.1106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.7422]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3640.0
39. Loss: 0.03827793896198273
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.0982]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1353]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6360.0
39. Loss: 59633.82421875
Action 0 - predicted reward: tensor([[-0.0789]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0773]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7100.0
39. Loss: 68615.859375
Action 0 - predicted reward: tensor([[0.7247]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7246]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5655.0
39. Loss: 52064.14453125
Action 0 - predicted reward: tensor([[0.0918]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0979]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7670.0
39. Loss: 66142.3671875
2599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3660]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.6429]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4900.0
40. Loss: 0.0814654529094696
Action 0 - predicted reward: tensor([[0.4766]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.2335]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4360.0
40. Loss: 0.06179218739271164
Action 0 - predicted reward: tensor([[0.1914]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.0998]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4085.0
40. Loss: 0.007691557053476572
Action 0 - predicted reward: tensor([[0.3010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9547]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4130.0
40. Loss: 0.04476795345544815
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2633]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.3721]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2795.0
40. Loss: 0.030484791845083237
Action 0 - predicted reward: tensor([[0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.4898]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4685.0
40. Loss: 0.05956228822469711
Action 0 - predicted reward: tensor([[-0.0787]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.3461]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3760.0
40. Loss: 0.030530037358403206
Action 0 - predicted reward: tensor([[0.2765]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.5088]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4155.0
40. Loss: 0.023532435297966003
Greedy
Action 0 - predicted reward: tensor([[0.0571]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.1111]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2750.0
40. Loss: 0.024753892794251442
Action 0 - predicted reward: tensor([[-0.0157]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0042]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3710.0
40. Loss: 0.06926953792572021
Action 0 - predicted reward: tensor([[0.9466]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0746]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2940.0
40. Loss: 0.020602095872163773
Action 0 - predicted reward: tensor([[-0.0542]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.2872]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3690.0
40. Loss: 0.03356901556253433
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4569]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4684]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6380.0
40. Loss: 57409.453125
Action 0 - predicted reward: tensor([[-0.4180]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4432]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7230.0
40. Loss: 66577.375
Action 0 - predicted reward: tensor([[0.5875]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4629]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5670.0
40. Loss: 50079.6875
Action 0 - predicted reward: tensor([[0.6220]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6254]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7720.0
40. Loss: 63248.5546875
2699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0251]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.3132]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5010.0
42. Loss: 0.07731594145298004
Action 0 - predicted reward: tensor([[-0.3342]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0903]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4440.0
42. Loss: 0.05168507993221283
Action 0 - predicted reward: tensor([[-0.1139]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3293]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4130.0
42. Loss: 0.008750378154218197
Action 0 - predicted reward: tensor([[-3.1535]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.2641]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4245.0
42. Loss: 0.06753358244895935
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0656]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0082]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2835.0
42. Loss: 0.027687368914484978
Action 0 - predicted reward: tensor([[-0.1648]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4782]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4835.0
42. Loss: 0.06332477927207947
Action 0 - predicted reward: tensor([[0.9571]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[8.0837]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3760.0
42. Loss: 0.04143715277314186
Action 0 - predicted reward: tensor([[0.1919]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1353]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4270.0
42. Loss: 0.04388153925538063
Greedy
Action 0 - predicted reward: tensor([[-0.0307]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0124]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2750.0
42. Loss: 0.018683679401874542
Action 0 - predicted reward: tensor([[0.0794]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.6551]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3750.0
42. Loss: 0.06870830059051514
Action 0 - predicted reward: tensor([[-0.1700]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.4163]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3015.0
42. Loss: 0.01639781892299652
Action 0 - predicted reward: tensor([[0.0920]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2485]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3800.0
42. Loss: 0.07027731090784073
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4648]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1697]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6435.0
42. Loss: 55925.08984375
Action 0 - predicted reward: tensor([[-0.2203]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2201]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7430.0
42. Loss: 66667.8515625
Action 0 - predicted reward: tensor([[0.4265]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2125]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5745.0
42. Loss: 49133.8046875
Action 0 - predicted reward: tensor([[-0.1212]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4014]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7825.0
42. Loss: 61689.95703125
2799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.6237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3367]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5195.0
43. Loss: 0.09027419984340668
Action 0 - predicted reward: tensor([[0.1118]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0482]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4525.0
43. Loss: 0.039962247014045715
Action 0 - predicted reward: tensor([[-0.0234]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9965]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4170.0
43. Loss: 0.006272659171372652
Action 0 - predicted reward: tensor([[0.0350]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.7636]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4300.0
43. Loss: 0.03853632137179375
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1017]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.3534]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2905.0
43. Loss: 0.025276679545640945
Action 0 - predicted reward: tensor([[0.2746]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.3405]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4840.0
43. Loss: 0.04226093739271164
Action 0 - predicted reward: tensor([[0.0585]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.1526]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3830.0
43. Loss: 0.02774808369576931
Action 0 - predicted reward: tensor([[0.0553]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.0853]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4410.0
43. Loss: 0.041225824505090714
Greedy
Action 0 - predicted reward: tensor([[-0.0452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9700]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2785.0
43. Loss: 0.015222452580928802
Action 0 - predicted reward: tensor([[-0.1189]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.0594]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3820.0
43. Loss: 0.060405950993299484
Action 0 - predicted reward: tensor([[0.0565]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.1190]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3165.0
43. Loss: 0.02749287523329258
Action 0 - predicted reward: tensor([[-0.5882]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6364]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3905.0
43. Loss: 0.051559269428253174
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.1159]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4728]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6500.0
43. Loss: 54432.88671875
Action 0 - predicted reward: tensor([[0.2116]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2120]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7550.0
43. Loss: 65311.0703125
Action 0 - predicted reward: tensor([[0.3108]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3172]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5795.0
43. Loss: 48016.11328125
Action 0 - predicted reward: tensor([[-0.2072]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2073]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7980.0
43. Loss: 60128.515625
2899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0439]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.5600]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5415.0
45. Loss: 0.09018050879240036
Action 0 - predicted reward: tensor([[-0.2539]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3387]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4635.0
45. Loss: 0.03616850823163986
Action 0 - predicted reward: tensor([[-0.0426]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4066]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4205.0
45. Loss: 0.005218292586505413
Action 0 - predicted reward: tensor([[0.0572]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4455.0
45. Loss: 0.045882485806941986
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0440]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2980.0
45. Loss: 0.024878764525055885
Action 0 - predicted reward: tensor([[0.0797]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0208]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4915.0
45. Loss: 0.043528251349925995
Action 0 - predicted reward: tensor([[-0.4389]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.8030]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 3875.0
45. Loss: 0.04716164991259575
Action 0 - predicted reward: tensor([[0.1413]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.3423]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4555.0
45. Loss: 0.050560738891363144
Greedy
Action 0 - predicted reward: tensor([[0.1443]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0027]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2785.0
45. Loss: 0.013662857934832573
Action 0 - predicted reward: tensor([[-0.0948]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6566]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3895.0
45. Loss: 0.05499658361077309
Action 0 - predicted reward: tensor([[0.1668]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-69.3051]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3250.0
45. Loss: 0.023057352751493454
Action 0 - predicted reward: tensor([[0.0522]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0574]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3940.0
45. Loss: 0.03991968184709549
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.2302]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4165]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6550.0
45. Loss: 53487.33203125
Action 0 - predicted reward: tensor([[0.4831]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4920]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7655.0
45. Loss: 63505.30859375
Action 0 - predicted reward: tensor([[0.8314]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8513]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5875.0
45. Loss: 47460.37890625
Action 0 - predicted reward: tensor([[-0.3475]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9321]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8065.0
45. Loss: 58451.12109375
2999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1116]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.4287]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5695.0
46. Loss: 0.09538355469703674
Action 0 - predicted reward: tensor([[-0.1550]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7879]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4730.0
46. Loss: 0.03487975895404816
Action 0 - predicted reward: tensor([[0.2118]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.0337]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4385.0
46. Loss: 0.03146648034453392
Action 0 - predicted reward: tensor([[-0.4322]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7009]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4630.0
46. Loss: 0.059668947011232376
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1168]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.2860]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3025.0
46. Loss: 0.0266657005995512
Action 0 - predicted reward: tensor([[-0.0856]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1461]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5025.0
46. Loss: 0.04818569868803024
Action 0 - predicted reward: tensor([[0.1821]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.0300]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3915.0
46. Loss: 0.03354107588529587
Action 0 - predicted reward: tensor([[0.1851]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.0424]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4635.0
46. Loss: 0.05147625133395195
Greedy
Action 0 - predicted reward: tensor([[-0.0658]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9470]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2855.0
46. Loss: 0.022946273908019066
Action 0 - predicted reward: tensor([[0.4267]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.0065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3930.0
46. Loss: 0.04690876603126526
Action 0 - predicted reward: tensor([[0.0125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.0049]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3300.0
46. Loss: 0.026431187987327576
Action 0 - predicted reward: tensor([[-0.1006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9699]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3980.0
46. Loss: 0.03712060675024986
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.2440]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2338]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6650.0
46. Loss: 52614.1875
Action 0 - predicted reward: tensor([[0.2616]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1065]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7805.0
46. Loss: 62913.10546875
Action 0 - predicted reward: tensor([[0.2865]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2536]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5985.0
46. Loss: 47068.8359375
Action 0 - predicted reward: tensor([[0.2643]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2512]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8140.0
46. Loss: 56815.2734375
3099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.6950]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5882]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5905.0
48. Loss: 0.09516444802284241
Action 0 - predicted reward: tensor([[-0.0399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1841]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4845.0
48. Loss: 0.036211490631103516
Action 0 - predicted reward: tensor([[-0.0215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0058]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4540.0
48. Loss: 0.04684027284383774
Action 0 - predicted reward: tensor([[-0.5665]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9029]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4745.0
48. Loss: 0.07170230895280838
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4168]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.2133]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3105.0
48. Loss: 0.02950727939605713
Action 0 - predicted reward: tensor([[0.1426]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.4213]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5165.0
48. Loss: 0.055483683943748474
Action 0 - predicted reward: tensor([[-0.9162]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.6926]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3915.0
48. Loss: 0.030039209872484207
Action 0 - predicted reward: tensor([[0.3821]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7657]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4845.0
48. Loss: 0.04675779864192009
Greedy
Action 0 - predicted reward: tensor([[-0.0855]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.3599]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2890.0
48. Loss: 0.016228539869189262
Action 0 - predicted reward: tensor([[-0.4622]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.3117]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3965.0
48. Loss: 0.048311181366443634
Action 0 - predicted reward: tensor([[-0.0287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.3100]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3385.0
48. Loss: 0.029690148308873177
Action 0 - predicted reward: tensor([[-0.0172]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.3061]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4030.0
48. Loss: 0.03347429633140564
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0843]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9954]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6745.0
48. Loss: 51987.36328125
Action 0 - predicted reward: tensor([[0.3204]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2326]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7895.0
48. Loss: 61792.66015625
Action 0 - predicted reward: tensor([[0.7108]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1002]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6000.0
48. Loss: 45807.41796875
Action 0 - predicted reward: tensor([[0.5206]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5213]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8285.0
48. Loss: 55073.46484375
3199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9054]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6085.0
49. Loss: 0.09890375286340714
Action 0 - predicted reward: tensor([[0.1589]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3086]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
49. Loss: 0.04562933370471001
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4575.0
49. Loss: 0.026787253096699715
Action 0 - predicted reward: tensor([[0.2080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.6236]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4890.0
49. Loss: 0.0671839714050293
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1974]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3248]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3180.0
49. Loss: 0.030718835070729256
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5225.0
49. Loss: 0.05328470095992088
Action 0 - predicted reward: tensor([[-0.1212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2729]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3990.0
49. Loss: 0.02968473918735981
Action 0 - predicted reward: tensor([[0.0028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3354]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5055.0
49. Loss: 0.037244006991386414
Greedy
Action 0 - predicted reward: tensor([[-0.0785]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0539]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2960.0
49. Loss: 0.023176763206720352
Action 0 - predicted reward: tensor([[-0.1012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.6844]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4040.0
49. Loss: 0.04531366378068924
Action 0 - predicted reward: tensor([[0.3718]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2761]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3395.0
49. Loss: 0.02131376788020134
Action 0 - predicted reward: tensor([[-0.1273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.7025]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4065.0
49. Loss: 0.0379226990044117
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8108]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3584]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6785.0
49. Loss: 50854.546875
Action 0 - predicted reward: tensor([[0.2602]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0450]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7990.0
49. Loss: 60995.33984375
Action 0 - predicted reward: tensor([[0.9286]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9200]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6060.0
49. Loss: 45195.3984375
Action 0 - predicted reward: tensor([[0.0852]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0652]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8425.0
49. Loss: 54782.71875
3299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[1.0032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0917]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6165.0
51. Loss: 0.10367738455533981
Action 0 - predicted reward: tensor([[0.2974]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3730]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5145.0
51. Loss: 0.047812674194574356
Action 0 - predicted reward: tensor([[-0.4499]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3544]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4610.0
51. Loss: 0.03184639662504196
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5000.0
51. Loss: 0.050813816487789154
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1894]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.7295]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3250.0
51. Loss: 0.02490422874689102
Action 0 - predicted reward: tensor([[0.0563]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8732]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5380.0
51. Loss: 0.06262189149856567
Action 0 - predicted reward: tensor([[0.2759]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.0966]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4030.0
51. Loss: 0.02845694310963154
Action 0 - predicted reward: tensor([[0.7071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[7.9241]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5300.0
51. Loss: 0.041660748422145844
Greedy
Action 0 - predicted reward: tensor([[0.6475]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7883]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2965.0
51. Loss: 0.021206054836511612
Action 0 - predicted reward: tensor([[-0.1265]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1554]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4075.0
51. Loss: 0.043190859258174896
Action 0 - predicted reward: tensor([[-0.2835]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.1030]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3435.0
51. Loss: 0.019686024636030197
Action 0 - predicted reward: tensor([[0.0053]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.1824]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4215.0
51. Loss: 0.040941040962934494
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6946]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7032]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6805.0
51. Loss: 49030.62890625
Action 0 - predicted reward: tensor([[0.6056]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6055]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8095.0
51. Loss: 59631.54296875
Action 0 - predicted reward: tensor([[0.9598]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5969]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6075.0
51. Loss: 44031.953125
Action 0 - predicted reward: tensor([[0.4117]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0869]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8520.0
51. Loss: 53085.0390625
3399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0549]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0806]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6380.0
53. Loss: 0.12216544896364212
Action 0 - predicted reward: tensor([[-0.0345]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1696]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5255.0
53. Loss: 0.05331438407301903
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4720.0
53. Loss: 0.03574550524353981
Action 0 - predicted reward: tensor([[0.3979]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7761]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5120.0
53. Loss: 0.05591995641589165
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0871]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.6645]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3255.0
53. Loss: 0.021992607042193413
Action 0 - predicted reward: tensor([[0.3656]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.9344]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5455.0
53. Loss: 0.06216932833194733
Action 0 - predicted reward: tensor([[-0.2330]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3016]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4070.0
53. Loss: 0.027252018451690674
Action 0 - predicted reward: tensor([[0.0405]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8784]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5370.0
53. Loss: 0.03977049142122269
Greedy
Action 0 - predicted reward: tensor([[-0.0945]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.2201]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3070.0
53. Loss: 0.03563811257481575
Action 0 - predicted reward: tensor([[0.2846]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8634]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4145.0
53. Loss: 0.06180403754115105
Action 0 - predicted reward: tensor([[-0.0425]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.0749]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3550.0
53. Loss: 0.02476656250655651
Action 0 - predicted reward: tensor([[0.1621]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1295]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4255.0
53. Loss: 0.035055480897426605
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4968]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4573]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6805.0
53. Loss: 48405.70703125
Action 0 - predicted reward: tensor([[-0.4437]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4086]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8195.0
53. Loss: 59267.95703125
Action 0 - predicted reward: tensor([[1.2048]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0596]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6120.0
53. Loss: 43311.62890625
Action 0 - predicted reward: tensor([[0.0265]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0308]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8595.0
53. Loss: 51953.58203125
3499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1923]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.7672]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6575.0
54. Loss: 0.10957981646060944
Action 0 - predicted reward: tensor([[0.2379]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5141]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5370.0
54. Loss: 0.05717610940337181
Action 0 - predicted reward: tensor([[0.1687]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.0189]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4865.0
54. Loss: 0.03349298611283302
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5280.0
54. Loss: 0.056056492030620575
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1710]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6023]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3300.0
54. Loss: 0.0207082387059927
Action 0 - predicted reward: tensor([[-0.5205]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.2764]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5665.0
54. Loss: 0.0769958570599556
Action 0 - predicted reward: tensor([[-0.0598]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5220]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4155.0
54. Loss: 0.027819929644465446
Action 0 - predicted reward: tensor([[0.0671]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.9938]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5445.0
54. Loss: 0.03024187870323658
Greedy
Action 0 - predicted reward: tensor([[-0.0933]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2137]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3070.0
54. Loss: 0.03056054189801216
Action 0 - predicted reward: tensor([[-0.1441]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.5526]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4145.0
54. Loss: 0.04469984769821167
Action 0 - predicted reward: tensor([[0.3046]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9991]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3705.0
54. Loss: 0.02197994478046894
Action 0 - predicted reward: tensor([[-0.2589]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1800]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4465.0
54. Loss: 0.048899903893470764
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6005]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1600]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6810.0
54. Loss: 47155.8125
Action 0 - predicted reward: tensor([[0.1654]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1676]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 8460.0
54. Loss: 59792.53515625
Action 0 - predicted reward: tensor([[0.6336]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6839]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6280.0
54. Loss: 43220.5703125
Action 0 - predicted reward: tensor([[0.4681]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1912]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8680.0
54. Loss: 50641.15234375
3599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[1.0760]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.4089]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6680.0
56. Loss: 0.21315668523311615
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5560.0
56. Loss: 0.04435548558831215
Action 0 - predicted reward: tensor([[0.0558]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0982]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5015.0
56. Loss: 0.03600993752479553
Action 0 - predicted reward: tensor([[-0.0380]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0099]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5390.0
56. Loss: 0.05583275854587555
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0590]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.2766]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3375.0
56. Loss: 0.025791266933083534
Action 0 - predicted reward: tensor([[0.1102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.0867]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5820.0
56. Loss: 0.07857775688171387
Action 0 - predicted reward: tensor([[0.0748]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8036]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4225.0
56. Loss: 0.032381389290094376
Action 0 - predicted reward: tensor([[-0.1556]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.5353]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5520.0
56. Loss: 0.02744429185986519
Greedy
Action 0 - predicted reward: tensor([[-0.2027]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9826]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3175.0
56. Loss: 0.029499869793653488
Action 0 - predicted reward: tensor([[-0.1592]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8988]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4180.0
56. Loss: 0.043571554124355316
Action 0 - predicted reward: tensor([[0.1687]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2592]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3750.0
56. Loss: 0.024824215099215508
Action 0 - predicted reward: tensor([[0.1209]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0033]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4505.0
56. Loss: 0.04531155899167061
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0557]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0583]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6815.0
56. Loss: 46161.2578125
Action 0 - predicted reward: tensor([[0.3923]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4160]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8605.0
56. Loss: 59206.62109375
Action 0 - predicted reward: tensor([[1.0527]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1080]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6285.0
56. Loss: 42346.0625
Action 0 - predicted reward: tensor([[0.0325]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0326]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8870.0
56. Loss: 50659.71484375
3699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.2255]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2910]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6890.0
57. Loss: 0.14020636677742004
Action 0 - predicted reward: tensor([[0.1674]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.2249]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5710.0
57. Loss: 0.045593712478876114
Action 0 - predicted reward: tensor([[-0.0308]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1867]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5135.0
57. Loss: 0.035223618149757385
Action 0 - predicted reward: tensor([[0.1277]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4813]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5580.0
57. Loss: 0.054259445518255234
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0053]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.4108]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3480.0
57. Loss: 0.024581877514719963
Action 0 - predicted reward: tensor([[0.3097]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.6940]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5855.0
57. Loss: 0.07209106534719467
Action 0 - predicted reward: tensor([[-0.0475]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3202]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4295.0
57. Loss: 0.028119320049881935
Action 0 - predicted reward: tensor([[-0.0309]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9617]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5520.0
57. Loss: 0.02523447945713997
Greedy
Action 0 - predicted reward: tensor([[0.0570]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0128]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3175.0
57. Loss: 0.022765817120671272
Action 0 - predicted reward: tensor([[0.0955]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.5531]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4215.0
57. Loss: 0.047472190111875534
Action 0 - predicted reward: tensor([[-0.5204]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3801]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3865.0
57. Loss: 0.030971454456448555
Action 0 - predicted reward: tensor([[-0.0728]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.5059]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4580.0
57. Loss: 0.04334103688597679
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8449]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8847]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6850.0
57. Loss: 45376.9765625
Action 0 - predicted reward: tensor([[0.2699]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2699]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8665.0
57. Loss: 58280.60546875
Action 0 - predicted reward: tensor([[1.5590]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0196]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6335.0
57. Loss: 41854.640625
Action 0 - predicted reward: tensor([[-0.2550]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3825]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9045.0
57. Loss: 50037.58203125
3799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0473]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.2339]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7045.0
59. Loss: 0.13764779269695282
Action 0 - predicted reward: tensor([[0.2100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6604]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5750.0
59. Loss: 0.03979794308543205
Action 0 - predicted reward: tensor([[0.0387]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4324]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5210.0
59. Loss: 0.03897608071565628
Action 0 - predicted reward: tensor([[0.1316]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.4182]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5700.0
59. Loss: 0.06076761335134506
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.9991]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3520.0
59. Loss: 0.028726646676659584
Action 0 - predicted reward: tensor([[0.2116]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.6326]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5865.0
59. Loss: 0.06483129411935806
Action 0 - predicted reward: tensor([[-0.1143]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3472]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4300.0
59. Loss: 0.027733923867344856
Action 0 - predicted reward: tensor([[-0.1537]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9461]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5590.0
59. Loss: 0.02496051788330078
Greedy
Action 0 - predicted reward: tensor([[0.0100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.4329]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3210.0
59. Loss: 0.02617677114903927
Action 0 - predicted reward: tensor([[0.1603]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9863]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4325.0
59. Loss: 0.05028175190091133
Action 0 - predicted reward: tensor([[0.3754]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6198]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4035.0
59. Loss: 0.034678731113672256
Action 0 - predicted reward: tensor([[-0.1210]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0417]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4655.0
59. Loss: 0.03817090764641762
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.1329]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4645]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6960.0
59. Loss: 45085.359375
Action 0 - predicted reward: tensor([[-0.2247]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2247]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8755.0
59. Loss: 57585.4375
Action 0 - predicted reward: tensor([[1.2022]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2302]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6405.0
59. Loss: 41672.76953125
Action 0 - predicted reward: tensor([[0.0038]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0037]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 9155.0
59. Loss: 49028.46875
3899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4660]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.4054]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7330.0
60. Loss: 0.1375340223312378
Action 0 - predicted reward: tensor([[0.0997]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2994]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5820.0
60. Loss: 0.04576937481760979
Action 0 - predicted reward: tensor([[0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9915]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5250.0
60. Loss: 0.03636763617396355
Action 0 - predicted reward: tensor([[0.0832]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5527]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5790.0
60. Loss: 0.05762946605682373
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1873]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8779]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3555.0
60. Loss: 0.03273949399590492
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5970.0
60. Loss: 0.06884218007326126
Action 0 - predicted reward: tensor([[-0.3960]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0370]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4335.0
60. Loss: 0.027076100930571556
Action 0 - predicted reward: tensor([[0.1142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9063]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5735.0
60. Loss: 0.04080163687467575
Greedy
Action 0 - predicted reward: tensor([[0.0216]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8407]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3210.0
60. Loss: 0.023877674713730812
Action 0 - predicted reward: tensor([[-0.0891]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6514]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4400.0
60. Loss: 0.04637398198246956
Action 0 - predicted reward: tensor([[-0.0386]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.3526]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4080.0
60. Loss: 0.036534082144498825
Action 0 - predicted reward: tensor([[0.0732]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.6933]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4700.0
60. Loss: 0.03971339762210846
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.3159]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0171]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6960.0
60. Loss: 44247.4765625
Action 0 - predicted reward: tensor([[0.2647]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0937]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8775.0
60. Loss: 56337.4609375
Action 0 - predicted reward: tensor([[1.1361]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1511]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6475.0
60. Loss: 41365.078125
Action 0 - predicted reward: tensor([[0.6943]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7530]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9275.0
60. Loss: 48477.96484375
3999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0974]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.1150]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
62. Loss: 0.12883725762367249
Action 0 - predicted reward: tensor([[0.2743]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6009]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5975.0
62. Loss: 0.04870603233575821
Action 0 - predicted reward: tensor([[0.0699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.6286]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5325.0
62. Loss: 0.04031260684132576
Action 0 - predicted reward: tensor([[0.0721]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.1942]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5875.0
62. Loss: 0.0635179802775383
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1412]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0406]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3635.0
62. Loss: 0.03346538543701172
Action 0 - predicted reward: tensor([[0.0399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.7929]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6015.0
62. Loss: 0.07001720368862152
Action 0 - predicted reward: tensor([[0.0100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.3358]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4335.0
62. Loss: 0.025539051741361618
Action 0 - predicted reward: tensor([[0.1325]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.2689]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5850.0
62. Loss: 0.03820406645536423
Greedy
Action 0 - predicted reward: tensor([[-0.0164]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.1429]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3245.0
62. Loss: 0.02802010253071785
Action 0 - predicted reward: tensor([[0.0686]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6336]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4435.0
62. Loss: 0.04439042881131172
Action 0 - predicted reward: tensor([[-0.3228]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9161]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4115.0
62. Loss: 0.03963521122932434
Action 0 - predicted reward: tensor([[0.2451]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1095]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4840.0
62. Loss: 0.048351358622312546
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.1637]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2064]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6975.0
62. Loss: 43221.375
Action 0 - predicted reward: tensor([[0.5553]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6024]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 8995.0
62. Loss: 56216.640625
Action 0 - predicted reward: tensor([[0.9759]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5622]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6475.0
62. Loss: 40243.109375
Action 0 - predicted reward: tensor([[0.4554]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3393]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9410.0
62. Loss: 47409.1953125
4099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4870]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8544]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7590.0
63. Loss: 0.12378452718257904
Action 0 - predicted reward: tensor([[0.1288]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.5225]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6055.0
63. Loss: 0.04309195652604103
Action 0 - predicted reward: tensor([[0.2019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.0809]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5450.0
63. Loss: 0.039126090705394745
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6140.0
63. Loss: 0.07402218878269196
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0914]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.8060]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3675.0
63. Loss: 0.02967667579650879
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6050.0
63. Loss: 0.0720919817686081
Action 0 - predicted reward: tensor([[0.0806]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.5543]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4340.0
63. Loss: 0.020116953179240227
Action 0 - predicted reward: tensor([[-0.0728]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.6678]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5920.0
63. Loss: 0.03260742500424385
Greedy
Action 0 - predicted reward: tensor([[-0.1496]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9868]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3280.0
63. Loss: 0.026530014351010323
Action 0 - predicted reward: tensor([[0.0892]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.8585]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4470.0
63. Loss: 0.041940346360206604
Action 0 - predicted reward: tensor([[-0.0049]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2736]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4260.0
63. Loss: 0.047400858253240585
Action 0 - predicted reward: tensor([[0.0875]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.8881]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4915.0
63. Loss: 0.049759261310100555
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4958]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1353]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7115.0
63. Loss: 43628.64453125
Action 0 - predicted reward: tensor([[1.0022]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7528]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9075.0
63. Loss: 55885.3515625
Action 0 - predicted reward: tensor([[0.7235]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7271]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6550.0
63. Loss: 39774.5703125
Action 0 - predicted reward: tensor([[0.3518]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1384]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9545.0
63. Loss: 46749.89453125
4199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5183]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2878]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7695.0
63. Loss: 0.10443496704101562
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6095.0
63. Loss: 0.0415717251598835
Action 0 - predicted reward: tensor([[0.0651]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0960]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5575.0
63. Loss: 0.04157911241054535
Action 0 - predicted reward: tensor([[0.1241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.1941]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6255.0
63. Loss: 0.051603201776742935
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0136]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5242]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3725.0
63. Loss: 0.025529326871037483
Action 0 - predicted reward: tensor([[-0.0890]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.5613]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6165.0
63. Loss: 0.07562851160764694
Action 0 - predicted reward: tensor([[0.1042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.1909]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4455.0
63. Loss: 0.02646593749523163
Action 0 - predicted reward: tensor([[0.0661]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.4964]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6060.0
63. Loss: 0.05211947485804558
Greedy
Action 0 - predicted reward: tensor([[-0.0067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.0992]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3280.0
63. Loss: 0.023109523579478264
Action 0 - predicted reward: tensor([[-0.0460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.4227]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4510.0
63. Loss: 0.040498051792383194
Action 0 - predicted reward: tensor([[0.2510]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8644]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4315.0
63. Loss: 0.04255905747413635
Action 0 - predicted reward: tensor([[-0.5821]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0175]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4915.0
63. Loss: 0.043519310653209686
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9976]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0367]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7115.0
63. Loss: 38686.2578125
Action 0 - predicted reward: tensor([[-0.1286]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0917]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9085.0
63. Loss: 53006.28515625
Action 0 - predicted reward: tensor([[1.5021]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1812]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6550.0
63. Loss: 38711.5234375
Action 0 - predicted reward: tensor([[0.1721]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3226]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9620.0
63. Loss: 42945.31640625
4299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.7050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9463]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7765.0
63. Loss: 0.09524231404066086
Action 0 - predicted reward: tensor([[-0.0674]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.7292]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
63. Loss: 0.04649081453680992
Action 0 - predicted reward: tensor([[-0.4484]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9112]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5690.0
63. Loss: 0.04798968508839607
Action 0 - predicted reward: tensor([[-0.0764]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.4897]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6410.0
63. Loss: 0.05258895084261894
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0288]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2443]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3795.0
63. Loss: 0.026771707460284233
Action 0 - predicted reward: tensor([[0.0741]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9887]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6165.0
63. Loss: 0.07110941410064697
Action 0 - predicted reward: tensor([[0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0454]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4525.0
63. Loss: 0.02644077129662037
Action 0 - predicted reward: tensor([[0.0804]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3836]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6165.0
63. Loss: 0.05246061459183693
Greedy
Action 0 - predicted reward: tensor([[-0.1629]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0606]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3350.0
63. Loss: 0.03226722404360771
Action 0 - predicted reward: tensor([[0.1192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0290]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4545.0
63. Loss: 0.04323375225067139
Action 0 - predicted reward: tensor([[0.0396]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.6277]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4360.0
63. Loss: 0.04186582565307617
Action 0 - predicted reward: tensor([[0.0458]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.3850]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4985.0
63. Loss: 0.0444367378950119
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.5093]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3780]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
63. Loss: 37646.7578125
Action 0 - predicted reward: tensor([[0.8142]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8186]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9190.0
63. Loss: 50543.9453125
Action 0 - predicted reward: tensor([[1.1901]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2463]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
63. Loss: 39014.86328125
Action 0 - predicted reward: tensor([[0.5965]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6083]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9675.0
63. Loss: 37471.3828125
4399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.3882]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7880.0
63. Loss: 0.0920199528336525
Action 0 - predicted reward: tensor([[0.1869]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.6989]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6395.0
63. Loss: 0.04088739678263664
Action 0 - predicted reward: tensor([[0.1068]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4822]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5765.0
63. Loss: 0.03869127109646797
Action 0 - predicted reward: tensor([[-0.1573]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.5964]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6485.0
63. Loss: 0.05090605467557907
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1823]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3875.0
63. Loss: 0.03135267645120621
Action 0 - predicted reward: tensor([[0.1222]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0775]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
63. Loss: 0.07052480429410934
Action 0 - predicted reward: tensor([[-0.0265]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.8023]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4545.0
63. Loss: 0.03035018965601921
Action 0 - predicted reward: tensor([[-0.1158]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2248]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6270.0
63. Loss: 0.04451877623796463
Greedy
Action 0 - predicted reward: tensor([[0.0347]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.1098]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3490.0
63. Loss: 0.04343515634536743
Action 0 - predicted reward: tensor([[0.1588]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.9855]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4580.0
63. Loss: 0.04784321039915085
Action 0 - predicted reward: tensor([[-0.0417]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5616]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4435.0
63. Loss: 0.04029097408056259
Action 0 - predicted reward: tensor([[-0.2517]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0100]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4990.0
63. Loss: 0.04213615506887436
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.3328]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3999]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7165.0
63. Loss: 34912.78515625
Action 0 - predicted reward: tensor([[0.1650]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0845]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9255.0
63. Loss: 50098.7109375
Action 0 - predicted reward: tensor([[1.6238]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5118]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6695.0
63. Loss: 36294.78125
Action 0 - predicted reward: tensor([[0.6718]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7864]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 9795.0
63. Loss: 34535.5546875
4499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3974]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.0050]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7955.0
63. Loss: 0.08459944278001785
Action 0 - predicted reward: tensor([[0.0613]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.4368]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6545.0
63. Loss: 0.04537822678685188
Action 0 - predicted reward: tensor([[-0.0650]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.6792]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5765.0
63. Loss: 0.036669399589300156
Action 0 - predicted reward: tensor([[0.0954]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1401]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6630.0
63. Loss: 0.0493047758936882
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1572]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0269]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3955.0
63. Loss: 0.03612571582198143
Action 0 - predicted reward: tensor([[0.0113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.5471]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6315.0
63. Loss: 0.07451866567134857
Action 0 - predicted reward: tensor([[-0.0021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.8360]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4655.0
63. Loss: 0.035632871091365814
Action 0 - predicted reward: tensor([[0.1108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9647]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6380.0
63. Loss: 0.0455254428088665
Greedy
Action 0 - predicted reward: tensor([[-0.1067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.8040]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3525.0
63. Loss: 0.03654699772596359
Action 0 - predicted reward: tensor([[0.0154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.4815]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4615.0
63. Loss: 0.04740043729543686
Action 0 - predicted reward: tensor([[-0.0287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.2624]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4585.0
63. Loss: 0.045232851058244705
Action 0 - predicted reward: tensor([[0.0718]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.3790]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5030.0
63. Loss: 0.044811613857746124
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4508]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5017]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7245.0
63. Loss: 31299.6484375
Action 0 - predicted reward: tensor([[1.1444]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1866]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9415.0
63. Loss: 48753.71875
Action 0 - predicted reward: tensor([[1.1410]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1683]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6730.0
63. Loss: 33275.3671875
Action 0 - predicted reward: tensor([[1.1010]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1008]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 9885.0
63. Loss: 31778.712890625
