Use GPU: False
1.0.1.post2
99.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3846]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4107]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 300.0
Loss: 0.341190367937088
Action 0 - predicted reward: tensor([[-0.9335]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.1849]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 380.0
Loss: 1.2624653577804565
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2095]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 315.0
Loss: 1.3868440389633179
Action 0 - predicted reward: tensor([[-0.5370]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6484]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 255.0
Loss: 0.18097837269306183
Greedy
Action 0 - predicted reward: tensor([[-0.9172]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.0177]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 345.0
Loss: 1.1097607612609863
Action 0 - predicted reward: tensor([[-0.0775]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1096]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 520.0
Loss: 1.5058035850524902
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.4427]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.4614]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 555.0
Loss: 116469.4765625
KL Divergence: 712.0812377929688
Action 0 - predicted reward: tensor([[-0.9936]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0020]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 420.0
Loss: 64921.6953125
KL Divergence: 707.349609375
199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1121]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1392]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 605.0
Loss: 0.20277392864227295
Action 0 - predicted reward: tensor([[0.6782]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.6284]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 650.0
Loss: 0.354926198720932
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4984]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4991]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 465.0
Loss: 0.6135625243186951
Action 0 - predicted reward: tensor([[-0.2867]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4830]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 520.0
Loss: 0.08755506575107574
Greedy
Action 0 - predicted reward: tensor([[0.0652]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.9929]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 590.0
Loss: 0.4216916561126709
Action 0 - predicted reward: tensor([[0.6402]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4734]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 800.0
Loss: 0.10545489937067032
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.4346]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.4488]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 910.0
Loss: 84146.8515625
KL Divergence: 352.3326416015625
Action 0 - predicted reward: tensor([[-0.9439]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9482]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 760.0
Loss: 45972.5390625
KL Divergence: 351.5783996582031
299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0681]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0144]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 865.0
Loss: 0.16807974874973297
Action 0 - predicted reward: tensor([[0.2252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1824]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 860.0
Loss: 0.046943437308073044
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[4.6210]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5476]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 710.0
Loss: 0.3449377715587616
Action 0 - predicted reward: tensor([[0.0145]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 800.0
Loss: 0.11079363524913788
Greedy
Action 0 - predicted reward: tensor([[0.4853]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.5717]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 875.0
Loss: 0.07304886728525162
Action 0 - predicted reward: tensor([[0.1732]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.3361]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1060.0
Loss: 0.040035560727119446
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.9362]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9422]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1190.0
Loss: 58560.578125
KL Divergence: 233.6289825439453
Action 0 - predicted reward: tensor([[-0.5799]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5810]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1005.0
Loss: 34866.94140625
KL Divergence: 232.98825073242188
399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4322]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6034]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1170.0
Loss: 0.14974448084831238
Action 0 - predicted reward: tensor([[0.4385]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.5005]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1175.0
Loss: 0.29419371485710144
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3606]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.0590]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 925.0
Loss: 0.16695398092269897
Action 0 - predicted reward: tensor([[-0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4127]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1065.0
Loss: 0.07109800726175308
Greedy
Action 0 - predicted reward: tensor([[0.1872]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0640]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1125.0
Loss: 0.019275706261396408
Action 0 - predicted reward: tensor([[0.2330]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2559]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1305.0
Loss: 0.0740780308842659
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.1455]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1521]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1675.0
Loss: 67201.421875
KL Divergence: 174.24652099609375
Action 0 - predicted reward: tensor([[-0.3937]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3943]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1255.0
Loss: 29923.09765625
KL Divergence: 173.8901824951172
499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.8054]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.5079]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1445.0
Loss: 0.10737309604883194
Action 0 - predicted reward: tensor([[0.3142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.4248]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1595.0
Loss: 0.4620811641216278
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8714]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1045.0
Loss: 0.08368062227964401
Action 0 - predicted reward: tensor([[0.1556]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1214]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1310.0
Loss: 0.03548535332083702
Greedy
Action 0 - predicted reward: tensor([[0.4487]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.0770]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1260.0
Loss: 0.02036251313984394
Action 0 - predicted reward: tensor([[0.3480]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0299]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1465.0
Loss: 0.16779161989688873
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.0574]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0564]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2095.0
Loss: 66538.71875
KL Divergence: 139.03851318359375
Action 0 - predicted reward: tensor([[-0.1918]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1944]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1470.0
Loss: 27755.81640625
KL Divergence: 138.62371826171875
599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1300]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6101]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1780.0
Loss: 0.12421348690986633
Action 0 - predicted reward: tensor([[1.5640]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7893]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1955.0
Loss: 0.3256474733352661
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1635]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.1028]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1300.0
Loss: 0.15507231652736664
Action 0 - predicted reward: tensor([[0.0588]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1043]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1555.0
Loss: 0.00690943468362093
Greedy
Action 0 - predicted reward: tensor([[-0.5235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.2415]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1420.0
Loss: 0.07950563728809357
Action 0 - predicted reward: tensor([[-0.1216]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.7902]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1640.0
Loss: 0.11718708276748657
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.1140]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1171]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2470.0
Loss: 66192.3671875
KL Divergence: 115.68748474121094
Action 0 - predicted reward: tensor([[-0.1132]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1127]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1740.0
Loss: 28109.009765625
KL Divergence: 115.18915557861328
699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2619]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1240]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1995.0
Loss: 0.04381299763917923
Action 0 - predicted reward: tensor([[0.2629]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.4551]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2080.0
Loss: 0.14534339308738708
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.2504]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1485.0
Loss: 0.10133661329746246
Action 0 - predicted reward: tensor([[0.0490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1850.0
Loss: 0.031692177057266235
Greedy
Action 0 - predicted reward: tensor([[0.3569]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1702]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1655.0
Loss: 0.18767407536506653
Action 0 - predicted reward: tensor([[-0.2062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2436]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1755.0
Loss: 0.16781112551689148
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.1242]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1238]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2900.0
Loss: 68799.234375
KL Divergence: 98.97110748291016
Action 0 - predicted reward: tensor([[-0.0600]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0605]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1995.0
Loss: 27577.193359375
KL Divergence: 98.64691925048828
799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2740]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3427]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2260.0
Loss: 0.017820701003074646
Action 0 - predicted reward: tensor([[0.0701]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.0737]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2315.0
Loss: 0.35159435868263245
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.8146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5397]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1575.0
Loss: 0.01068627368658781
Action 0 - predicted reward: tensor([[-0.0062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0426]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2130.0
Loss: 0.10725904256105423
Greedy
Action 0 - predicted reward: tensor([[0.3897]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7161]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1850.0
Loss: 0.13382259011268616
Action 0 - predicted reward: tensor([[0.4194]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3553]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1955.0
Loss: 0.17611750960350037
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.9912]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9986]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3170.0
Loss: 65248.88671875
KL Divergence: 86.30301666259766
Action 0 - predicted reward: tensor([[0.0332]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0330]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2215.0
Loss: 25378.287109375
KL Divergence: 86.15252685546875
899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-2.1347]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4715]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2515.0
Loss: 0.10167425125837326
Action 0 - predicted reward: tensor([[-0.7124]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.1713]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2575.0
Loss: 0.11183275282382965
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.5408]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.3131]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1695.0
Loss: 0.0030187335796654224
Action 0 - predicted reward: tensor([[0.1576]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1295]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2410.0
Loss: 0.11391758918762207
Greedy
Action 0 - predicted reward: tensor([[-0.1848]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4964]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1885.0
Loss: 0.07586687058210373
Action 0 - predicted reward: tensor([[0.1582]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8202]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2035.0
Loss: 0.08468654751777649
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.9272]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9232]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3525.0
Loss: 65939.53125
KL Divergence: 76.44366455078125
Action 0 - predicted reward: tensor([[0.0582]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0580]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2470.0
Loss: 27410.2578125
KL Divergence: 76.41639709472656
999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0763]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8926]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2870.0
Loss: 0.06232104077935219
Action 0 - predicted reward: tensor([[0.0800]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9139]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2655.0
Loss: 0.046963565051555634
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.2514]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1740.0
Loss: 0.0010438618483021855
Action 0 - predicted reward: tensor([[0.0562]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7810]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2690.0
Loss: 0.11731584370136261
Greedy
Action 0 - predicted reward: tensor([[0.1538]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.5024]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1940.0
Loss: 0.043048642575740814
Action 0 - predicted reward: tensor([[-0.0795]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.1330]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2120.0
Loss: 0.058201052248477936
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.7479]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7518]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3710.0
Loss: 60952.84375
KL Divergence: 68.639892578125
Action 0 - predicted reward: tensor([[0.0408]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0438]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2735.0
Loss: 27873.6171875
KL Divergence: 68.65644073486328
1099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1798]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.4218]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3045.0
Loss: 0.07475098967552185
Action 0 - predicted reward: tensor([[-0.0308]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9670]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2810.0
Loss: 0.06672191619873047
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2120]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.9537]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1780.0
Loss: 0.023356039077043533
Action 0 - predicted reward: tensor([[-0.3146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.1717]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2940.0
Loss: 0.06876083463430405
Greedy
Action 0 - predicted reward: tensor([[0.0819]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2358]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1975.0
Loss: 0.032819103449583054
Action 0 - predicted reward: tensor([[0.2416]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6534]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2235.0
Loss: 0.06974891573190689
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.5896]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5890]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3920.0
Loss: 56285.94921875
KL Divergence: 62.31022644042969
Action 0 - predicted reward: tensor([[-0.0902]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0929]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3155.0
Loss: 32883.78125
KL Divergence: 62.28575897216797
1199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0248]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.5625]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3170.0
Loss: 0.038029663264751434
Action 0 - predicted reward: tensor([[0.4882]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2554]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2970.0
Loss: 0.055033087730407715
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1862]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7714]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1855.0
Loss: 0.011927776038646698
Action 0 - predicted reward: tensor([[-0.2786]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0656]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 3185.0
Loss: 0.0808611735701561
Greedy
Action 0 - predicted reward: tensor([[-0.2669]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7658]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2045.0
Loss: 0.02389044687151909
Action 0 - predicted reward: tensor([[-0.2648]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6516]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2350.0
Loss: 0.07780325412750244
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.4788]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4903]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4095.0
Loss: 52498.078125
KL Divergence: 56.939537048339844
Action 0 - predicted reward: tensor([[-0.0960]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0987]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3395.0
Loss: 31806.24609375
KL Divergence: 57.064849853515625
1299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5787]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.6855]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3370.0
Loss: 0.047623056918382645
Action 0 - predicted reward: tensor([[-0.2987]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0223]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3080.0
Loss: 0.06827066838741302
Epsilon Greedy 1%
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1935.0
Loss: 0.0045603374019265175
Action 0 - predicted reward: tensor([[0.1600]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1048]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3320.0
Loss: 0.03603082150220871
Greedy
Action 0 - predicted reward: tensor([[1.1740]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.8441]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2045.0
Loss: 0.0328594334423542
Action 0 - predicted reward: tensor([[0.2499]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.2279]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2435.0
Loss: 0.07743773609399796
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.4868]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4999]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4310.0
Loss: 49986.2265625
KL Divergence: 52.55717086791992
Action 0 - predicted reward: tensor([[-0.1527]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1537]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3755.0
Loss: 35031.98828125
KL Divergence: 52.58715057373047
1399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1602]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0422]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3465.0
Loss: 0.03627406433224678
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3225.0
Loss: 0.09388607740402222
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0608]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2045.0
Loss: 0.009691379964351654
Action 0 - predicted reward: tensor([[-0.0607]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.3908]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3415.0
Loss: 0.013000800274312496
Greedy
Action 0 - predicted reward: tensor([[0.2508]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.3964]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2080.0
Loss: 0.012500581331551075
Action 0 - predicted reward: tensor([[0.6492]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.1261]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2520.0
Loss: 0.03195164352655411
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3532]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3518]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4450.0
Loss: 47176.52734375
KL Divergence: 48.68313217163086
Action 0 - predicted reward: tensor([[-0.1244]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1253]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4010.0
Loss: 34177.12890625
KL Divergence: 48.813865661621094
1499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.2236]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3580.0
Loss: 0.06876461952924728
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3370.0
Loss: 0.10005148500204086
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1392]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.1182]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2165.0
Loss: 0.0006701954989694059
Action 0 - predicted reward: tensor([[0.0316]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6961]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3505.0
Loss: 0.03441895917057991
Greedy
Action 0 - predicted reward: tensor([[-0.2234]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7538]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2120.0
Loss: 0.010349665768444538
Action 0 - predicted reward: tensor([[-0.1491]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9972]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2555.0
Loss: 0.037899866700172424
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.2694]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2910]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4600.0
Loss: 44338.01171875
KL Divergence: 45.37342834472656
Action 0 - predicted reward: tensor([[-0.1235]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1261]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4280.0
Loss: 34425.84765625
KL Divergence: 45.499568939208984
1599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0248]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.1064]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3770.0
Loss: 0.07391849160194397
Action 0 - predicted reward: tensor([[-0.8442]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1661]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3480.0
Loss: 0.08801096677780151
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0437]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8749]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2320.0
Loss: 0.027161812409758568
Action 0 - predicted reward: tensor([[0.0478]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6726]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3675.0
Loss: 0.05821811035275459
Greedy
Action 0 - predicted reward: tensor([[0.4306]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5206]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2230.0
Loss: 0.02069401554763317
Action 0 - predicted reward: tensor([[0.2416]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.1573]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2640.0
Loss: 0.044633716344833374
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.1852]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1829]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4730.0
Loss: 41952.76171875
KL Divergence: 42.529296875
Action 0 - predicted reward: tensor([[-0.1170]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1150]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4470.0
Loss: 33146.74609375
KL Divergence: 42.60036087036133
1699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0278]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.5589]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3860.0
Loss: 0.08812951296567917
Action 0 - predicted reward: tensor([[-0.2768]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.8927]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3590.0
Loss: 0.05306650325655937
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4401]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2565.0
Loss: 0.02531842142343521
Action 0 - predicted reward: tensor([[0.0859]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7479]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3735.0
Loss: 0.02064131200313568
Greedy
Action 0 - predicted reward: tensor([[-0.8472]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.7407]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2265.0
Loss: 0.023344149813055992
Action 0 - predicted reward: tensor([[1.1222]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4341]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2815.0
Loss: 0.29578644037246704
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.1037]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1050]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4825.0
Loss: 39458.859375
KL Divergence: 39.97030258178711
Action 0 - predicted reward: tensor([[-0.0277]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0334]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4585.0
Loss: 31181.716796875
KL Divergence: 40.03985595703125
1799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.8132]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.6408]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3910.0
Loss: 0.05464685708284378
Action 0 - predicted reward: tensor([[-0.0089]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.5832]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3630.0
Loss: 0.03377850726246834
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2367]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0483]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2675.0
Loss: 0.023656519129872322
Action 0 - predicted reward: tensor([[-0.1330]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1917]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3815.0
Loss: 0.021583277732133865
Greedy
Action 0 - predicted reward: tensor([[0.1311]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.8145]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2275.0
Loss: 0.012064926326274872
Action 0 - predicted reward: tensor([[-2.3119]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.2451]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2820.0
Loss: 0.3770337998867035
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.0583]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0583]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4950.0
Loss: 37901.06640625
KL Divergence: 37.76866912841797
Action 0 - predicted reward: tensor([[0.0290]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0244]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4760.0
Loss: 29514.58984375
KL Divergence: 37.782371520996094
1899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.5030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4853]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4050.0
Loss: 0.06187085062265396
Action 0 - predicted reward: tensor([[0.0219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1910]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3775.0
Loss: 0.06575915962457657
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1693]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.5787]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2815.0
Loss: 0.024637371301651
Action 0 - predicted reward: tensor([[-0.1268]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6928]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3975.0
Loss: 0.018441801890730858
Greedy
Action 0 - predicted reward: tensor([[-0.1022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.2341]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2320.0
Loss: 0.010976383462548256
Action 0 - predicted reward: tensor([[0.6528]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.9627]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2825.0
Loss: 0.06289035826921463
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.0604]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0658]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5045.0
Loss: 36279.421875
KL Divergence: 35.73915481567383
Action 0 - predicted reward: tensor([[0.0978]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0989]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4985.0
Loss: 29078.580078125
KL Divergence: 35.73468780517578
1999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.9968]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.5563]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4175.0
Loss: 0.06255902349948883
Action 0 - predicted reward: tensor([[-0.2694]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4450]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3935.0
Loss: 0.058066532015800476
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4167]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0313]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2885.0
Loss: 0.02715083211660385
Action 0 - predicted reward: tensor([[-0.3711]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.2318]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4025.0
Loss: 0.01631588488817215
Greedy
Action 0 - predicted reward: tensor([[0.2125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.4444]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2320.0
Loss: 0.0091549726203084
Action 0 - predicted reward: tensor([[0.0266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.6033]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3000.0
Loss: 0.09022488445043564
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.1528]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1536]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5125.0
Loss: 34799.21875
KL Divergence: 33.92110824584961
Action 0 - predicted reward: tensor([[0.0780]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0788]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5195.0
Loss: 27803.30859375
KL Divergence: 33.919212341308594
2099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0516]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2550]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4235.0
Loss: 0.057055000215768814
Action 0 - predicted reward: tensor([[0.2302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4864]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4095.0
Loss: 0.05227800831198692
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0188]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9493]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2990.0
Loss: 0.025830883532762527
Action 0 - predicted reward: tensor([[0.0115]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.5450]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4110.0
Loss: 0.016980528831481934
Greedy
Action 0 - predicted reward: tensor([[-0.0251]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.9996]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2355.0
Loss: 0.00799498800188303
Action 0 - predicted reward: tensor([[0.3072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.4124]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3035.0
Loss: 0.06817510724067688
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.2564]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2694]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5150.0
Loss: 33504.109375
KL Divergence: 32.29533386230469
Action 0 - predicted reward: tensor([[0.1684]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1693]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5340.0
Loss: 27372.265625
KL Divergence: 32.297367095947266
2199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4718]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1036]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4365.0
Loss: 0.05224042013287544
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4285.0
Loss: 0.044495537877082825
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1390]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9295]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3045.0
Loss: 0.01540316455066204
Action 0 - predicted reward: tensor([[-0.1527]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0726]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4225.0
Loss: 0.01941050961613655
Greedy
Action 0 - predicted reward: tensor([[-0.0668]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.5344]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2395.0
Loss: 0.017304478213191032
Action 0 - predicted reward: tensor([[-0.1243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.0748]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3115.0
Loss: 0.06536746770143509
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.3377]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3421]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5170.0
Loss: 32172.083984375
KL Divergence: 30.793163299560547
Action 0 - predicted reward: tensor([[0.2249]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2251]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5495.0
Loss: 26404.8984375
KL Divergence: 30.791669845581055
2299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5381]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.0948]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4490.0
Loss: 0.040497150272130966
Action 0 - predicted reward: tensor([[0.3062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.0855]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4590.0
Loss: 0.07217182964086533
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3388]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9542]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3090.0
Loss: 0.01887074112892151
Action 0 - predicted reward: tensor([[-0.0550]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8911]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4335.0
Loss: 0.017392318695783615
Greedy
Action 0 - predicted reward: tensor([[-0.0687]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.0440]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2395.0
Loss: 0.01636474020779133
Action 0 - predicted reward: tensor([[0.4085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.5559]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3150.0
Loss: 0.05992751941084862
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4221]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2725]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5195.0
Loss: 31518.890625
KL Divergence: 29.465625762939453
Action 0 - predicted reward: tensor([[0.2495]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2348]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5600.0
Loss: 25427.9375
KL Divergence: 29.394018173217773
2399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3985]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7423]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4680.0
Loss: 0.036085184663534164
Action 0 - predicted reward: tensor([[0.0079]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.0735]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4730.0
Loss: 0.08197705447673798
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0306]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0998]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3115.0
Loss: 0.013058099895715714
Action 0 - predicted reward: tensor([[0.2771]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.2519]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4525.0
Loss: 0.015225459821522236
Greedy
Action 0 - predicted reward: tensor([[-0.2048]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9546]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2395.0
Loss: 0.015871845185756683
Action 0 - predicted reward: tensor([[0.2894]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.1592]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3290.0
Loss: 0.07090385258197784
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.5315]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5444]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5235.0
Loss: 30504.94921875
KL Divergence: 28.190195083618164
Action 0 - predicted reward: tensor([[0.2932]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2702]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5720.0
Loss: 24201.5390625
KL Divergence: 28.155393600463867
2499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0881]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7143]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4755.0
Loss: 0.042197782546281815
Action 0 - predicted reward: tensor([[0.4879]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.1134]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4835.0
Loss: 0.0618978813290596
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0468]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1340]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3290.0
Loss: 0.02396596036851406
Action 0 - predicted reward: tensor([[0.1651]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.9511]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4640.0
Loss: 0.01762278564274311
Greedy
Action 0 - predicted reward: tensor([[0.0460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.0154]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2435.0
Loss: 0.0211891308426857
Action 0 - predicted reward: tensor([[-0.0013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.4823]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3340.0
Loss: 0.05895659327507019
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.5898]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6299]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5280.0
Loss: 30080.20703125
KL Divergence: 27.070886611938477
Action 0 - predicted reward: tensor([[0.3658]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3482]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5810.0
Loss: 24000.986328125
KL Divergence: 27.00871467590332
2599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4625]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0201]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4875.0
Loss: 0.04108289256691933
Action 0 - predicted reward: tensor([[0.1162]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5674]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4905.0
Loss: 0.05474075302481651
Epsilon Greedy 1%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3400.0
Loss: 0.02241080068051815
Action 0 - predicted reward: tensor([[0.0667]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9141]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4720.0
Loss: 0.032054997980594635
Greedy
Action 0 - predicted reward: tensor([[0.1010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.5695]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2470.0
Loss: 0.02447444573044777
Action 0 - predicted reward: tensor([[-0.1490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8312]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3385.0
Loss: 0.053123150020837784
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6577]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4617]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5320.0
Loss: 29551.12890625
KL Divergence: 26.024600982666016
Action 0 - predicted reward: tensor([[0.4200]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4254]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5890.0
Loss: 23110.681640625
KL Divergence: 25.964487075805664
2699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1053]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2047]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5035.0
Loss: 0.0470721535384655
Action 0 - predicted reward: tensor([[0.1464]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0937]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5125.0
Loss: 0.05452711135149002
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0728]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0489]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3440.0
Loss: 0.019648561254143715
Action 0 - predicted reward: tensor([[0.1447]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1414]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4730.0
Loss: 0.023136481642723083
Greedy
Action 0 - predicted reward: tensor([[0.0489]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6394]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2470.0
Loss: 0.019358772784471512
Action 0 - predicted reward: tensor([[-0.0254]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0491]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3530.0
Loss: 0.0602235421538353
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6746]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7152]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5435.0
Loss: 30144.572265625
KL Divergence: 25.02639389038086
Action 0 - predicted reward: tensor([[0.4668]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4280]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5940.0
Loss: 22668.919921875
KL Divergence: 24.965885162353516
2799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.6358]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.1746]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5075.0
Loss: 0.04256009683012962
Action 0 - predicted reward: tensor([[0.4673]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7186]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5315.0
Loss: 0.05351583659648895
Epsilon Greedy 1%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3565.0
Loss: 0.027419624850153923
Action 0 - predicted reward: tensor([[-0.1214]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0589]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4845.0
Loss: 0.03557214513421059
Greedy
Action 0 - predicted reward: tensor([[0.2142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.5447]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2510.0
Loss: 0.02041739784181118
Action 0 - predicted reward: tensor([[-0.0473]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3684]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3605.0
Loss: 0.05684885382652283
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7424]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7474]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5520.0
Loss: 30174.314453125
KL Divergence: 24.093591690063477
Action 0 - predicted reward: tensor([[0.5086]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4426]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6000.0
Loss: 22106.498046875
KL Divergence: 24.057050704956055
2899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1509]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.2002]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5145.0
Loss: 0.036228179931640625
Action 0 - predicted reward: tensor([[-0.3081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3530]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5425.0
Loss: 0.0639936625957489
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1118]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-81.7719]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3675.0
Loss: 0.03619564697146416
Action 0 - predicted reward: tensor([[0.2630]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.1759]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5030.0
Loss: 0.04001455381512642
Greedy
Action 0 - predicted reward: tensor([[0.2342]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9935]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2510.0
Loss: 0.01659099943935871
Action 0 - predicted reward: tensor([[-0.5653]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.6296]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3695.0
Loss: 0.05373893305659294
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7945]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5668]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5525.0
Loss: 29420.26171875
KL Divergence: 23.252180099487305
Action 0 - predicted reward: tensor([[0.5915]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5122]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6045.0
Loss: 21628.15625
KL Divergence: 23.23540687561035
2999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2321]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4121]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5265.0
Loss: 0.03828629478812218
Action 0 - predicted reward: tensor([[-0.0664]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.7040]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5615.0
Loss: 0.062299348413944244
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0282]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9449]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3750.0
Loss: 0.034949321299791336
Action 0 - predicted reward: tensor([[-0.0204]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.6073]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5100.0
Loss: 0.04314713180065155
Greedy
Action 0 - predicted reward: tensor([[0.0051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.9165]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2545.0
Loss: 0.020396489650011063
Action 0 - predicted reward: tensor([[-0.0661]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0591]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3800.0
Loss: 0.06095249950885773
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8162]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5826]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5525.0
Loss: 28689.984375
KL Divergence: 22.44005012512207
Action 0 - predicted reward: tensor([[0.6116]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6362]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6060.0
Loss: 21148.75
KL Divergence: 22.444841384887695
3099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0480]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.1441]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5415.0
Loss: 0.0489487387239933
Action 0 - predicted reward: tensor([[0.0939]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-66.8980]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5630.0
Loss: 0.051642365753650665
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0362]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8844]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3790.0
Loss: 0.04137517139315605
Action 0 - predicted reward: tensor([[-0.1450]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0110]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 5255.0
Loss: 0.040157075971364975
Greedy
Action 0 - predicted reward: tensor([[-0.0858]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9139]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2550.0
Loss: 0.02047351561486721
Action 0 - predicted reward: tensor([[-0.0758]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.8230]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3875.0
Loss: 0.0557258315384388
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8373]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4737]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5640.0
Loss: 29128.19921875
KL Divergence: 21.722232818603516
Action 0 - predicted reward: tensor([[0.6579]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5419]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6130.0
Loss: 21076.0390625
KL Divergence: 21.733247756958008
3199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4721]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6720]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5615.0
Loss: 0.04371761158108711
Action 0 - predicted reward: tensor([[0.2889]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.0112]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5850.0
Loss: 0.06750458478927612
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0441]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.8000]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3895.0
Loss: 0.04212687537074089
Action 0 - predicted reward: tensor([[-0.0861]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8793]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5305.0
Loss: 0.03204701095819473
Greedy
Action 0 - predicted reward: tensor([[0.3401]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.1426]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2550.0
Loss: 0.019537556916475296
Action 0 - predicted reward: tensor([[-0.2329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.7813]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4020.0
Loss: 0.06101935729384422
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9185]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6943]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5675.0
Loss: 28863.494140625
KL Divergence: 21.0142765045166
Action 0 - predicted reward: tensor([[0.6847]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4996]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6140.0
Loss: 20641.984375
KL Divergence: 21.021635055541992
3299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4705]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5695.0
Loss: 0.035324472934007645
Action 0 - predicted reward: tensor([[-0.1085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.4198]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5905.0
Loss: 0.05484345927834511
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2443]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0162]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3945.0
Loss: 0.03884283825755119
Action 0 - predicted reward: tensor([[-0.0512]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1869]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5340.0
Loss: 0.03213252127170563
Greedy
Action 0 - predicted reward: tensor([[0.0629]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.1334]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2585.0
Loss: 0.01898426190018654
Action 0 - predicted reward: tensor([[0.0172]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.0645]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4095.0
Loss: 0.0579398050904274
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9603]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8414]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5675.0
Loss: 28191.732421875
KL Divergence: 20.36264419555664
Action 0 - predicted reward: tensor([[0.7588]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7933]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6145.0
Loss: 20065.916015625
KL Divergence: 20.379541397094727
3399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2846]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3336]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5730.0
Loss: 0.033012304455041885
Action 0 - predicted reward: tensor([[0.2603]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.8296]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6025.0
Loss: 0.06059635058045387
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.6904]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4015.0
Loss: 0.034381940960884094
Action 0 - predicted reward: tensor([[0.1728]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-76.5290]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5430.0
Loss: 0.04146681725978851
Greedy
Action 0 - predicted reward: tensor([[0.0162]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8966]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2620.0
Loss: 0.020736010745167732
Action 0 - predicted reward: tensor([[0.0367]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8998]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4095.0
Loss: 0.05020071193575859
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9971]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8764]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5710.0
Loss: 27969.703125
KL Divergence: 19.759626388549805
Action 0 - predicted reward: tensor([[0.7932]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8477]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6190.0
Loss: 20184.32421875
KL Divergence: 19.764415740966797
3499.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5865.0
Loss: 0.03492029756307602
Action 0 - predicted reward: tensor([[0.1713]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5907]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6150.0
Loss: 0.07263677567243576
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0233]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.4031]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4200.0
Loss: 0.04594457894563675
Action 0 - predicted reward: tensor([[-0.2028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.3779]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5515.0
Loss: 0.0510275699198246
Greedy
Action 0 - predicted reward: tensor([[-0.1905]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8140]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2625.0
Loss: 0.01746532879769802
Action 0 - predicted reward: tensor([[0.0040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4654]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4095.0
Loss: 0.04708314314484596
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0229]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9978]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5780.0
Loss: 27926.474609375
KL Divergence: 19.176332473754883
Action 0 - predicted reward: tensor([[0.7771]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8414]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6260.0
Loss: 20277.708984375
KL Divergence: 19.18366813659668
3599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.6672]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.0191]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6050.0
Loss: 0.03491852059960365
Action 0 - predicted reward: tensor([[-0.1960]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.0489]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6230.0
Loss: 0.07667549699544907
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.6666]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8969]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4200.0
Loss: 0.047045573592185974
Action 0 - predicted reward: tensor([[-0.2747]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5511]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5605.0
Loss: 0.05285342410206795
Greedy
Action 0 - predicted reward: tensor([[-0.0699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9647]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2660.0
Loss: 0.016589215025305748
Action 0 - predicted reward: tensor([[0.1880]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5073]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4135.0
Loss: 0.04886212572455406
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0826]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1541]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5820.0
Loss: 27792.865234375
KL Divergence: 18.643056869506836
Action 0 - predicted reward: tensor([[0.8733]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9281]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6305.0
Loss: 20226.84375
KL Divergence: 18.6571044921875
3699.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6270.0
Loss: 0.05202552303671837
Action 0 - predicted reward: tensor([[0.0083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.8027]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6285.0
Loss: 0.0650186538696289
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0643]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4200.0
Loss: 0.033428508788347244
Action 0 - predicted reward: tensor([[-0.0521]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6872]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5650.0
Loss: 0.0440920889377594
Greedy
Action 0 - predicted reward: tensor([[-0.0444]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4229]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2695.0
Loss: 0.01642913743853569
Action 0 - predicted reward: tensor([[-0.1932]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.6073]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4210.0
Loss: 0.04574669897556305
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0602]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4626]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5895.0
Loss: 27797.759765625
KL Divergence: 18.118345260620117
Action 0 - predicted reward: tensor([[0.9218]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5756]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6325.0
Loss: 20062.935546875
KL Divergence: 18.142641067504883
3799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.9445]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6415.0
Loss: 0.054507263004779816
Action 0 - predicted reward: tensor([[0.1514]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6365]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6360.0
Loss: 0.06559396535158157
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0833]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9348]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4235.0
Loss: 0.03734443336725235
Action 0 - predicted reward: tensor([[0.5179]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.0484]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5690.0
Loss: 0.04174015298485756
Greedy
Action 0 - predicted reward: tensor([[0.2171]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4307]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2695.0
Loss: 0.01627899520099163
Action 0 - predicted reward: tensor([[-0.0519]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2741]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4210.0
Loss: 0.04255927354097366
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0873]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1475]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5895.0
Loss: 27113.484375
KL Divergence: 17.628646850585938
Action 0 - predicted reward: tensor([[0.9954]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0695]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6335.0
Loss: 19801.693359375
KL Divergence: 17.65039825439453
3899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3417]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5291]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6605.0
Loss: 0.04905670881271362
Action 0 - predicted reward: tensor([[-0.0107]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1190]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6515.0
Loss: 0.07185295224189758
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0027]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.3770]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4240.0
Loss: 0.03561686351895332
Action 0 - predicted reward: tensor([[0.1387]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.7752]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5800.0
Loss: 0.04141265153884888
Greedy
Action 0 - predicted reward: tensor([[-0.5128]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0661]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2765.0
Loss: 0.01782993972301483
Action 0 - predicted reward: tensor([[-0.0858]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7664]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4315.0
Loss: 0.05597727373242378
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.1606]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0422]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5895.0
Loss: 26755.306640625
KL Divergence: 17.186063766479492
Action 0 - predicted reward: tensor([[0.9556]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8644]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6390.0
Loss: 19751.505859375
KL Divergence: 17.184907913208008
3999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0733]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9607]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6745.0
Loss: 0.05924012139439583
Action 0 - predicted reward: tensor([[0.2582]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7180]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6740.0
Loss: 0.06378506869077682
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.5716]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4240.0
Loss: 0.034869130700826645
Action 0 - predicted reward: tensor([[0.0296]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9146]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6020.0
Loss: 0.046805281192064285
Greedy
Action 0 - predicted reward: tensor([[-0.2288]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3784]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2765.0
Loss: 0.015135830268263817
Action 0 - predicted reward: tensor([[-0.2544]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9299]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4385.0
Loss: 0.056416045874357224
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.1902]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2473]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5895.0
Loss: 26160.111328125
KL Divergence: 16.744243621826172
Action 0 - predicted reward: tensor([[1.0439]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1091]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6430.0
Loss: 19743.142578125
KL Divergence: 16.754701614379883
4099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4903]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5681]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6935.0
Loss: 0.052028924226760864
Action 0 - predicted reward: tensor([[-0.0318]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.1625]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6855.0
Loss: 0.05991450324654579
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1426]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4315.0
Loss: 0.03741572052240372
Action 0 - predicted reward: tensor([[-0.1137]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.7602]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6160.0
Loss: 0.052008986473083496
Greedy
Action 0 - predicted reward: tensor([[0.0276]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0406]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2770.0
Loss: 0.014641834422945976
Action 0 - predicted reward: tensor([[-0.0531]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6432]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4420.0
Loss: 0.05017824098467827
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.2449]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3163]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5930.0
Loss: 26152.794921875
KL Divergence: 16.357955932617188
Action 0 - predicted reward: tensor([[1.1026]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1680]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6465.0
Loss: 19210.48828125
KL Divergence: 16.369985580444336
4199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0419]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.3815]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7115.0
Loss: 0.058604896068573
Action 0 - predicted reward: tensor([[-0.0890]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.7245]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7080.0
Loss: 0.0656498596072197
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.9496]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4325.0
Loss: 0.033478833734989166
Action 0 - predicted reward: tensor([[0.6056]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3232]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 0.05572808161377907
Greedy
Action 0 - predicted reward: tensor([[-0.0196]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0085]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2770.0
Loss: 0.011230478063225746
Action 0 - predicted reward: tensor([[0.2406]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3384]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4500.0
Loss: 0.056078363209962845
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.3015]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8646]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5930.0
Loss: 22784.25390625
KL Divergence: 16.34697151184082
Action 0 - predicted reward: tensor([[1.1504]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2176]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6510.0
Loss: 17901.318359375
KL Divergence: 16.363100051879883
4299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3495]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3435]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7305.0
Loss: 0.05538620427250862
Action 0 - predicted reward: tensor([[0.1173]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.2745]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7120.0
Loss: 0.06284580379724503
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0444]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.0826]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4325.0
Loss: 0.03279608115553856
Action 0 - predicted reward: tensor([[-0.5195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.3729]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
Loss: 0.051656484603881836
Greedy
Action 0 - predicted reward: tensor([[-0.0513]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.6220]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2840.0
Loss: 0.017034774646162987
Action 0 - predicted reward: tensor([[0.1020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.9283]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4555.0
Loss: 0.05597987398505211
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4262]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4976]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5930.0
Loss: 22058.1171875
KL Divergence: 16.34769058227539
Action 0 - predicted reward: tensor([[1.1788]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2559]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6615.0
Loss: 18622.423828125
KL Divergence: 16.345970153808594
4399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.6775]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6060]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7415.0
Loss: 0.05646846815943718
Action 0 - predicted reward: tensor([[-0.1063]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.7013]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7260.0
Loss: 0.07479364424943924
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0717]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9878]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4325.0
Loss: 0.032424211502075195
Action 0 - predicted reward: tensor([[0.0133]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.4492]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6315.0
Loss: 0.05475999414920807
Greedy
Action 0 - predicted reward: tensor([[-0.2783]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.4013]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2915.0
Loss: 0.020797021687030792
Action 0 - predicted reward: tensor([[-0.6599]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.2148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4660.0
Loss: 0.05350087583065033
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4698]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5458]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5965.0
Loss: 22133.916015625
KL Divergence: 16.345191955566406
Action 0 - predicted reward: tensor([[1.2680]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7789]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 18732.677734375
KL Divergence: 16.3302001953125
4499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5871]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.2735]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7490.0
Loss: 0.050976209342479706
Action 0 - predicted reward: tensor([[-0.1734]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.2576]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7315.0
Loss: 0.06660699099302292
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0855]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.6453]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4365.0
Loss: 0.0332164466381073
Action 0 - predicted reward: tensor([[0.0832]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0664]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6390.0
Loss: 0.05538196116685867
Greedy
Action 0 - predicted reward: tensor([[-0.0826]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.7558]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2950.0
Loss: 0.023063939064741135
Action 0 - predicted reward: tensor([[0.0360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2081]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4670.0
Loss: 0.05297434329986572
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.5274]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3955]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5965.0
Loss: 19913.51953125
KL Divergence: 16.338260650634766
Action 0 - predicted reward: tensor([[1.3024]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1149]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6660.0
Loss: 18553.21484375
KL Divergence: 16.32656478881836
4599.
Epsilon Greedy 5%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7650.0
Loss: 0.05928552523255348
Action 0 - predicted reward: tensor([[0.0745]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.3490]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7570.0
Loss: 0.072456955909729
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0166]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0668]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4400.0
Loss: 0.03795810043811798
Action 0 - predicted reward: tensor([[0.0654]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0130]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6430.0
Loss: 0.05368103086948395
Greedy
Action 0 - predicted reward: tensor([[0.1498]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.2620]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2990.0
Loss: 0.021715963259339333
Action 0 - predicted reward: tensor([[0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.3688]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4670.0
Loss: 0.04637995362281799
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7127]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4647]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5965.0
Loss: 18264.798828125
KL Divergence: 16.363584518432617
Action 0 - predicted reward: tensor([[1.3461]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4193]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6665.0
Loss: 18074.22265625
KL Divergence: 16.31964874267578
4699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2633]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6838]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7730.0
Loss: 0.0626802146434784
Action 0 - predicted reward: tensor([[-0.1792]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8786]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7795.0
Loss: 0.08675878494977951
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9392]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4400.0
Loss: 0.03701658174395561
Action 0 - predicted reward: tensor([[0.2443]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.0411]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6510.0
Loss: 0.056116826832294464
Greedy
Action 0 - predicted reward: tensor([[-0.0450]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4440]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3025.0
Loss: 0.016654590144753456
Action 0 - predicted reward: tensor([[-0.1896]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.8544]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4670.0
Loss: 0.04537089169025421
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7200]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3749]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5965.0
Loss: 16292.5146484375
KL Divergence: 16.335458755493164
Action 0 - predicted reward: tensor([[1.3882]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4477]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6675.0
Loss: 17850.32421875
KL Divergence: 16.317617416381836
4799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3487]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.3573]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7875.0
Loss: 0.06833918392658234
Action 0 - predicted reward: tensor([[-0.3376]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.4980]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7890.0
Loss: 0.08919622004032135
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8346]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4405.0
Loss: 0.033962227404117584
Action 0 - predicted reward: tensor([[-0.0923]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5551]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6580.0
Loss: 0.05631837621331215
Greedy
Action 0 - predicted reward: tensor([[0.8496]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9155]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3025.0
Loss: 0.01240577083081007
Action 0 - predicted reward: tensor([[0.5332]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5993]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4715.0
Loss: 0.04692138731479645
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.8128]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7790]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5970.0
Loss: 14595.083984375
KL Divergence: 16.341379165649414
Action 0 - predicted reward: tensor([[1.4552]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3920]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6730.0
Loss: 17627.228515625
KL Divergence: 16.306272506713867
4899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2216]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.8493]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8020.0
Loss: 0.07038643956184387
Action 0 - predicted reward: tensor([[0.1641]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.9634]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8145.0
Loss: 0.08118464797735214
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0578]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0643]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4410.0
Loss: 0.03372334688901901
Action 0 - predicted reward: tensor([[-0.2337]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.8552]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6580.0
Loss: 0.050732918083667755
Greedy
Action 0 - predicted reward: tensor([[-0.0091]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.3205]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3030.0
Loss: 0.011156941764056683
Action 0 - predicted reward: tensor([[-0.0979]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.2149]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4825.0
Loss: 0.04488421231508255
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.8848]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9578]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5975.0
Loss: 13769.3095703125
KL Divergence: 16.33452033996582
Action 0 - predicted reward: tensor([[1.4774]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5500]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6735.0
Loss: 17399.64453125
KL Divergence: 16.313465118408203
4999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3259]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.7932]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8160.0
Loss: 0.06763001531362534
Action 0 - predicted reward: tensor([[0.2053]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7364]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8205.0
Loss: 0.07382185757160187
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0275]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.8138]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4515.0
Loss: 0.03607384115457535
Action 0 - predicted reward: tensor([[-0.0845]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9494]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6650.0
Loss: 0.05059843510389328
Greedy
Action 0 - predicted reward: tensor([[-0.1113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.6302]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3035.0
Loss: 0.01112933736294508
Action 0 - predicted reward: tensor([[0.0159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0229]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4900.0
Loss: 0.03939959779381752
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.9345]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0091]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5975.0
Loss: 12048.9970703125
KL Divergence: 16.339900970458984
Action 0 - predicted reward: tensor([[1.5535]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6135]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6740.0
Loss: 16309.599609375
KL Divergence: 16.299720764160156
5099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3698]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7339]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8340.0
Loss: 0.06778763234615326
Action 0 - predicted reward: tensor([[-0.1703]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.0070]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8385.0
Loss: 0.07159025967121124
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9970]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4530.0
Loss: 0.03461723402142525
Action 0 - predicted reward: tensor([[0.0498]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-82.7724]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6755.0
Loss: 0.0590144507586956
Greedy
Action 0 - predicted reward: tensor([[-0.0410]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.9287]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3035.0
Loss: 0.01042008213698864
Action 0 - predicted reward: tensor([[-0.1076]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-77.9530]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4945.0
Loss: 0.03467397764325142
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.9617]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0224]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5975.0
Loss: 11803.7509765625
KL Divergence: 16.31759262084961
Action 0 - predicted reward: tensor([[1.5880]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6688]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6745.0
Loss: 15504.7744140625
KL Divergence: 16.30113410949707
5199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[1.8777]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[11.8258]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 8560.0
Loss: 0.07496660202741623
Action 0 - predicted reward: tensor([[-0.1448]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.1317]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8530.0
Loss: 0.07341925799846649
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0205]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1521]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4565.0
Loss: 0.031386423856019974
Action 0 - predicted reward: tensor([[-0.0059]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0632]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6860.0
Loss: 0.05968562886118889
Greedy
Action 0 - predicted reward: tensor([[-0.6667]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2048]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3070.0
Loss: 0.015007584355771542
Action 0 - predicted reward: tensor([[0.2506]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9618]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4985.0
Loss: 0.02491110749542713
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.0382]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7116]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5975.0
Loss: 11530.5927734375
KL Divergence: 16.322080612182617
Action 0 - predicted reward: tensor([[1.7023]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7694]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6745.0
Loss: 13854.0146484375
KL Divergence: 16.315269470214844
5299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.9488]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8560.0
Loss: 0.06384875625371933
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 8685.0
Loss: 0.06875807791948318
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0391]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9134]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4565.0
Loss: 0.0268219243735075
Action 0 - predicted reward: tensor([[0.1772]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0028]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6900.0
Loss: 0.0609549954533577
Greedy
Action 0 - predicted reward: tensor([[-0.2651]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0153]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3105.0
Loss: 0.016322271898388863
Action 0 - predicted reward: tensor([[-0.1866]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.3577]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4985.0
Loss: 0.01800597459077835
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.0819]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0042]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5975.0
Loss: 11558.2216796875
KL Divergence: 16.31991958618164
Action 0 - predicted reward: tensor([[1.8031]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1859]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6745.0
Loss: 13346.986328125
KL Divergence: 16.30246925354004
5399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0783]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8251]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8605.0
Loss: 0.06376072764396667
Action 0 - predicted reward: tensor([[-0.0397]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-83.9561]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8840.0
Loss: 0.06762301921844482
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-71.8783]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4680.0
Loss: 0.0355537123978138
Action 0 - predicted reward: tensor([[0.0260]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0063]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6905.0
Loss: 0.05826709792017937
Greedy
Action 0 - predicted reward: tensor([[-0.0529]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.9563]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3105.0
Loss: 0.013545333407819271
Action 0 - predicted reward: tensor([[0.0085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.9634]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5060.0
Loss: 0.02142743207514286
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.1283]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6478]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5975.0
Loss: 11001.328125
KL Divergence: 16.3171329498291
Action 0 - predicted reward: tensor([[1.8418]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2289]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6755.0
Loss: 11651.2685546875
KL Divergence: 16.289915084838867
5499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-2.2953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7759]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8610.0
Loss: 0.05762351304292679
Action 0 - predicted reward: tensor([[-0.1374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.3061]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8960.0
Loss: 0.07266464829444885
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0340]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.6861]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4820.0
Loss: 0.038193050771951675
Action 0 - predicted reward: tensor([[-0.0885]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.2468]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6975.0
Loss: 0.05059003829956055
Greedy
Action 0 - predicted reward: tensor([[-0.8618]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0394]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3105.0
Loss: 0.013695606030523777
Action 0 - predicted reward: tensor([[0.9165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9476]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5095.0
Loss: 0.014878510497510433
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.1773]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7800]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5975.0
Loss: 11025.4111328125
KL Divergence: 16.315786361694336
Action 0 - predicted reward: tensor([[1.8892]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9605]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6755.0
Loss: 11118.4423828125
KL Divergence: 16.2918701171875
5599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0967]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.1731]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8835.0
Loss: 0.05789543688297272
Action 0 - predicted reward: tensor([[0.1705]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2487]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9185.0
Loss: 0.07870222628116608
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0514]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.2960]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4855.0
Loss: 0.03183513507246971
Action 0 - predicted reward: tensor([[-0.2802]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1598]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7090.0
Loss: 0.05291566252708435
Greedy
Action 0 - predicted reward: tensor([[-0.7902]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0324]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3105.0
Loss: 0.013630769215524197
Action 0 - predicted reward: tensor([[-0.0085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.9154]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5095.0
Loss: 0.013760292902588844
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2240]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2968]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5980.0
Loss: 11045.2646484375
KL Divergence: 16.30801010131836
Action 0 - predicted reward: tensor([[1.9392]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0077]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6765.0
Loss: 10282.4345703125
KL Divergence: 16.28544807434082
5699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0147]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9165]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8955.0
Loss: 0.060035474598407745
Action 0 - predicted reward: tensor([[-0.0863]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.8433]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9295.0
Loss: 0.07649111747741699
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0830]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.0439]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4905.0
Loss: 0.025911128148436546
Action 0 - predicted reward: tensor([[-0.2154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.4079]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 0.05515268072485924
Greedy
Action 0 - predicted reward: tensor([[-0.4664]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9081]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3140.0
Loss: 0.013646847568452358
Action 0 - predicted reward: tensor([[0.1528]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-80.4306]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5130.0
Loss: 0.01819426752626896
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2474]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9300]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5980.0
Loss: 11060.0244140625
KL Divergence: 16.295345306396484
Action 0 - predicted reward: tensor([[2.0414]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6353]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6765.0
Loss: 10032.4580078125
KL Divergence: 16.28033447265625
5799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2544]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9270.0
Loss: 0.06436420232057571
Action 0 - predicted reward: tensor([[-0.0163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2263]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9480.0
Loss: 0.086235411465168
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0769]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0411]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4985.0
Loss: 0.02991565689444542
Action 0 - predicted reward: tensor([[2.4863]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2151]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7230.0
Loss: 0.051950499415397644
Greedy
Action 0 - predicted reward: tensor([[-0.6973]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9868]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3140.0
Loss: 0.013505334965884686
Action 0 - predicted reward: tensor([[-0.4527]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0260]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5130.0
Loss: 0.014600137248635292
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2901]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3611]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5980.0
Loss: 11073.859375
KL Divergence: 16.28009033203125
Action 0 - predicted reward: tensor([[2.0314]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1033]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6770.0
Loss: 10057.1923828125
KL Divergence: 16.268430709838867
5899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0332]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0772]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9345.0
Loss: 0.05651549994945526
Action 0 - predicted reward: tensor([[-0.3571]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.7253]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9690.0
Loss: 0.0905512198805809
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1849]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1012]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5020.0
Loss: 0.0328541062772274
Action 0 - predicted reward: tensor([[-0.4169]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.7037]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7350.0
Loss: 0.045737750828266144
Greedy
Action 0 - predicted reward: tensor([[-0.4394]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9613]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3140.0
Loss: 0.01192605309188366
Action 0 - predicted reward: tensor([[0.0923]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.7405]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5135.0
Loss: 0.013971487991511822
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3025]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3776]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5980.0
Loss: 11085.55859375
KL Divergence: 16.281578063964844
Action 0 - predicted reward: tensor([[2.0942]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5806]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6770.0
Loss: 10082.2421875
KL Divergence: 16.270401000976562
5999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1540]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.1651]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9425.0
Loss: 0.05687134340405464
Action 0 - predicted reward: tensor([[-0.1299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1005]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9735.0
Loss: 0.08952245116233826
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0375]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.6877]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5090.0
Loss: 0.03699275851249695
Action 0 - predicted reward: tensor([[2.5761]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8755]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7385.0
Loss: 0.04153865948319435
Greedy
Action 0 - predicted reward: tensor([[-0.2450]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0999]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3175.0
Loss: 0.005863450933247805
Action 0 - predicted reward: tensor([[0.0813]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.2867]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5135.0
Loss: 0.013720592483878136
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3741]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4441]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5980.0
Loss: 11089.150390625
KL Divergence: 16.278478622436523
Action 0 - predicted reward: tensor([[2.1098]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5350]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6770.0
Loss: 9808.009765625
KL Divergence: 16.268186569213867
6099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.5017]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9465.0
Loss: 0.05109758675098419
Action 0 - predicted reward: tensor([[-0.0443]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8710]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9885.0
Loss: 0.08664292097091675
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0640]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-81.5759]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5090.0
Loss: 0.02979130484163761
Action 0 - predicted reward: tensor([[-0.4847]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0985]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7390.0
Loss: 0.03881700336933136
Greedy
Action 0 - predicted reward: tensor([[0.0232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.5460]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3175.0
Loss: 0.0038738646544516087
Action 0 - predicted reward: tensor([[0.3255]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0988]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.015985408797860146
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3579]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7976]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5985.0
Loss: 11094.943359375
KL Divergence: 16.27412986755371
Action 0 - predicted reward: tensor([[2.1732]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2486]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6805.0
Loss: 9827.509765625
KL Divergence: 16.270811080932617
6199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5389]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9261]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9470.0
Loss: 0.04800649359822273
Action 0 - predicted reward: tensor([[-0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.3608]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9990.0
Loss: 0.0944485291838646
Epsilon Greedy 1%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5160.0
Loss: 0.03380684182047844
Action 0 - predicted reward: tensor([[-0.5613]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9571]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7465.0
Loss: 0.038025468587875366
Greedy
Action 0 - predicted reward: tensor([[1.6669]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9280]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3175.0
Loss: 0.003715797094628215
Action 0 - predicted reward: tensor([[0.5535]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9919]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5215.0
Loss: 0.014724764041602612
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3438]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4189]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6020.0
Loss: 11391.9208984375
KL Divergence: 16.26482582092285
Action 0 - predicted reward: tensor([[2.1920]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2610]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6810.0
Loss: 9843.2529296875
KL Divergence: 16.257328033447266
6299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.6033]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8966]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9680.0
Loss: 0.05861697345972061
Action 0 - predicted reward: tensor([[0.0382]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.5235]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10065.0
Loss: 0.09581980109214783
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0663]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.8012]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5200.0
Loss: 0.03274291753768921
Action 0 - predicted reward: tensor([[1.7043]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1300]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7470.0
Loss: 0.03403272479772568
Greedy
Action 0 - predicted reward: tensor([[-0.0957]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.9857]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3210.0
Loss: 0.003749025519937277
Action 0 - predicted reward: tensor([[0.3733]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0151]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5250.0
Loss: 0.013774829916656017
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3508]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4131]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6090.0
Loss: 11987.2958984375
KL Divergence: 16.255765914916992
Action 0 - predicted reward: tensor([[2.2221]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2975]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6845.0
Loss: 10145.9033203125
KL Divergence: 16.254274368286133
6399.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 9900.0
Loss: 0.05791788175702095
Action 0 - predicted reward: tensor([[-0.0606]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.2433]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10225.0
Loss: 0.10138058662414551
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0683]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-70.2936]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5270.0
Loss: 0.03171515092253685
Action 0 - predicted reward: tensor([[-0.0004]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.4752]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7610.0
Loss: 0.03347201272845268
Greedy
Action 0 - predicted reward: tensor([[-0.0272]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.2973]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3210.0
Loss: 0.0035080653615295887
Action 0 - predicted reward: tensor([[0.2623]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.6281]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5250.0
Loss: 0.01324908621609211
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3428]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9576]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6090.0
Loss: 11984.3583984375
KL Divergence: 16.240428924560547
Action 0 - predicted reward: tensor([[2.2338]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1982]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6845.0
Loss: 10165.421875
KL Divergence: 16.25153160095215
6499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.5179]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9945.0
Loss: 0.05409224331378937
Action 0 - predicted reward: tensor([[-0.0405]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9046]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10240.0
Loss: 0.09758113324642181
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0136]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8469]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5375.0
Loss: 0.030129538848996162
Action 0 - predicted reward: tensor([[0.1955]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.2012]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7610.0
Loss: 0.02642424963414669
Greedy
Action 0 - predicted reward: tensor([[0.4650]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1028]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3210.0
Loss: 0.0034532053396105766
Action 0 - predicted reward: tensor([[0.3425]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9997]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5290.0
Loss: 0.013143915683031082
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3881]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6885]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6090.0
Loss: 11993.8037109375
KL Divergence: 16.220460891723633
Action 0 - predicted reward: tensor([[2.2415]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1110]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6845.0
Loss: 10174.4873046875
KL Divergence: 16.237396240234375
6599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1264]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10025.0
Loss: 0.05051334574818611
Action 0 - predicted reward: tensor([[0.0511]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3640]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10360.0
Loss: 0.09119248390197754
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0579]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.2324]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5515.0
Loss: 0.025646211579442024
Action 0 - predicted reward: tensor([[0.4158]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9429]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7685.0
Loss: 0.03148652985692024
Greedy
Action 0 - predicted reward: tensor([[-0.0278]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.4589]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3210.0
Loss: 0.0035165429580956697
Action 0 - predicted reward: tensor([[0.0912]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.9026]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5335.0
Loss: 0.016772519797086716
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3881]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4513]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6090.0
Loss: 11693.3232421875
KL Divergence: 16.22176742553711
Action 0 - predicted reward: tensor([[2.2496]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3155]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6885.0
Loss: 10473.3466796875
KL Divergence: 16.233901977539062
6699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1546]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.8535]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10070.0
Loss: 0.051018889993429184
Action 0 - predicted reward: tensor([[0.0132]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.7623]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10400.0
Loss: 0.09280804544687271
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2784]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3835]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5550.0
Loss: 0.02823140285909176
Action 0 - predicted reward: tensor([[-0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.4373]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7755.0
Loss: 0.02983386628329754
Greedy
Action 0 - predicted reward: tensor([[0.0251]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.0359]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3245.0
Loss: 0.0038129622116684914
Action 0 - predicted reward: tensor([[0.0481]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.2198]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5345.0
Loss: 0.011364353820681572
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3649]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4292]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6090.0
Loss: 11399.107421875
KL Divergence: 16.209701538085938
Action 0 - predicted reward: tensor([[2.2781]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3440]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6885.0
Loss: 10480.12890625
KL Divergence: 16.225889205932617
6799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0454]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.7462]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10215.0
Loss: 0.05176267400383949
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 10470.0
Loss: 0.08628274500370026
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0379]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1624]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5690.0
Loss: 0.040151529014110565
Action 0 - predicted reward: tensor([[0.0458]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.8878]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7795.0
Loss: 0.029359839856624603
Greedy
Action 0 - predicted reward: tensor([[0.0311]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.4035]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3280.0
Loss: 0.004279920365661383
Action 0 - predicted reward: tensor([[-0.1467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.1374]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5390.0
Loss: 0.013738229870796204
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4631]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5231]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6125.0
Loss: 10806.4306640625
KL Divergence: 16.199846267700195
Action 0 - predicted reward: tensor([[2.2938]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3592]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6920.0
Loss: 10785.44140625
KL Divergence: 16.223173141479492
6899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.6680]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10295.0
Loss: 0.05111506208777428
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10515.0
Loss: 0.07931054383516312
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.0507]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5800.0
Loss: 0.030197249725461006
Action 0 - predicted reward: tensor([[-0.3896]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9357]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7830.0
Loss: 0.02579060010612011
Greedy
Action 0 - predicted reward: tensor([[-0.1051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-76.2835]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3280.0
Loss: 0.0034227350261062384
Action 0 - predicted reward: tensor([[0.4490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0330]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5395.0
Loss: 0.013551169075071812
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3812]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1135]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6165.0
Loss: 10508.4697265625
KL Divergence: 16.187715530395508
Action 0 - predicted reward: tensor([[2.3604]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4205]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6930.0
Loss: 10784.8515625
KL Divergence: 16.200902938842773
6999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8911]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10345.0
Loss: 0.044097017496824265
Action 0 - predicted reward: tensor([[0.0329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0067]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10555.0
Loss: 0.06979244947433472
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0141]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.5987]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5875.0
Loss: 0.031825076788663864
Action 0 - predicted reward: tensor([[0.2155]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0109]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7900.0
Loss: 0.02637176401913166
Greedy
Action 0 - predicted reward: tensor([[-0.0173]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.5575]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3280.0
Loss: 0.003374670632183552
Action 0 - predicted reward: tensor([[0.6789]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0616]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5435.0
Loss: 0.013633248396217823
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4215]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4917]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6200.0
Loss: 10805.720703125
KL Divergence: 16.185562133789062
Action 0 - predicted reward: tensor([[2.3327]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7013]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6930.0
Loss: 10794.0400390625
KL Divergence: 16.198322296142578
7099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1502]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7918]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10465.0
Loss: 0.04085161164402962
Action 0 - predicted reward: tensor([[0.0274]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.6366]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10635.0
Loss: 0.06558138877153397
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0371]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.4740]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5910.0
Loss: 0.02933376468718052
Action 0 - predicted reward: tensor([[-0.0095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.0043]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7980.0
Loss: 0.03170327469706535
Greedy
Action 0 - predicted reward: tensor([[1.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0673]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3280.0
Loss: 0.0033266667742282152
Action 0 - predicted reward: tensor([[-0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.4926]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5505.0
Loss: 0.010530289262533188
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4311]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4926]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6200.0
Loss: 10806.6357421875
KL Divergence: 16.17759132385254
Action 0 - predicted reward: tensor([[2.3403]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0366]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6935.0
Loss: 10794.2197265625
KL Divergence: 16.182239532470703
7199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3000]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4125]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10505.0
Loss: 0.041181087493896484
Action 0 - predicted reward: tensor([[-0.0414]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0898]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10745.0
Loss: 0.06464122235774994
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1274]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0320]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6015.0
Loss: 0.030646201223134995
Action 0 - predicted reward: tensor([[0.7826]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0212]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8025.0
Loss: 0.026317032054066658
Greedy
Action 0 - predicted reward: tensor([[0.0094]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.1668]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3315.0
Loss: 0.005016682203859091
Action 0 - predicted reward: tensor([[0.9909]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9778]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5505.0
Loss: 0.010075070895254612
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4075]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8444]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6200.0
Loss: 9907.4423828125
KL Divergence: 16.152240753173828
Action 0 - predicted reward: tensor([[2.3454]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6325]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6935.0
Loss: 10499.2587890625
KL Divergence: 16.168554306030273
7299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3244]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.7801]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10620.0
Loss: 0.04500546678900719
Action 0 - predicted reward: tensor([[0.1373]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7426]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10820.0
Loss: 0.07167291641235352
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0313]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4968]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6020.0
Loss: 0.02522938884794712
Action 0 - predicted reward: tensor([[-0.4746]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9882]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8060.0
Loss: 0.021165622398257256
Greedy
Action 0 - predicted reward: tensor([[-1.0497]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9653]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3385.0
Loss: 0.001133973360992968
Action 0 - predicted reward: tensor([[-0.0013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6531]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5545.0
Loss: 0.01402375940233469
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4641]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5205]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6200.0
Loss: 9613.9560546875
KL Divergence: 16.135604858398438
Action 0 - predicted reward: tensor([[2.3709]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9990]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6940.0
Loss: 10500.2333984375
KL Divergence: 16.171592712402344
7399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1087]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.8856]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10660.0
Loss: 0.045182179659605026
Action 0 - predicted reward: tensor([[0.1148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.2857]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 10950.0
Loss: 0.06418810039758682
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0800]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.7704]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6095.0
Loss: 0.03072088584303856
Action 0 - predicted reward: tensor([[-0.6692]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9916]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8140.0
Loss: 0.020663095638155937
Greedy
Action 0 - predicted reward: tensor([[0.0663]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0817]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3455.0
Loss: 0.0008598266285844147
Action 0 - predicted reward: tensor([[-0.5383]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-72.3469]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5615.0
Loss: 0.013826451264321804
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4719]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3714]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6200.0
Loss: 9613.771484375
KL Divergence: 16.139528274536133
Action 0 - predicted reward: tensor([[2.3447]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7764]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6980.0
Loss: 10794.2802734375
KL Divergence: 16.14853858947754
7499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2569]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.3785]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10700.0
Loss: 0.04402432590723038
Action 0 - predicted reward: tensor([[-0.0007]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9732]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11080.0
Loss: 0.06611080467700958
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0965]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5095]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6100.0
Loss: 0.028195790946483612
Action 0 - predicted reward: tensor([[0.0737]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0207]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8245.0
Loss: 0.027206631377339363
Greedy
Action 0 - predicted reward: tensor([[0.5586]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9508]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3455.0
Loss: 5.931673149461858e-05
Action 0 - predicted reward: tensor([[-0.0923]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4638]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5620.0
Loss: 0.01345352828502655
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4279]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4943]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 9613.90234375
KL Divergence: 16.11168098449707
Action 0 - predicted reward: tensor([[2.4011]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4647]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6980.0
Loss: 10504.7861328125
KL Divergence: 16.152685165405273
7599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3484]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9532]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10710.0
Loss: 0.038173142820596695
Action 0 - predicted reward: tensor([[0.0602]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7158]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11095.0
Loss: 0.06107602268457413
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1265]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6135.0
Loss: 0.024336688220500946
Action 0 - predicted reward: tensor([[-0.0772]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1455]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8280.0
Loss: 0.02474980242550373
Greedy
Action 0 - predicted reward: tensor([[0.0035]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.5748]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3455.0
Loss: 3.671492959256284e-05
Action 0 - predicted reward: tensor([[-0.1109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.4623]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5660.0
Loss: 0.015188259072601795
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4369]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4977]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 9016.6865234375
KL Divergence: 16.110252380371094
Action 0 - predicted reward: tensor([[2.4416]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5100]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6980.0
Loss: 9903.197265625
KL Divergence: 16.14232635498047
7699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4356]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9702]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10750.0
Loss: 0.03777850791811943
Action 0 - predicted reward: tensor([[-0.0149]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.7865]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11200.0
Loss: 0.06590727716684341
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.2998]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6205.0
Loss: 0.022342180833220482
Action 0 - predicted reward: tensor([[-0.2795]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8365]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8320.0
Loss: 0.022548029199242592
Greedy
Action 0 - predicted reward: tensor([[-0.6736]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0299]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3455.0
Loss: 3.176051905029453e-05
Action 0 - predicted reward: tensor([[-0.0232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.0561]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5670.0
Loss: 0.013452421873807907
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4696]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9841]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6235.0
Loss: 8713.3134765625
KL Divergence: 16.08997344970703
Action 0 - predicted reward: tensor([[2.4116]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4813]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 9609.5634765625
KL Divergence: 16.13228416442871
7799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.2766]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10830.0
Loss: 0.04348920285701752
Action 0 - predicted reward: tensor([[0.0490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9661]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11350.0
Loss: 0.06110553443431854
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0681]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.6233]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6245.0
Loss: 0.020976463332772255
Action 0 - predicted reward: tensor([[-0.0823]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.8236]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8430.0
Loss: 0.027123745530843735
Greedy
Action 0 - predicted reward: tensor([[-1.7752]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9935]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3490.0
Loss: 0.0022234530188143253
Action 0 - predicted reward: tensor([[-0.0023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.5837]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5705.0
Loss: 0.01693369448184967
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4548]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0823]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6235.0
Loss: 8120.81982421875
KL Divergence: 16.077136993408203
Action 0 - predicted reward: tensor([[2.4290]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4872]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 9611.8603515625
KL Divergence: 16.12408447265625
7899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.3991]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10830.0
Loss: 0.035874031484127045
Action 0 - predicted reward: tensor([[0.0363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.2409]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11535.0
Loss: 0.058683186769485474
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.7821]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3497]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6280.0
Loss: 0.021765833720564842
Action 0 - predicted reward: tensor([[-0.2981]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8412]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8435.0
Loss: 0.024606604129076004
Greedy
Action 0 - predicted reward: tensor([[0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.8377]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3525.0
Loss: 0.0008624884067103267
Action 0 - predicted reward: tensor([[1.1091]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9807]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5745.0
Loss: 0.01736891083419323
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4518]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5223]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 8119.9638671875
KL Divergence: 16.06535530090332
Action 0 - predicted reward: tensor([[2.3903]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4567]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6995.0
Loss: 9610.9189453125
KL Divergence: 16.112173080444336
7999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3621]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1601]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10910.0
Loss: 0.03544783219695091
Action 0 - predicted reward: tensor([[0.0279]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.3998]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11650.0
Loss: 0.056171394884586334
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.6278]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.026077751070261
Action 0 - predicted reward: tensor([[-0.7638]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1258]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8470.0
Loss: 0.024839699268341064
Greedy
Action 0 - predicted reward: tensor([[0.0009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.3697]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3525.0
Loss: 0.00014917539374437183
Action 0 - predicted reward: tensor([[0.8929]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7955]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5745.0
Loss: 0.01096206996589899
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4761]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5345]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 8120.9599609375
KL Divergence: 16.058773040771484
Action 0 - predicted reward: tensor([[2.4523]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5118]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6995.0
Loss: 9314.6630859375
KL Divergence: 16.092119216918945
8099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3311]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9233]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11060.0
Loss: 0.04916408285498619
Action 0 - predicted reward: tensor([[-0.2687]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9728]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11735.0
Loss: 0.05193307250738144
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0508]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0630]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.024103455245494843
Action 0 - predicted reward: tensor([[-2.4558]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.7877]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8480.0
Loss: 0.025508509948849678
Greedy
Action 0 - predicted reward: tensor([[-0.1727]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0777]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3525.0
Loss: 7.13936606189236e-05
Action 0 - predicted reward: tensor([[-0.0081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.6240]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5750.0
Loss: 0.010204104706645012
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4912]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5568]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 8120.75390625
KL Divergence: 16.043636322021484
Action 0 - predicted reward: tensor([[2.4285]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4907]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7000.0
Loss: 9016.1455078125
KL Divergence: 16.079444885253906
8199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0782]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.4411]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11095.0
Loss: 0.03979049623012543
Action 0 - predicted reward: tensor([[-0.0775]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8544]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11915.0
Loss: 0.05480768904089928
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0267]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-74.4362]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6425.0
Loss: 0.02070579119026661
Action 0 - predicted reward: tensor([[-0.5114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0192]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8520.0
Loss: 0.027473976835608482
Greedy
Action 0 - predicted reward: tensor([[-0.0314]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.3936]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3525.0
Loss: 5.284268263494596e-05
Action 0 - predicted reward: tensor([[0.7450]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1331]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5750.0
Loss: 0.010320811532437801
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4558]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5261]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 7821.05859375
KL Divergence: 16.03398895263672
Action 0 - predicted reward: tensor([[2.4732]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5295]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7000.0
Loss: 8717.193359375
KL Divergence: 16.07839012145996
8299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0563]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.7250]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11145.0
Loss: 0.04074857756495476
Action 0 - predicted reward: tensor([[0.1230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.6358]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11995.0
Loss: 0.042254991829395294
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0140]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3258]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6605.0
Loss: 0.028383851051330566
Action 0 - predicted reward: tensor([[-0.2561]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9020]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8560.0
Loss: 0.02479141764342785
Greedy
Action 0 - predicted reward: tensor([[-0.0014]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.5147]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3525.0
Loss: 3.9884867874206975e-05
Action 0 - predicted reward: tensor([[1.3583]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9328]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5755.0
Loss: 0.010091394186019897
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4939]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2762]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6235.0
Loss: 7821.79248046875
KL Divergence: 16.02800750732422
Action 0 - predicted reward: tensor([[2.4218]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6839]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7035.0
Loss: 8717.2841796875
KL Divergence: 16.06096839904785
8399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1569]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1008]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11215.0
Loss: 0.04201793298125267
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 12055.0
Loss: 0.041522759944200516
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0201]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.3803]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6675.0
Loss: 0.020398851484060287
Action 0 - predicted reward: tensor([[0.0802]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.8411]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8640.0
Loss: 0.02820456586778164
Greedy
Action 0 - predicted reward: tensor([[-0.1427]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9706]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3525.0
Loss: 3.403388473088853e-05
Action 0 - predicted reward: tensor([[-0.0407]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.1120]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5790.0
Loss: 0.009969152510166168
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4834]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5408]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 7816.6689453125
KL Divergence: 16.011320114135742
Action 0 - predicted reward: tensor([[2.4896]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5522]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7035.0
Loss: 7822.08935546875
KL Divergence: 16.067110061645508
8499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1604]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0463]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11290.0
Loss: 0.045279499143362045
Action 0 - predicted reward: tensor([[-0.0019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.4657]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12145.0
Loss: 0.040305301547050476
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-78.0812]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6680.0
Loss: 0.02012690342962742
Action 0 - predicted reward: tensor([[0.2766]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.3319]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8640.0
Loss: 0.027889283373951912
Greedy
Action 0 - predicted reward: tensor([[-1.3688]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0154]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3525.0
Loss: 2.775527536869049e-05
Action 0 - predicted reward: tensor([[-0.9764]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-77.0150]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5790.0
Loss: 0.009902136400341988
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5133]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5693]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 7522.615234375
KL Divergence: 16.00286102294922
Action 0 - predicted reward: tensor([[2.4703]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8175]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 7513.9052734375
KL Divergence: 16.0621337890625
8599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0201]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.2617]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11300.0
Loss: 0.04158724471926689
Action 0 - predicted reward: tensor([[0.0178]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.0729]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12215.0
Loss: 0.04120168462395668
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-93.9415]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6755.0
Loss: 0.019816061481833458
Action 0 - predicted reward: tensor([[0.2800]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9225]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8675.0
Loss: 0.03176162391901016
Greedy
Action 0 - predicted reward: tensor([[-1.2620]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9469]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3560.0
Loss: 0.004859795793890953
Action 0 - predicted reward: tensor([[1.1067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0085]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5790.0
Loss: 0.0072616939432919025
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4882]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0857]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6235.0
Loss: 7522.42138671875
KL Divergence: 16.000232696533203
Action 0 - predicted reward: tensor([[2.4660]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0509]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 7523.267578125
KL Divergence: 16.0380916595459
8699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0035]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.3853]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11315.0
Loss: 0.036275189369916916
Action 0 - predicted reward: tensor([[0.3095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9265]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12300.0
Loss: 0.037269968539476395
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0437]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.5643]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6830.0
Loss: 0.024832528084516525
Action 0 - predicted reward: tensor([[1.0798]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9333]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8710.0
Loss: 0.03224192187190056
Greedy
Action 0 - predicted reward: tensor([[0.3178]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0399]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3560.0
Loss: 0.004535938613116741
Action 0 - predicted reward: tensor([[-0.0075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.2555]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5790.0
Loss: 0.003620235016569495
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4585]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5160]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 7523.005859375
KL Divergence: 15.977999687194824
Action 0 - predicted reward: tensor([[2.4426]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9404]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 7523.001953125
KL Divergence: 16.032024383544922
8799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0149]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.0771]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11425.0
Loss: 0.037499457597732544
Action 0 - predicted reward: tensor([[-0.0430]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9314]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12425.0
Loss: 0.04158655181527138
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0213]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0170]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6870.0
Loss: 0.02150341309607029
Action 0 - predicted reward: tensor([[-0.9467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9819]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8790.0
Loss: 0.02726520225405693
Greedy
Action 0 - predicted reward: tensor([[-0.1051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9216]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3560.0
Loss: 0.00450092600658536
Action 0 - predicted reward: tensor([[-0.0512]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.3660]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5795.0
Loss: 0.003570069093257189
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4862]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5129]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6270.0
Loss: 7821.23876953125
KL Divergence: 15.968060493469238
Action 0 - predicted reward: tensor([[2.4603]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5157]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7045.0
Loss: 7523.14404296875
KL Divergence: 16.030160903930664
8899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4867]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9404]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11475.0
Loss: 0.03495049476623535
Action 0 - predicted reward: tensor([[-0.0818]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.7371]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12475.0
Loss: 0.0426667220890522
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4346]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8823]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6905.0
Loss: 0.026988869532942772
Action 0 - predicted reward: tensor([[-0.6232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9353]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8860.0
Loss: 0.027224965393543243
Greedy
Action 0 - predicted reward: tensor([[-0.0640]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7133]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3595.0
Loss: 0.008414392359554768
Action 0 - predicted reward: tensor([[-0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9931]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5830.0
Loss: 0.003562326543033123
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4750]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2760]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6270.0
Loss: 7821.28466796875
KL Divergence: 15.953140258789062
Action 0 - predicted reward: tensor([[2.5364]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5895]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7045.0
Loss: 7223.451171875
KL Divergence: 16.019386291503906
8999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0797]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.0843]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11590.0
Loss: 0.029582036659121513
Action 0 - predicted reward: tensor([[0.1555]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0000]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12490.0
Loss: 0.034118011593818665
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2993]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6975.0
Loss: 0.03158330172300339
Action 0 - predicted reward: tensor([[-0.2009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9908]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8895.0
Loss: 0.028070420026779175
Greedy
Action 0 - predicted reward: tensor([[0.0421]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.5217]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3595.0
Loss: 0.00815038662403822
Action 0 - predicted reward: tensor([[1.1667]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9814]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5830.0
Loss: 0.003430664539337158
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4786]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8864]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6270.0
Loss: 7816.6474609375
KL Divergence: 15.947466850280762
Action 0 - predicted reward: tensor([[2.5057]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5652]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 7522.81201171875
KL Divergence: 15.995569229125977
9099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0555]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9581]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11745.0
Loss: 0.035006050020456314
Action 0 - predicted reward: tensor([[-0.0409]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9229]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12640.0
Loss: 0.038119543343782425
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1638]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.4652]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7080.0
Loss: 0.028146319091320038
Action 0 - predicted reward: tensor([[0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.3021]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8930.0
Loss: 0.027184968814253807
Greedy
Action 0 - predicted reward: tensor([[-0.3336]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9900]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3595.0
Loss: 0.007815437391400337
Action 0 - predicted reward: tensor([[1.0140]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0256]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5830.0
Loss: 0.0034255916252732277
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4865]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5440]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6305.0
Loss: 8119.779296875
KL Divergence: 15.939505577087402
Action 0 - predicted reward: tensor([[2.4954]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5664]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 7522.17041015625
KL Divergence: 16.00166130065918
9199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0421]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.3519]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11820.0
Loss: 0.03510390967130661
Action 0 - predicted reward: tensor([[-0.0247]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9138]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12715.0
Loss: 0.030484015122056007
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1449]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9277]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7150.0
Loss: 0.026427047327160835
Action 0 - predicted reward: tensor([[-0.5497]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9772]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9000.0
Loss: 0.02330777607858181
Greedy
Action 0 - predicted reward: tensor([[-0.5350]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8540]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3630.0
Loss: 0.005567564629018307
Action 0 - predicted reward: tensor([[0.4783]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0066]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5830.0
Loss: 0.0034368345513939857
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4870]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9891]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6305.0
Loss: 8116.4404296875
KL Divergence: 15.92456340789795
Action 0 - predicted reward: tensor([[2.4605]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9357]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7095.0
Loss: 7522.87158203125
KL Divergence: 15.977706909179688
9299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5642]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0642]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11930.0
Loss: 0.03984794393181801
Action 0 - predicted reward: tensor([[0.0030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2206]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12775.0
Loss: 0.02693178690969944
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1097]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7190.0
Loss: 0.025746341794729233
Action 0 - predicted reward: tensor([[0.4525]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9395]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9035.0
Loss: 0.023154255002737045
Greedy
Action 0 - predicted reward: tensor([[0.0298]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7307]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3630.0
Loss: 0.004463164135813713
Action 0 - predicted reward: tensor([[0.0582]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.1251]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5830.0
Loss: 0.0033924374729394913
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5121]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2410]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6305.0
Loss: 8117.5419921875
KL Divergence: 15.907730102539062
Action 0 - predicted reward: tensor([[2.4955]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9231]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7135.0
Loss: 7821.50927734375
KL Divergence: 15.974054336547852
9399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.6355]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9733]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11970.0
Loss: 0.037443503737449646
Action 0 - predicted reward: tensor([[0.0732]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8220]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12790.0
Loss: 0.024541301652789116
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0197]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0418]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7260.0
Loss: 0.02981485053896904
Action 0 - predicted reward: tensor([[0.0613]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9555]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9110.0
Loss: 0.028022632002830505
Greedy
Action 0 - predicted reward: tensor([[-0.0010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.5502]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3665.0
Loss: 0.008453138172626495
Action 0 - predicted reward: tensor([[-0.0095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.3815]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5830.0
Loss: 0.0033612994011491537
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4836]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8486]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6305.0
Loss: 8118.57666015625
KL Divergence: 15.90072250366211
Action 0 - predicted reward: tensor([[2.4961]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8967]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7145.0
Loss: 7821.587890625
KL Divergence: 15.96059513092041
9499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4474]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0322]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12115.0
Loss: 0.04809100925922394
Action 0 - predicted reward: tensor([[-0.0329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.3281]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12865.0
Loss: 0.02359136939048767
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8130]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7260.0
Loss: 0.028863394632935524
Action 0 - predicted reward: tensor([[-0.9524]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8466]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9185.0
Loss: 0.026641523465514183
Greedy
Action 0 - predicted reward: tensor([[0.0526]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.5350]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3665.0
Loss: 0.008422193117439747
Action 0 - predicted reward: tensor([[0.0540]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.5016]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5835.0
Loss: 0.0033368209842592478
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4588]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9714]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6375.0
Loss: 8717.921875
KL Divergence: 15.895421981811523
Action 0 - predicted reward: tensor([[2.4828]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5354]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7155.0
Loss: 7817.44091796875
KL Divergence: 15.944703102111816
9599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.1518]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12225.0
Loss: 0.04719449579715729
Action 0 - predicted reward: tensor([[-6.5953e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-91.3032]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12935.0
Loss: 0.016344323754310608
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1561]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3028]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7295.0
Loss: 0.030463168397545815
Action 0 - predicted reward: tensor([[-0.3615]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0241]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9220.0
Loss: 0.021711181849241257
Greedy
Action 0 - predicted reward: tensor([[-0.0308]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.7588]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3665.0
Loss: 0.008364451117813587
Action 0 - predicted reward: tensor([[0.8211]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0007]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5835.0
Loss: 0.0033103993628174067
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4624]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2497]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6375.0
Loss: 8717.7119140625
KL Divergence: 15.881086349487305
Action 0 - predicted reward: tensor([[2.4812]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5429]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7155.0
Loss: 7822.02783203125
KL Divergence: 15.938765525817871
9699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1787]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.7143]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12235.0
Loss: 0.045781347900629044
Action 0 - predicted reward: tensor([[-0.0678]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8154]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12985.0
Loss: 0.016378680244088173
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0590]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1718]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7330.0
Loss: 0.032008927315473557
Action 0 - predicted reward: tensor([[0.3068]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8298]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9220.0
Loss: 0.020355403423309326
Greedy
Action 0 - predicted reward: tensor([[-0.1217]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0081]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3665.0
Loss: 0.008149176836013794
Action 0 - predicted reward: tensor([[1.0968]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0493]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5840.0
Loss: 0.0033110424410551786
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5006]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5651]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6375.0
Loss: 8712.5146484375
KL Divergence: 15.869345664978027
Action 0 - predicted reward: tensor([[2.4896]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5416]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7225.0
Loss: 8419.4697265625
KL Divergence: 15.938820838928223
9799.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12305.0
Loss: 0.047065090388059616
Action 0 - predicted reward: tensor([[0.0318]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9930]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13035.0
Loss: 0.013825287111103535
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1297]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9927]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7410.0
Loss: 0.03308584541082382
Action 0 - predicted reward: tensor([[-0.0282]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0378]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9255.0
Loss: 0.02172756940126419
Greedy
Action 0 - predicted reward: tensor([[-0.1109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9368]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3665.0
Loss: 0.007967129349708557
Action 0 - predicted reward: tensor([[-0.0018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.1520]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5875.0
Loss: 0.003474004566669464
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5226]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5802]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6375.0
Loss: 8717.9296875
KL Divergence: 15.864479064941406
Action 0 - predicted reward: tensor([[2.4952]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5491]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7225.0
Loss: 8418.7197265625
KL Divergence: 15.919989585876465
9899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0824]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6427]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12450.0
Loss: 0.04684208706021309
Action 0 - predicted reward: tensor([[0.0185]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.0136]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13120.0
Loss: 0.017088579013943672
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.5858]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7480.0
Loss: 0.032745350152254105
Action 0 - predicted reward: tensor([[1.5653]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0549]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9325.0
Loss: 0.025218931958079338
Greedy
Action 0 - predicted reward: tensor([[0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.0842]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3665.0
Loss: 0.006948419846594334
Action 0 - predicted reward: tensor([[1.4870]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9764]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5915.0
Loss: 0.004950670059770346
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4880]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2020]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6375.0
Loss: 8716.935546875
KL Divergence: 15.856282234191895
Action 0 - predicted reward: tensor([[2.4670]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7803]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7225.0
Loss: 8417.853515625
KL Divergence: 15.9140625
9999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0521]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4172]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12535.0
Loss: 0.050396282225847244
Action 0 - predicted reward: tensor([[0.2287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8418]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13270.0
Loss: 0.023771559819579124
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.2625]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7515.0
Loss: 0.0320294126868248
Action 0 - predicted reward: tensor([[-0.1810]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9409]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9360.0
Loss: 0.025533359497785568
Greedy
Action 0 - predicted reward: tensor([[-0.0429]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.6956]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3665.0
Loss: 0.006918136961758137
Action 0 - predicted reward: tensor([[1.4690]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9068]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5985.0
Loss: 0.009146438911557198
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5013]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1856]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6375.0
Loss: 8717.5654296875
KL Divergence: 15.843180656433105
Action 0 - predicted reward: tensor([[2.5066]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8861]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7230.0
Loss: 8419.0986328125
KL Divergence: 15.90656852722168
10099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0583]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.1647]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12650.0
Loss: 0.04750577732920647
Action 0 - predicted reward: tensor([[0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.6966]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13345.0
Loss: 0.023851070553064346
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0134]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.6238]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7515.0
Loss: 0.031531669199466705
Action 0 - predicted reward: tensor([[0.0132]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.3379]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9395.0
Loss: 0.024825233966112137
Greedy
Action 0 - predicted reward: tensor([[-0.3177]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0377]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3700.0
Loss: 0.008075880818068981
Action 0 - predicted reward: tensor([[1.3550]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9940]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5990.0
Loss: 0.005561694968491793
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4801]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0855]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6380.0
Loss: 8714.94140625
KL Divergence: 15.834504127502441
Action 0 - predicted reward: tensor([[2.4842]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8517]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7230.0
Loss: 8411.3955078125
KL Divergence: 15.901382446289062
10199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2001]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12765.0
Loss: 0.05065584555268288
Action 0 - predicted reward: tensor([[-0.0175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9332]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13430.0
Loss: 0.029584616422653198
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.6519]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7550.0
Loss: 0.03615507110953331
Action 0 - predicted reward: tensor([[-0.3295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9989]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9430.0
Loss: 0.02098204754292965
Greedy
Action 0 - predicted reward: tensor([[0.0500]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.5214]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3700.0
Loss: 0.006842187140136957
Action 0 - predicted reward: tensor([[-0.0807]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.1003]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5990.0
Loss: 0.004411757458001375
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4748]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9302]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6380.0
Loss: 8715.5029296875
KL Divergence: 15.838541984558105
Action 0 - predicted reward: tensor([[2.5117]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5659]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7230.0
Loss: 8117.03955078125
KL Divergence: 15.895437240600586
10299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8584]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12815.0
Loss: 0.05089105665683746
Action 0 - predicted reward: tensor([[0.0027]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.2566]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13580.0
Loss: 0.0291636623442173
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0693]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9971]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7555.0
Loss: 0.03492163494229317
Action 0 - predicted reward: tensor([[0.0053]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.3475]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9430.0
Loss: 0.020180020481348038
Greedy
Action 0 - predicted reward: tensor([[0.0036]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.4302]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3700.0
Loss: 0.006789725739508867
Action 0 - predicted reward: tensor([[0.6682]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0309]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5995.0
Loss: 0.004010359290987253
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4897]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5521]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6415.0
Loss: 8715.181640625
KL Divergence: 15.829584121704102
Action 0 - predicted reward: tensor([[2.5112]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5682]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7230.0
Loss: 8117.41943359375
KL Divergence: 15.891318321228027
10399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1546]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1108]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12855.0
Loss: 0.04692979156970978
Action 0 - predicted reward: tensor([[-0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9651]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13685.0
Loss: 0.031031116843223572
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1447]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9561]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7560.0
Loss: 0.03366738557815552
Action 0 - predicted reward: tensor([[-0.0286]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.7949]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9465.0
Loss: 0.019177468493580818
Greedy
Action 0 - predicted reward: tensor([[-0.3400]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9442]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3700.0
Loss: 0.006715336814522743
Action 0 - predicted reward: tensor([[1.8461]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1793]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6000.0
Loss: 0.0036422586999833584
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4822]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5472]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6450.0
Loss: 8416.9365234375
KL Divergence: 15.836068153381348
Action 0 - predicted reward: tensor([[2.5187]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5740]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7265.0
Loss: 8116.7109375
KL Divergence: 15.896864891052246
10499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0207]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.8822]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12860.0
Loss: 0.044246166944503784
Action 0 - predicted reward: tensor([[0.0098]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.1967]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13725.0
Loss: 0.028829114511609077
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0217]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1703]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7560.0
Loss: 0.02736879512667656
Action 0 - predicted reward: tensor([[0.4053]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8836]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9610.0
Loss: 0.02332615666091442
Greedy
Action 0 - predicted reward: tensor([[-0.0018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.4669]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3735.0
Loss: 0.006909366697072983
Action 0 - predicted reward: tensor([[1.4817]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0201]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6040.0
Loss: 0.007381774485111237
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4988]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5469]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6450.0
Loss: 8416.443359375
KL Divergence: 15.840508460998535
Action 0 - predicted reward: tensor([[2.4963]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9321]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7265.0
Loss: 8116.73388671875
KL Divergence: 15.888930320739746
10599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0320]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.3899]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12895.0
Loss: 0.047607868909835815
Action 0 - predicted reward: tensor([[-0.0559]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9609]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13845.0
Loss: 0.031339723616838455
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0867]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8560]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7605.0
Loss: 0.031191552057862282
Action 0 - predicted reward: tensor([[0.2895]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-80.8461]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9715.0
Loss: 0.025003859773278236
Greedy
Action 0 - predicted reward: tensor([[-0.2362]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9883]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3735.0
Loss: 0.006816613953560591
Action 0 - predicted reward: tensor([[1.1711]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8013]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6075.0
Loss: 0.010751290246844292
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4934]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5457]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6450.0
Loss: 8416.783203125
KL Divergence: 15.834290504455566
Action 0 - predicted reward: tensor([[2.4907]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7852]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7265.0
Loss: 8118.2626953125
KL Divergence: 15.891260147094727
10699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1103]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8618]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12970.0
Loss: 0.04735163226723671
Action 0 - predicted reward: tensor([[0.0234]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9464]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13930.0
Loss: 0.02800688147544861
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.8986]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7675.0
Loss: 0.03345292806625366
Action 0 - predicted reward: tensor([[0.3530]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.1699]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9750.0
Loss: 0.02354571409523487
Greedy
Action 0 - predicted reward: tensor([[0.9674]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0668]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3735.0
Loss: 0.006677864585071802
Action 0 - predicted reward: tensor([[-0.0369]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9373]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6115.0
Loss: 0.01517010573297739
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4762]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5333]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6450.0
Loss: 8418.1025390625
KL Divergence: 15.836967468261719
Action 0 - predicted reward: tensor([[2.5104]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5705]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7265.0
Loss: 7818.78076171875
KL Divergence: 15.889793395996094
10799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0719]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0843]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13045.0
Loss: 0.04646574705839157
Action 0 - predicted reward: tensor([[0.1163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2319]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13975.0
Loss: 0.027483245357871056
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0115]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.7606]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7720.0
Loss: 0.030438289046287537
Action 0 - predicted reward: tensor([[-0.2186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.5202]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9785.0
Loss: 0.02459796890616417
Greedy
Action 0 - predicted reward: tensor([[-0.0500]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0733]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3735.0
Loss: 0.006629030220210552
Action 0 - predicted reward: tensor([[1.4931]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3256]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6115.0
Loss: 0.014055971056222916
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4595]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1394]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6450.0
Loss: 8412.4453125
KL Divergence: 15.823569297790527
Action 0 - predicted reward: tensor([[2.4976]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5600]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7270.0
Loss: 7819.53857421875
KL Divergence: 15.886847496032715
10899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0440]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.1220]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13160.0
Loss: 0.04958348721265793
Action 0 - predicted reward: tensor([[-0.7603]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1987]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14045.0
Loss: 0.025092031806707382
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0723]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.7518]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7790.0
Loss: 0.033686377108097076
Action 0 - predicted reward: tensor([[0.0897]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2495]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9790.0
Loss: 0.016215387731790543
Greedy
Action 0 - predicted reward: tensor([[0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.4715]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3735.0
Loss: 0.0033643823117017746
Action 0 - predicted reward: tensor([[-0.0108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.7211]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6150.0
Loss: 0.01732538267970085
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4648]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6978]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6450.0
Loss: 8118.79931640625
KL Divergence: 15.835959434509277
Action 0 - predicted reward: tensor([[2.5108]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5710]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7270.0
Loss: 7519.4091796875
KL Divergence: 15.891727447509766
10999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0582]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13280.0
Loss: 0.04711694270372391
Action 0 - predicted reward: tensor([[0.0266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0464]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14085.0
Loss: 0.02126202918589115
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.3726]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7790.0
Loss: 0.027599290013313293
Action 0 - predicted reward: tensor([[-0.0898]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9611]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9865.0
Loss: 0.015489388257265091
Greedy
Action 0 - predicted reward: tensor([[1.0399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9867]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3735.0
Loss: 0.0033299417700618505
Action 0 - predicted reward: tensor([[2.2657]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0425]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6150.0
Loss: 0.016750283539295197
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4675]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6806]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6450.0
Loss: 7818.26953125
KL Divergence: 15.84766674041748
Action 0 - predicted reward: tensor([[2.5282]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5834]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7275.0
Loss: 7519.87109375
KL Divergence: 15.892199516296387
11099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1457]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.7353]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13325.0
Loss: 0.04129038378596306
Action 0 - predicted reward: tensor([[-0.0143]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.8408]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14260.0
Loss: 0.024249287322163582
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0432]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.3125]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7830.0
Loss: 0.024509716778993607
Action 0 - predicted reward: tensor([[-0.2677]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7620]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9865.0
Loss: 0.014045455493032932
Greedy
Action 0 - predicted reward: tensor([[-0.0434]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.4731]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3770.0
Loss: 0.005894850008189678
Action 0 - predicted reward: tensor([[1.7026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0287]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6150.0
Loss: 0.016711391508579254
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4921]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5596]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6450.0
Loss: 7519.78173828125
KL Divergence: 15.843602180480957
Action 0 - predicted reward: tensor([[2.5170]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5884]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7280.0
Loss: 7515.07080078125
KL Divergence: 15.893181800842285
11199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0608]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.2056]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13325.0
Loss: 0.03161698207259178
Action 0 - predicted reward: tensor([[-0.0348]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0886]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14270.0
Loss: 0.024393023923039436
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0480]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.8763]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7865.0
Loss: 0.0233469195663929
Action 0 - predicted reward: tensor([[-0.0502]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.3302]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9940.0
Loss: 0.019754277542233467
Greedy
Action 0 - predicted reward: tensor([[0.0231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.2320]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3770.0
Loss: 0.0038846847601234913
Action 0 - predicted reward: tensor([[1.9762]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0208]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6155.0
Loss: 0.016115203499794006
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4932]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5601]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6450.0
Loss: 7521.5908203125
KL Divergence: 15.83916187286377
Action 0 - predicted reward: tensor([[2.5166]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7877]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 7518.4462890625
KL Divergence: 15.896383285522461
11299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0141]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13365.0
Loss: 0.0316273458302021
Action 0 - predicted reward: tensor([[0.0451]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.1469]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14345.0
Loss: 0.02651902474462986
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.6417]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7980.0
Loss: 0.03012930601835251
Action 0 - predicted reward: tensor([[-0.2026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9489]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9945.0
Loss: 0.02044922672212124
Greedy
Action 0 - predicted reward: tensor([[-0.0167]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.6933]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3770.0
Loss: 0.003967180848121643
Action 0 - predicted reward: tensor([[2.0891]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9792]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6155.0
Loss: 0.01631229557096958
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4526]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0846]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6520.0
Loss: 8118.060546875
KL Divergence: 15.832268714904785
Action 0 - predicted reward: tensor([[2.5306]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5957]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7280.0
Loss: 7519.326171875
KL Divergence: 15.889622688293457
11399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7954]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13400.0
Loss: 0.0335339792072773
Action 0 - predicted reward: tensor([[-0.0043]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.7212]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14380.0
Loss: 0.025938397273421288
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0823]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9909]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8015.0
Loss: 0.027748312801122665
Action 0 - predicted reward: tensor([[-0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.6413]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9945.0
Loss: 0.018801430240273476
Greedy
Action 0 - predicted reward: tensor([[1.4335]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0258]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3770.0
Loss: 4.426570740179159e-05
Action 0 - predicted reward: tensor([[0.0510]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.3404]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6165.0
Loss: 0.016084810718894005
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4452]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0530]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6520.0
Loss: 8118.5595703125
KL Divergence: 15.827361106872559
Action 0 - predicted reward: tensor([[2.5434]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6045]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7280.0
Loss: 7517.25146484375
KL Divergence: 15.892097473144531
11499.
Epsilon Greedy 5%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13450.0
Loss: 0.03498469293117523
Action 0 - predicted reward: tensor([[-0.0630]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9181]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14560.0
Loss: 0.02843639627099037
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0433]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.6688]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8050.0
Loss: 0.027278531342744827
Action 0 - predicted reward: tensor([[-0.1919]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-101.1242]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9945.0
Loss: 0.018703045323491096
Greedy
Action 0 - predicted reward: tensor([[0.3922]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2015]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3805.0
Loss: 2.6238092686980963e-05
Action 0 - predicted reward: tensor([[1.0689]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9865]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6175.0
Loss: 0.01589440554380417
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4301]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0144]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6555.0
Loss: 8415.228515625
KL Divergence: 15.826833724975586
Action 0 - predicted reward: tensor([[2.5210]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8311]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 7218.79443359375
KL Divergence: 15.89360237121582
11599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.2675]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13555.0
Loss: 0.03117920272052288
Action 0 - predicted reward: tensor([[0.0688]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0415]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14565.0
Loss: 0.024317728355526924
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9258]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8050.0
Loss: 0.02373284101486206
Action 0 - predicted reward: tensor([[0.0186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7821]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9985.0
Loss: 0.01576227694749832
Greedy
Action 0 - predicted reward: tensor([[0.0277]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.6229]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3805.0
Loss: 2.0318811948527582e-05
Action 0 - predicted reward: tensor([[3.0853]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8719]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6210.0
Loss: 0.02082836627960205
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4576]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5190]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6555.0
Loss: 8116.99951171875
KL Divergence: 15.824102401733398
Action 0 - predicted reward: tensor([[2.5094]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9243]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 7220.71044921875
KL Divergence: 15.89182186126709
11699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0970]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13595.0
Loss: 0.030169762670993805
Action 0 - predicted reward: tensor([[0.0092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1444]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14575.0
Loss: 0.024195894598960876
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0260]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9704]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8085.0
Loss: 0.020667091012001038
Action 0 - predicted reward: tensor([[1.4158]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9874]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10090.0
Loss: 0.012227416038513184
Greedy
Action 0 - predicted reward: tensor([[-0.2647]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1006]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3805.0
Loss: 1.7560449123266153e-05
Action 0 - predicted reward: tensor([[1.3746]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0357]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6210.0
Loss: 0.02020362578332424
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4507]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5128]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6560.0
Loss: 8117.5478515625
KL Divergence: 15.816356658935547
Action 0 - predicted reward: tensor([[2.4834]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7268]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 7216.6298828125
KL Divergence: 15.87627124786377
11799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1802]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9373]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13635.0
Loss: 0.02694636955857277
Action 0 - predicted reward: tensor([[0.0231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0129]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14580.0
Loss: 0.023066820576786995
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1254]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9038]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8120.0
Loss: 0.01772768422961235
Action 0 - predicted reward: tensor([[-0.1525]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.5965]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10160.0
Loss: 0.01524272933602333
Greedy
Action 0 - predicted reward: tensor([[-0.4984]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0222]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3805.0
Loss: 1.5397641618619673e-05
Action 0 - predicted reward: tensor([[-0.1194]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.3835]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6210.0
Loss: 0.020396556705236435
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4234]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0465]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6560.0
Loss: 8116.65771484375
KL Divergence: 15.816734313964844
Action 0 - predicted reward: tensor([[2.5152]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5659]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7280.0
Loss: 7221.7333984375
KL Divergence: 15.879077911376953
11899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0442]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.6809]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13680.0
Loss: 0.030671993270516396
Action 0 - predicted reward: tensor([[-0.0065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9656]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14660.0
Loss: 0.022989865392446518
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0556]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0837]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8190.0
Loss: 0.014511858113110065
Action 0 - predicted reward: tensor([[-0.9473]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1418]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10230.0
Loss: 0.018758580088615417
Greedy
Action 0 - predicted reward: tensor([[-0.2213]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0148]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3805.0
Loss: 1.3554539691540413e-05
Action 0 - predicted reward: tensor([[-0.0387]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.4636]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6245.0
Loss: 0.02205296978354454
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4438]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5047]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6560.0
Loss: 8117.34521484375
KL Divergence: 15.81494426727295
Action 0 - predicted reward: tensor([[2.4901]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9733]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 7221.81689453125
KL Divergence: 15.869925498962402
11999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.5093]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13790.0
Loss: 0.031367070972919464
Action 0 - predicted reward: tensor([[0.0763]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7186]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14700.0
Loss: 0.023275671526789665
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0103]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.7890]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8190.0
Loss: 0.01302565261721611
Action 0 - predicted reward: tensor([[-0.0389]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8301]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10265.0
Loss: 0.019029712304472923
Greedy
Action 0 - predicted reward: tensor([[0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.0156]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3910.0
Loss: 0.006194993853569031
Action 0 - predicted reward: tensor([[-0.0457]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.8672]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6260.0
Loss: 0.02057664841413498
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4476]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5059]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6560.0
Loss: 8117.76123046875
KL Divergence: 15.81875228881836
Action 0 - predicted reward: tensor([[2.5101]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5654]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7285.0
Loss: 7220.9677734375
KL Divergence: 15.873438835144043
12099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.1657]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13980.0
Loss: 0.034860193729400635
Action 0 - predicted reward: tensor([[0.0494]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.3432]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14715.0
Loss: 0.022780051454901695
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0276]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-98.3044]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8225.0
Loss: 0.013072757050395012
Action 0 - predicted reward: tensor([[-0.0532]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.0623]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10265.0
Loss: 0.018703732639551163
Greedy
Action 0 - predicted reward: tensor([[-0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.4370]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3945.0
Loss: 0.005142995622009039
Action 0 - predicted reward: tensor([[1.6717]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8921]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6260.0
Loss: 0.020670609548687935
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4361]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4989]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 8117.0048828125
KL Divergence: 15.80907917022705
Action 0 - predicted reward: tensor([[2.5169]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5669]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7285.0
Loss: 7222.3603515625
KL Divergence: 15.880600929260254
12199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9558]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14060.0
Loss: 0.036621879786252975
Action 0 - predicted reward: tensor([[-0.0055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.3930]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14810.0
Loss: 0.026155628263950348
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9841]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8260.0
Loss: 0.016366781666874886
Action 0 - predicted reward: tensor([[0.1215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9629]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10265.0
Loss: 0.017346711829304695
Greedy
Action 0 - predicted reward: tensor([[0.0289]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.3379]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3945.0
Loss: 0.0035292631946504116
Action 0 - predicted reward: tensor([[-0.0875]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.7247]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6260.0
Loss: 0.0206176545470953
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4305]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4640]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 8117.25927734375
KL Divergence: 15.82005786895752
Action 0 - predicted reward: tensor([[2.4944]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8233]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7285.0
Loss: 7221.60498046875
KL Divergence: 15.875163078308105
12299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1344]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7373]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14100.0
Loss: 0.03150888532400131
Action 0 - predicted reward: tensor([[-0.0679]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.8773]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14920.0
Loss: 0.02628203108906746
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0238]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.9065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8335.0
Loss: 0.021919725462794304
Action 0 - predicted reward: tensor([[-0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.2627]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10265.0
Loss: 0.01726413704454899
Greedy
Action 0 - predicted reward: tensor([[-0.4573]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.6836]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3980.0
Loss: 0.0026477891951799393
Action 0 - predicted reward: tensor([[-0.0330]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-66.6841]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6260.0
Loss: 0.02053946629166603
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4412]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9473]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6565.0
Loss: 8112.84521484375
KL Divergence: 15.81333065032959
Action 0 - predicted reward: tensor([[2.4925]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5535]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7290.0
Loss: 7222.27294921875
KL Divergence: 15.861451148986816
12399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.8227]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14110.0
Loss: 0.02744300290942192
Action 0 - predicted reward: tensor([[-0.0864]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9272]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14955.0
Loss: 0.02082483284175396
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1039]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1234]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8405.0
Loss: 0.029106607660651207
Action 0 - predicted reward: tensor([[-0.7243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0011]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10305.0
Loss: 0.017729833722114563
Greedy
Action 0 - predicted reward: tensor([[-0.0163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5957]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3980.0
Loss: 0.0017024852568283677
Action 0 - predicted reward: tensor([[0.0720]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.8473]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6260.0
Loss: 0.01792503334581852
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4607]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5121]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 8117.76806640625
KL Divergence: 15.823573112487793
Action 0 - predicted reward: tensor([[2.4970]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0110]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7290.0
Loss: 6923.33984375
KL Divergence: 15.871790885925293
12499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0990]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.7633]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14220.0
Loss: 0.03420685604214668
Action 0 - predicted reward: tensor([[0.0098]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0532]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14960.0
Loss: 0.020103294402360916
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2688]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2980]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8405.0
Loss: 0.02577129378914833
Action 0 - predicted reward: tensor([[0.3241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8911]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10315.0
Loss: 0.016957562416791916
Greedy
Action 0 - predicted reward: tensor([[-0.0129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.3828]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3980.0
Loss: 0.0009785692673176527
Action 0 - predicted reward: tensor([[0.3450]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9523]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6300.0
Loss: 0.021272992715239525
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4475]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8301]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6565.0
Loss: 8117.91748046875
KL Divergence: 15.817676544189453
Action 0 - predicted reward: tensor([[2.4969]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9869]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7295.0
Loss: 6923.03125
KL Divergence: 15.862238883972168
12599.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14330.0
Loss: 0.03155418112874031
Action 0 - predicted reward: tensor([[-0.0107]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.5969]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15070.0
Loss: 0.022563012316823006
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0225]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.0638]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8405.0
Loss: 0.020863126963377
Action 0 - predicted reward: tensor([[-0.0882]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.8017]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10385.0
Loss: 0.020461486652493477
Greedy
Action 0 - predicted reward: tensor([[-0.0592]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.7632]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3980.0
Loss: 0.00046309459139592946
Action 0 - predicted reward: tensor([[-1.1082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.5775]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6300.0
Loss: 0.021400295197963715
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4683]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5223]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 8118.01611328125
KL Divergence: 15.81784439086914
Action 0 - predicted reward: tensor([[2.4919]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8009]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7295.0
Loss: 6922.81298828125
KL Divergence: 15.868511199951172
12699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1665]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8735]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14330.0
Loss: 0.024587679654359818
Action 0 - predicted reward: tensor([[0.1031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0186]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15075.0
Loss: 0.016597451642155647
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.7745]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8480.0
Loss: 0.025619618594646454
Action 0 - predicted reward: tensor([[0.0422]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9503]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10420.0
Loss: 0.023469090461730957
Greedy
Action 0 - predicted reward: tensor([[0.0450]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.0846]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3980.0
Loss: 0.0002130349021172151
Action 0 - predicted reward: tensor([[1.5001]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9493]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6335.0
Loss: 0.024836309254169464
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4843]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5477]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 8118.0419921875
KL Divergence: 15.814360618591309
Action 0 - predicted reward: tensor([[2.4954]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9455]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7300.0
Loss: 6921.95654296875
KL Divergence: 15.87563705444336
12799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0492]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.3127]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14405.0
Loss: 0.02479497529566288
Action 0 - predicted reward: tensor([[-0.0565]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.6726]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15125.0
Loss: 0.014159681275486946
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0317]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.4518]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8530.0
Loss: 0.02389269322156906
Action 0 - predicted reward: tensor([[-3.3452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8145]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10425.0
Loss: 0.018557149916887283
Greedy
Action 0 - predicted reward: tensor([[0.0150]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.8119]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3980.0
Loss: 0.00011584300955291837
Action 0 - predicted reward: tensor([[2.6908]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9557]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6340.0
Loss: 0.024413691833615303
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4804]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5394]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 8117.734375
KL Divergence: 15.815324783325195
Action 0 - predicted reward: tensor([[2.5163]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5735]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7300.0
Loss: 6922.11083984375
KL Divergence: 15.873136520385742
12899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0389]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14475.0
Loss: 0.027811503037810326
Action 0 - predicted reward: tensor([[-0.0094]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.5420]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15170.0
Loss: 0.012936941348016262
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0583]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1060]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8530.0
Loss: 0.022976966574788094
Action 0 - predicted reward: tensor([[0.0631]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.1582]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10425.0
Loss: 0.017091838642954826
Greedy
Action 0 - predicted reward: tensor([[0.1278]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0141]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4050.0
Loss: 0.005087123718112707
Action 0 - predicted reward: tensor([[3.2001]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9777]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6350.0
Loss: 0.022435428574681282
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4872]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5396]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 7815.89404296875
KL Divergence: 15.820941925048828
Action 0 - predicted reward: tensor([[2.4936]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9860]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7300.0
Loss: 6923.34619140625
KL Divergence: 15.858424186706543
12999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1004]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0402]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14580.0
Loss: 0.02475419081747532
Action 0 - predicted reward: tensor([[0.0612]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8407]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15230.0
Loss: 0.016793956980109215
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.9974]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8530.0
Loss: 0.016983795911073685
Action 0 - predicted reward: tensor([[-0.2187]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1104]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10425.0
Loss: 0.01724846661090851
Greedy
Action 0 - predicted reward: tensor([[0.1098]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9220]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4050.0
Loss: 0.0002717363240662962
Action 0 - predicted reward: tensor([[0.0680]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.7458]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6350.0
Loss: 0.01870274730026722
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4995]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5557]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 7818.60595703125
KL Divergence: 15.824498176574707
Action 0 - predicted reward: tensor([[2.5116]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5708]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7300.0
Loss: 6923.57958984375
KL Divergence: 15.863389015197754
13099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1690]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9328]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14650.0
Loss: 0.028195571154356003
Action 0 - predicted reward: tensor([[-0.0237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.5167]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15340.0
Loss: 0.021457821130752563
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.9965]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8535.0
Loss: 0.015805238857865334
Action 0 - predicted reward: tensor([[-0.7818]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8485]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10425.0
Loss: 0.016828123480081558
Greedy
Action 0 - predicted reward: tensor([[0.0662]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.1865]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4050.0
Loss: 8.512800559401512e-05
Action 0 - predicted reward: tensor([[2.2481]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1382]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6350.0
Loss: 0.017979668453335762
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4847]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6598]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6565.0
Loss: 7813.5830078125
KL Divergence: 15.819284439086914
Action 0 - predicted reward: tensor([[2.4897]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8113]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7300.0
Loss: 6625.03857421875
KL Divergence: 15.857949256896973
13199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1714]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.9905]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14795.0
Loss: 0.03379783406853676
Action 0 - predicted reward: tensor([[0.0314]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2388]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15415.0
Loss: 0.02409336343407631
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.8542]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8570.0
Loss: 0.013721113093197346
Action 0 - predicted reward: tensor([[-0.0064]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.5745]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10425.0
Loss: 0.01413818635046482
Greedy
Action 0 - predicted reward: tensor([[0.4279]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2469]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4050.0
Loss: 5.488855458679609e-05
Action 0 - predicted reward: tensor([[2.0169]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0185]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6350.0
Loss: 0.017520029097795486
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5152]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5729]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 7518.97998046875
KL Divergence: 15.810672760009766
Action 0 - predicted reward: tensor([[2.4852]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8997]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7305.0
Loss: 6625.3310546875
KL Divergence: 15.861047744750977
13299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0532]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14860.0
Loss: 0.03194731846451759
Action 0 - predicted reward: tensor([[-0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.4385]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15465.0
Loss: 0.021624654531478882
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0222]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9567]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8605.0
Loss: 0.016959454864263535
Action 0 - predicted reward: tensor([[0.3357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6915]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10460.0
Loss: 0.01485629752278328
Greedy
Action 0 - predicted reward: tensor([[-0.0043]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.8583]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4050.0
Loss: 4.091921073268168e-05
Action 0 - predicted reward: tensor([[0.0362]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.2414]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6360.0
Loss: 0.01723710261285305
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4812]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0039]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6565.0
Loss: 7516.1005859375
KL Divergence: 15.814359664916992
Action 0 - predicted reward: tensor([[2.5072]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5629]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7305.0
Loss: 6625.3701171875
KL Divergence: 15.851031303405762
13399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.6184]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14970.0
Loss: 0.032725680619478226
Action 0 - predicted reward: tensor([[0.0481]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.6023]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15575.0
Loss: 0.022168153896927834
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0548]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0170]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8640.0
Loss: 0.020226113498210907
Action 0 - predicted reward: tensor([[0.8636]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8039]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10495.0
Loss: 0.013768484815955162
Greedy
Action 0 - predicted reward: tensor([[-0.3954]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9919]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4050.0
Loss: 3.305263089714572e-05
Action 0 - predicted reward: tensor([[0.0387]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.5377]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6365.0
Loss: 0.016988391056656837
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4900]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5446]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 7521.29248046875
KL Divergence: 15.816919326782227
Action 0 - predicted reward: tensor([[2.5133]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8497]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7305.0
Loss: 6323.791015625
KL Divergence: 15.864803314208984
13499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0624]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9665]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15010.0
Loss: 0.02472858317196369
Action 0 - predicted reward: tensor([[0.0632]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1459]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15620.0
Loss: 0.02340688370168209
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0217]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.5148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8710.0
Loss: 0.020637227222323418
Action 0 - predicted reward: tensor([[-0.1481]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0150]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10495.0
Loss: 0.01318004634231329
Greedy
Action 0 - predicted reward: tensor([[-1.9451]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9837]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4050.0
Loss: 2.8690084945992567e-05
Action 0 - predicted reward: tensor([[-0.0413]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.2069]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6365.0
Loss: 0.01717977784574032
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5112]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5725]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 7520.33203125
KL Divergence: 15.818526268005371
Action 0 - predicted reward: tensor([[2.5203]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9297]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7310.0
Loss: 6323.27001953125
KL Divergence: 15.867077827453613
13599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1339]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9836]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15050.0
Loss: 0.023608334362506866
Action 0 - predicted reward: tensor([[0.0143]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.2001]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15705.0
Loss: 0.025596242398023605
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0117]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.1580]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8710.0
Loss: 0.019703960046172142
Action 0 - predicted reward: tensor([[0.1803]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8053]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10500.0
Loss: 0.013450590893626213
Greedy
Action 0 - predicted reward: tensor([[1.2213]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0571]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4085.0
Loss: 5.0189475587103516e-05
Action 0 - predicted reward: tensor([[-0.0367]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9242]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6375.0
Loss: 0.014345115050673485
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5113]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9594]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6565.0
Loss: 6921.91845703125
KL Divergence: 15.82528305053711
Action 0 - predicted reward: tensor([[2.5367]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5924]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 6324.1357421875
KL Divergence: 15.852670669555664
13699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0476]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.4852]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15055.0
Loss: 0.021018072962760925
Action 0 - predicted reward: tensor([[0.0399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.1677]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15815.0
Loss: 0.025438735261559486
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0810]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8989]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8780.0
Loss: 0.029286617413163185
Action 0 - predicted reward: tensor([[-0.9731]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7690]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10535.0
Loss: 0.012073810212314129
Greedy
Action 0 - predicted reward: tensor([[0.0756]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.5868]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4120.0
Loss: 0.001419059350155294
Action 0 - predicted reward: tensor([[2.2077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9750]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6375.0
Loss: 0.011499867774546146
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5158]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3822]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6565.0
Loss: 6920.39208984375
KL Divergence: 15.817177772521973
Action 0 - predicted reward: tensor([[2.5378]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5941]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 6324.0166015625
KL Divergence: 15.855302810668945
13799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1376]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9604]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15060.0
Loss: 0.022635864093899727
Action 0 - predicted reward: tensor([[-0.0879]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3166]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15895.0
Loss: 0.02354489080607891
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.9573]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8820.0
Loss: 0.026764314621686935
Action 0 - predicted reward: tensor([[-0.0258]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.6612]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10570.0
Loss: 0.009895239025354385
Greedy
Action 0 - predicted reward: tensor([[-0.0155]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.2764]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4120.0
Loss: 6.209727871464565e-05
Action 0 - predicted reward: tensor([[1.5308]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9478]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6375.0
Loss: 0.01030005794018507
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5255]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5899]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6575.0
Loss: 6921.6728515625
KL Divergence: 15.808161735534668
Action 0 - predicted reward: tensor([[2.5300]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9021]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7315.0
Loss: 5724.68798828125
KL Divergence: 15.86004638671875
13899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0171]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.8403]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15135.0
Loss: 0.028301259502768517
Action 0 - predicted reward: tensor([[0.0380]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6271]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15965.0
Loss: 0.020003288984298706
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.1349]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8820.0
Loss: 0.02379870042204857
Action 0 - predicted reward: tensor([[-0.0243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.9650]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10605.0
Loss: 0.013850056566298008
Greedy
Action 0 - predicted reward: tensor([[-0.0679]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.0867]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4120.0
Loss: 3.037171518371906e-05
Action 0 - predicted reward: tensor([[2.0495]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9865]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6380.0
Loss: 0.01030021719634533
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5207]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1108]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6575.0
Loss: 6920.94091796875
KL Divergence: 15.81251049041748
Action 0 - predicted reward: tensor([[2.5338]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6015]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 5726.0458984375
KL Divergence: 15.855833053588867
13999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4626]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9455]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15220.0
Loss: 0.025992652401328087
Action 0 - predicted reward: tensor([[-0.0124]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.9679]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15970.0
Loss: 0.0199737548828125
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0221]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.4539]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8855.0
Loss: 0.017175229266285896
Action 0 - predicted reward: tensor([[0.1084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9577]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10605.0
Loss: 0.009852456860244274
Greedy
Action 0 - predicted reward: tensor([[0.0102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-75.3355]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4120.0
Loss: 2.599311847006902e-05
Action 0 - predicted reward: tensor([[-0.0044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.8133]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6420.0
Loss: 0.012554188258945942
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5185]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0671]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6575.0
Loss: 6919.95263671875
KL Divergence: 15.81540584564209
Action 0 - predicted reward: tensor([[2.5352]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5946]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 5727.0078125
KL Divergence: 15.853535652160645
14099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1553]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.8394]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15365.0
Loss: 0.033082157373428345
Action 0 - predicted reward: tensor([[0.0169]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.1775]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16080.0
Loss: 0.016554610803723335
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0306]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.6471]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8855.0
Loss: 0.016887487843632698
Action 0 - predicted reward: tensor([[-0.0703]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.4144]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10640.0
Loss: 0.009810244664549828
Greedy
Action 0 - predicted reward: tensor([[-0.0765]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.1258]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4120.0
Loss: 2.183509241149295e-05
Action 0 - predicted reward: tensor([[1.6799]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5836]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6420.0
Loss: 0.01148362085223198
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5299]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6023]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6575.0
Loss: 6920.447265625
KL Divergence: 15.801789283752441
Action 0 - predicted reward: tensor([[2.5411]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5858]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 5726.6171875
KL Divergence: 15.850238800048828
14199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0541]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.7324]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 15500.0
Loss: 0.04083032160997391
Action 0 - predicted reward: tensor([[0.0519]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7340]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16295.0
Loss: 0.025281034409999847
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0331]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-69.2436]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8855.0
Loss: 0.016678083688020706
Action 0 - predicted reward: tensor([[-1.1250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0023]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10675.0
Loss: 0.006625254638493061
Greedy
Action 0 - predicted reward: tensor([[1.4339]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1205]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4120.0
Loss: 1.9641767721623182e-05
Action 0 - predicted reward: tensor([[-0.0169]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.8502]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6420.0
Loss: 0.011167948134243488
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5322]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1487]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6575.0
Loss: 6919.69287109375
KL Divergence: 15.809575080871582
Action 0 - predicted reward: tensor([[2.5189]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8399]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7315.0
Loss: 5726.060546875
KL Divergence: 15.850541114807129
14299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2182]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0811]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15545.0
Loss: 0.03659769892692566
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16370.0
Loss: 0.019402867183089256
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0206]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.4296]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8855.0
Loss: 0.013447041623294353
Action 0 - predicted reward: tensor([[-0.0200]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.6427]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10710.0
Loss: 0.0069245086051523685
Greedy
Action 0 - predicted reward: tensor([[-1.8815]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0278]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4120.0
Loss: 1.7001189917209558e-05
Action 0 - predicted reward: tensor([[3.0175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9998]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6420.0
Loss: 0.0035698148421943188
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5486]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6156]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6580.0
Loss: 6617.7626953125
KL Divergence: 15.807329177856445
Action 0 - predicted reward: tensor([[2.5327]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5864]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 5727.111328125
KL Divergence: 15.854588508605957
14399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0286]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.0632]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15660.0
Loss: 0.03677773475646973
Action 0 - predicted reward: tensor([[0.1209]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9892]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16585.0
Loss: 0.03861065208911896
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1438]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1472]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8855.0
Loss: 0.013426666148006916
Action 0 - predicted reward: tensor([[0.8108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9660]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10750.0
Loss: 0.006650406401604414
Greedy
Action 0 - predicted reward: tensor([[-0.0371]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0101]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4120.0
Loss: 1.5483830793527886e-05
Action 0 - predicted reward: tensor([[-0.0222]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.5062]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6430.0
Loss: 0.00023070814495440573
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5412]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4612]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6580.0
Loss: 6617.99072265625
KL Divergence: 15.808197975158691
Action 0 - predicted reward: tensor([[2.5312]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5870]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 5727.279296875
KL Divergence: 15.840911865234375
14499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3775]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8239]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15800.0
Loss: 0.038428086787462234
Action 0 - predicted reward: tensor([[-0.0224]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.0528]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16625.0
Loss: 0.0328809916973114
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0063]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.0012]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8855.0
Loss: 0.013239763677120209
Action 0 - predicted reward: tensor([[0.0493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.5776]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10785.0
Loss: 0.006971240974962711
Greedy
Action 0 - predicted reward: tensor([[-0.6499]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9931]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4120.0
Loss: 1.4070857105252799e-05
Action 0 - predicted reward: tensor([[0.0157]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.6944]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6430.0
Loss: 6.919982843101025e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5510]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6207]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6580.0
Loss: 6318.62353515625
KL Divergence: 15.806976318359375
Action 0 - predicted reward: tensor([[2.5436]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5991]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 5427.8701171875
KL Divergence: 15.847610473632812
14599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1056]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.0052]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15805.0
Loss: 0.03790142387151718
Action 0 - predicted reward: tensor([[0.0415]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7845]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16660.0
Loss: 0.03104868344962597
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0222]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.2065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8855.0
Loss: 0.013074258342385292
Action 0 - predicted reward: tensor([[0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-71.0490]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10785.0
Loss: 0.0065888711251318455
Greedy
Action 0 - predicted reward: tensor([[-0.0012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-71.7073]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4120.0
Loss: 1.531366797280498e-05
Action 0 - predicted reward: tensor([[0.0810]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.2253]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6430.0
Loss: 4.376828292151913e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5699]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6321]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6580.0
Loss: 6318.912109375
KL Divergence: 15.81440544128418
Action 0 - predicted reward: tensor([[2.5248]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7274]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7315.0
Loss: 5427.37890625
KL Divergence: 15.84214973449707
14699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0980]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7296]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15915.0
Loss: 0.040355321019887924
Action 0 - predicted reward: tensor([[0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6799]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16670.0
Loss: 0.02495979703962803
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0535]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8900.0
Loss: 0.01900199055671692
Action 0 - predicted reward: tensor([[0.0629]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.7676]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10785.0
Loss: 0.0066141607239842415
Greedy
Action 0 - predicted reward: tensor([[-0.2794]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0086]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4120.0
Loss: 1.586395410413388e-05
Action 0 - predicted reward: tensor([[0.0291]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.3334]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6430.0
Loss: 3.2078245567390695e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5489]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2136]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6580.0
Loss: 6320.1181640625
KL Divergence: 15.794976234436035
Action 0 - predicted reward: tensor([[2.5250]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8929]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7315.0
Loss: 5427.6884765625
KL Divergence: 15.848921775817871
14799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2242]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7108]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15925.0
Loss: 0.03762669861316681
Action 0 - predicted reward: tensor([[-0.3447]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5585]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16740.0
Loss: 0.031385939568281174
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0217]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1221]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8900.0
Loss: 0.010691246017813683
Action 0 - predicted reward: tensor([[-0.0568]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.1816]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10820.0
Loss: 0.011048353277146816
Greedy
Action 0 - predicted reward: tensor([[0.4801]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0003]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4120.0
Loss: 1.3999053408042528e-05
Action 0 - predicted reward: tensor([[0.0290]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.4112]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6430.0
Loss: 2.6025947590824217e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5785]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6376]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6580.0
Loss: 6318.015625
KL Divergence: 15.798938751220703
Action 0 - predicted reward: tensor([[2.5545]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6066]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 5427.08642578125
KL Divergence: 15.841806411743164
14899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1255]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1465]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16005.0
Loss: 0.04010447859764099
Action 0 - predicted reward: tensor([[0.0069]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.8570]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16850.0
Loss: 0.028928227722644806
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0688]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0085]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8975.0
Loss: 0.010886142030358315
Action 0 - predicted reward: tensor([[-0.9736]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1945]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10855.0
Loss: 0.010626530274748802
Greedy
Action 0 - predicted reward: tensor([[-0.0301]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.9148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4120.0
Loss: 1.2284737749723718e-05
Action 0 - predicted reward: tensor([[0.2052]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7293]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6470.0
Loss: 5.359781425795518e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5549]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9865]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6580.0
Loss: 6318.94140625
KL Divergence: 15.799236297607422
Action 0 - predicted reward: tensor([[2.5695]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6219]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7315.0
Loss: 5424.310546875
KL Divergence: 15.84192943572998
14999.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16040.0
Loss: 0.04668295010924339
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16925.0
Loss: 0.03835856169462204
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0456]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0730]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9010.0
Loss: 0.013102371245622635
Action 0 - predicted reward: tensor([[0.0791]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.8872]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10855.0
Loss: 0.010452980175614357
Greedy
Action 0 - predicted reward: tensor([[-1.9043]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9467]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4120.0
Loss: 3.518008816172369e-05
Action 0 - predicted reward: tensor([[0.0185]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5868]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6510.0
Loss: 5.207002323004417e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5665]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6318]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6580.0
Loss: 6314.4375
KL Divergence: 15.792691230773926
Action 0 - predicted reward: tensor([[2.5610]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6240]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7320.0
Loss: 5425.1953125
KL Divergence: 15.846461296081543
15099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0622]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0509]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16115.0
Loss: 0.044236332178115845
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2984]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17030.0
Loss: 0.04019999876618385
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6463]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9045.0
Loss: 0.010741258971393108
Action 0 - predicted reward: tensor([[-0.0588]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.6625]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10855.0
Loss: 0.010173286311328411
Greedy
Action 0 - predicted reward: tensor([[-1.3725]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0338]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4120.0
Loss: 1.2618470464076381e-05
Action 0 - predicted reward: tensor([[-0.0284]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.8769]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6520.0
Loss: 3.60836238542106e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5770]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6356]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6580.0
Loss: 6313.447265625
KL Divergence: 15.804561614990234
Action 0 - predicted reward: tensor([[2.5241]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8219]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7320.0
Loss: 5427.27783203125
KL Divergence: 15.827410697937012
15199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0934]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.3325]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16160.0
Loss: 0.04303787276148796
Action 0 - predicted reward: tensor([[-0.0370]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.7784]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17105.0
Loss: 0.04195314645767212
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1925]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0474]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9085.0
Loss: 0.01513215433806181
Action 0 - predicted reward: tensor([[0.0112]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-73.7144]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10855.0
Loss: 0.010083144530653954
Greedy
Action 0 - predicted reward: tensor([[-0.9607]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1128]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4155.0
Loss: 0.0045024617575109005
Action 0 - predicted reward: tensor([[-0.0144]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.9594]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6525.0
Loss: 2.7796289941761643e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5615]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6209]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6615.0
Loss: 6619.02392578125
KL Divergence: 15.787396430969238
Action 0 - predicted reward: tensor([[2.4975]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0748]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7320.0
Loss: 5429.71142578125
KL Divergence: 15.829537391662598
15299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1654]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8739]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16165.0
Loss: 0.04043036699295044
Action 0 - predicted reward: tensor([[0.0914]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9741]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17145.0
Loss: 0.04042074456810951
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0981]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1329]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9155.0
Loss: 0.017696628347039223
Action 0 - predicted reward: tensor([[-0.3346]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0293]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10890.0
Loss: 0.006853571627289057
Greedy
Action 0 - predicted reward: tensor([[-0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.7743]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4190.0
Loss: 0.0002466186706442386
Action 0 - predicted reward: tensor([[2.4764]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9617]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6525.0
Loss: 2.285700429638382e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5473]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9476]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6615.0
Loss: 6618.73583984375
KL Divergence: 15.786802291870117
Action 0 - predicted reward: tensor([[2.5322]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5880]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7320.0
Loss: 5428.01953125
KL Divergence: 15.829280853271484
15399.
Epsilon Greedy 5%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16280.0
Loss: 0.0397942028939724
Action 0 - predicted reward: tensor([[0.0704]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.0580]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17255.0
Loss: 0.03793637827038765
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0609]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9396]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9155.0
Loss: 0.017116231843829155
Action 0 - predicted reward: tensor([[0.0303]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.2454]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10895.0
Loss: 0.006774172652512789
Greedy
Action 0 - predicted reward: tensor([[-0.0094]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9038]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4190.0
Loss: 3.16794503305573e-05
Action 0 - predicted reward: tensor([[0.7175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0538]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6530.0
Loss: 1.910098762891721e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5750]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3019]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 6016.421875
KL Divergence: 15.789250373840332
Action 0 - predicted reward: tensor([[2.5174]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9054]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7320.0
Loss: 5428.46630859375
KL Divergence: 15.824692726135254
15499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.6134]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16315.0
Loss: 0.041638437658548355
Action 0 - predicted reward: tensor([[-0.0825]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.8075]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17335.0
Loss: 0.03760458156466484
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.6212]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9155.0
Loss: 0.01442505232989788
Action 0 - predicted reward: tensor([[-1.6601]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9888]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10900.0
Loss: 0.006724610924720764
Greedy
Action 0 - predicted reward: tensor([[-0.1627]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9788]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4190.0
Loss: 2.106754072883632e-05
Action 0 - predicted reward: tensor([[0.0233]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.0646]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6530.0
Loss: 2.166022932215128e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5693]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1514]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 6016.6376953125
KL Divergence: 15.798221588134766
Action 0 - predicted reward: tensor([[2.5292]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5863]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7320.0
Loss: 5428.515625
KL Divergence: 15.83363151550293
15599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.6764]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16320.0
Loss: 0.04017547145485878
Action 0 - predicted reward: tensor([[0.1460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1772]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17480.0
Loss: 0.0406077541410923
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0315]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.0837]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9155.0
Loss: 0.013631731271743774
Action 0 - predicted reward: tensor([[0.0492]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-92.6440]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10935.0
Loss: 0.0036080835852771997
Greedy
Action 0 - predicted reward: tensor([[-0.6006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9871]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4190.0
Loss: 1.7477503206464462e-05
Action 0 - predicted reward: tensor([[1.6479]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0020]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6535.0
Loss: 1.9384608094696887e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6078]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6562]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5715.2919921875
KL Divergence: 15.806350708007812
Action 0 - predicted reward: tensor([[2.5050]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9548]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7320.0
Loss: 5429.017578125
KL Divergence: 15.829275131225586
15699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9213]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16425.0
Loss: 0.03970984369516373
Action 0 - predicted reward: tensor([[-0.0117]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.5405]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17515.0
Loss: 0.03994937241077423
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0491]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0256]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9160.0
Loss: 0.011469435878098011
Action 0 - predicted reward: tensor([[0.6855]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9452]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10970.0
Loss: 0.0035680269356817007
Greedy
Action 0 - predicted reward: tensor([[-0.2913]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9878]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4225.0
Loss: 0.00010198858944931999
Action 0 - predicted reward: tensor([[0.0275]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.4159]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6540.0
Loss: 1.6698684703442268e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6107]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6652]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5713.54833984375
KL Divergence: 15.812021255493164
Action 0 - predicted reward: tensor([[2.5170]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9466]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7325.0
Loss: 5427.791015625
KL Divergence: 15.819759368896484
15799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0712]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3359]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16435.0
Loss: 0.03625250607728958
Action 0 - predicted reward: tensor([[0.0217]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8418]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17520.0
Loss: 0.039762865751981735
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0090]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.0247]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9195.0
Loss: 0.010580894537270069
Action 0 - predicted reward: tensor([[-0.7158]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0042]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11010.0
Loss: 0.00890518818050623
Greedy
Action 0 - predicted reward: tensor([[-0.9515]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9271]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4260.0
Loss: 0.00349802547134459
Action 0 - predicted reward: tensor([[1.4545]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9681]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6540.0
Loss: 1.4341650967253372e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6156]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6758]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5712.6689453125
KL Divergence: 15.807950973510742
Action 0 - predicted reward: tensor([[2.5402]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1549]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7325.0
Loss: 5425.41064453125
KL Divergence: 15.831364631652832
15899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.9938]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16615.0
Loss: 0.03911905363202095
Action 0 - predicted reward: tensor([[0.0365]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.9512]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17590.0
Loss: 0.03876854479312897
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0634]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9553]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9195.0
Loss: 0.010315514169633389
Action 0 - predicted reward: tensor([[0.9146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9951]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11010.0
Loss: 0.006328665651381016
Greedy
Action 0 - predicted reward: tensor([[0.0399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-103.3423]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4295.0
Loss: 0.007225100416690111
Action 0 - predicted reward: tensor([[0.4565]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9087]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6540.0
Loss: 1.3467623830365483e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6153]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6798]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5712.06884765625
KL Divergence: 15.801547050476074
Action 0 - predicted reward: tensor([[2.5445]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8937]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7325.0
Loss: 5424.36865234375
KL Divergence: 15.835673332214355
15999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0335]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0491]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16760.0
Loss: 0.03715205937623978
Action 0 - predicted reward: tensor([[0.0750]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17600.0
Loss: 0.03507465124130249
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0343]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7496]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9195.0
Loss: 0.010115689598023891
Action 0 - predicted reward: tensor([[0.0376]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.4958]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11010.0
Loss: 0.005018031690269709
Greedy
Action 0 - predicted reward: tensor([[0.6108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9661]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4295.0
Loss: 0.007229098584502935
Action 0 - predicted reward: tensor([[2.2446]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0041]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6540.0
Loss: 1.2250433428562246e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5987]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2114]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5712.3994140625
KL Divergence: 15.80806827545166
Action 0 - predicted reward: tensor([[2.5341]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9687]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7325.0
Loss: 5425.68798828125
KL Divergence: 15.828453063964844
16099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0373]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.0620]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16805.0
Loss: 0.03557270020246506
Action 0 - predicted reward: tensor([[-0.0780]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9226]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17710.0
Loss: 0.03887726739048958
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0341]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.3508]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9195.0
Loss: 0.010018773376941681
Action 0 - predicted reward: tensor([[0.0226]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.5895]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11010.0
Loss: 0.005153575912117958
Greedy
Action 0 - predicted reward: tensor([[-0.0083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.9682]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4295.0
Loss: 0.007285855710506439
Action 0 - predicted reward: tensor([[0.0466]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.4649]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6540.0
Loss: 1.1272377378190868e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6236]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6772]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5711.73388671875
KL Divergence: 15.800707817077637
Action 0 - predicted reward: tensor([[2.5388]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8662]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7325.0
Loss: 5425.11083984375
KL Divergence: 15.82450008392334
16199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4306]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0624]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16930.0
Loss: 0.033979177474975586
Action 0 - predicted reward: tensor([[-0.0021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8050]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 17785.0
Loss: 0.04324961453676224
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0914]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0635]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9270.0
Loss: 0.010279309935867786
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11045.0
Loss: 0.004697907250374556
Greedy
Action 0 - predicted reward: tensor([[0.0137]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.7082]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4295.0
Loss: 0.00710353022441268
Action 0 - predicted reward: tensor([[0.0041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.1683]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6545.0
Loss: 1.8841406927094795e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5964]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2762]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5713.6640625
KL Divergence: 15.80735969543457
Action 0 - predicted reward: tensor([[2.5523]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6110]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7325.0
Loss: 5425.97998046875
KL Divergence: 15.815909385681152
16299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0531]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.9508]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17005.0
Loss: 0.03305242955684662
Action 0 - predicted reward: tensor([[0.0179]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.2380]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17820.0
Loss: 0.04603210836648941
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0481]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0013]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9305.0
Loss: 0.010153011418879032
Action 0 - predicted reward: tensor([[0.1175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0040]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11125.0
Loss: 0.006366810295730829
Greedy
Action 0 - predicted reward: tensor([[-0.0154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.6929]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4330.0
Loss: 0.007255960255861282
Action 0 - predicted reward: tensor([[1.4940]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0133]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6585.0
Loss: 0.003872303292155266
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6227]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6732]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5712.46337890625
KL Divergence: 15.789994239807129
Action 0 - predicted reward: tensor([[2.5647]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6111]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7325.0
Loss: 5426.65771484375
KL Divergence: 15.829462051391602
16399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1204]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.5053]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17040.0
Loss: 0.031236035749316216
Action 0 - predicted reward: tensor([[0.0434]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.6051]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17895.0
Loss: 0.047871481627225876
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.5516]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9305.0
Loss: 0.006714730989187956
Action 0 - predicted reward: tensor([[-0.2137]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.6482]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11200.0
Loss: 0.004660457838326693
Greedy
Action 0 - predicted reward: tensor([[-0.3160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0122]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4330.0
Loss: 0.0034847594797611237
Action 0 - predicted reward: tensor([[-0.0121]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.0781]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6585.0
Loss: 0.0006343264831230044
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5867]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3189]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5715.677734375
KL Divergence: 15.786209106445312
Action 0 - predicted reward: tensor([[2.5370]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0853]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7330.0
Loss: 5426.15966796875
KL Divergence: 15.82260799407959
16499.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17055.0
Loss: 0.028000209480524063
Action 0 - predicted reward: tensor([[-0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.5679]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17965.0
Loss: 0.04938419163227081
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0122]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.9112]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9375.0
Loss: 0.010744119994342327
Action 0 - predicted reward: tensor([[-0.5414]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0769]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11200.0
Loss: 0.005310704931616783
Greedy
Action 0 - predicted reward: tensor([[-0.5912]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9693]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4330.0
Loss: 0.0034445745404809713
Action 0 - predicted reward: tensor([[-0.0092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.1271]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6585.0
Loss: 0.0001574354973854497
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5765]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2019]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5717.1435546875
KL Divergence: 15.786553382873535
Action 0 - predicted reward: tensor([[2.5265]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9468]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7330.0
Loss: 5426.79931640625
KL Divergence: 15.823165893554688
16599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0271]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6301]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17200.0
Loss: 0.02680143527686596
Action 0 - predicted reward: tensor([[0.0014]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.1020]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18045.0
Loss: 0.05083894729614258
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0449]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0939]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9415.0
Loss: 0.010349777527153492
Action 0 - predicted reward: tensor([[0.9803]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0700]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11270.0
Loss: 0.004584251902997494
Greedy
Action 0 - predicted reward: tensor([[-0.0159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.2461]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4330.0
Loss: 0.0033739558421075344
Action 0 - predicted reward: tensor([[0.0041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.8415]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6585.0
Loss: 5.405131378211081e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5806]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1493]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5716.8603515625
KL Divergence: 15.790452003479004
Action 0 - predicted reward: tensor([[2.5452]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5993]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7330.0
Loss: 5427.80078125
KL Divergence: 15.822206497192383
16699.
Epsilon Greedy 5%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 17310.0
Loss: 0.023423917591571808
Action 0 - predicted reward: tensor([[-0.0992]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9643]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18135.0
Loss: 0.041629303246736526
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.4612]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9450.0
Loss: 0.010393246077001095
Action 0 - predicted reward: tensor([[-0.6696]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0241]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11270.0
Loss: 0.005213005933910608
Greedy
Action 0 - predicted reward: tensor([[-0.4705]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9803]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4330.0
Loss: 0.003361690090969205
Action 0 - predicted reward: tensor([[-0.0602]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.4932]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6590.0
Loss: 2.714901347644627e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5780]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2506]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5718.4814453125
KL Divergence: 15.780460357666016
Action 0 - predicted reward: tensor([[2.5201]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8021]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7340.0
Loss: 5426.87109375
KL Divergence: 15.815760612487793
16799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0135]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17355.0
Loss: 0.029029088094830513
Action 0 - predicted reward: tensor([[0.0160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.3168]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18275.0
Loss: 0.03724862262606621
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6525]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9485.0
Loss: 0.010380139574408531
Action 0 - predicted reward: tensor([[-0.8455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9251]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11270.0
Loss: 0.004430065397173166
Greedy
Action 0 - predicted reward: tensor([[-0.0159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.2788]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4330.0
Loss: 0.0033515379764139652
Action 0 - predicted reward: tensor([[1.9182]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9932]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6595.0
Loss: 1.6920195776037872e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5555]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0489]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5721.0224609375
KL Divergence: 15.77860164642334
Action 0 - predicted reward: tensor([[2.5100]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9450]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7350.0
Loss: 5428.2705078125
KL Divergence: 15.811986923217773
16899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0663]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.3328]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17535.0
Loss: 0.02833840623497963
Action 0 - predicted reward: tensor([[-0.0411]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9634]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18310.0
Loss: 0.03361879661679268
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0128]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7450]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9490.0
Loss: 0.007044605910778046
Action 0 - predicted reward: tensor([[0.0447]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.4987]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11305.0
Loss: 0.00441822549328208
Greedy
Action 0 - predicted reward: tensor([[-1.8863]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0028]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4330.0
Loss: 0.003297510789707303
Action 0 - predicted reward: tensor([[-0.0237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.5069]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6595.0
Loss: 1.3090298125462141e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5633]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0534]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5721.57568359375
KL Divergence: 15.779646873474121
Action 0 - predicted reward: tensor([[2.5106]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0378]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7385.0
Loss: 5726.876953125
KL Divergence: 15.813623428344727
16999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2600]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3525]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17575.0
Loss: 0.02682006172835827
Action 0 - predicted reward: tensor([[0.0419]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.8478]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18450.0
Loss: 0.040451135486364365
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7810]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9595.0
Loss: 0.017031243070960045
Action 0 - predicted reward: tensor([[-0.0960]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0420]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11305.0
Loss: 0.004355217795819044
Greedy
Action 0 - predicted reward: tensor([[0.3336]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8882]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4330.0
Loss: 0.0032902078237384558
Action 0 - predicted reward: tensor([[1.5295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0267]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6600.0
Loss: 1.1155194442835636e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5750]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6426]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5722.0078125
KL Divergence: 15.775135040283203
Action 0 - predicted reward: tensor([[2.5011]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9380]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7385.0
Loss: 5727.94189453125
KL Divergence: 15.815670013427734
17099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0542]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.5451]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17695.0
Loss: 0.02422742359340191
Action 0 - predicted reward: tensor([[0.0008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1002]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18520.0
Loss: 0.033187028020620346
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.7829]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9595.0
Loss: 0.013385451398789883
Action 0 - predicted reward: tensor([[-0.0149]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.7854]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11410.0
Loss: 0.008151259273290634
Greedy
Action 0 - predicted reward: tensor([[0.0083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.5086]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4330.0
Loss: 0.0032852673903107643
Action 0 - predicted reward: tensor([[0.7694]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9976]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6600.0
Loss: 1.0235029549221508e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5581]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8047]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5717.05810546875
KL Divergence: 15.776317596435547
Action 0 - predicted reward: tensor([[2.5047]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8636]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7385.0
Loss: 5727.361328125
KL Divergence: 15.807042121887207
17199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0961]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.6499]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17730.0
Loss: 0.017915107309818268
Action 0 - predicted reward: tensor([[-0.0262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.3759]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18595.0
Loss: 0.03326971083879471
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9702]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9595.0
Loss: 0.013383538462221622
Action 0 - predicted reward: tensor([[-0.7862]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9444]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11410.0
Loss: 0.007914981804788113
Greedy
Action 0 - predicted reward: tensor([[-1.8993]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0454]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4330.0
Loss: 0.0032522312831133604
Action 0 - predicted reward: tensor([[1.5466]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0157]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6605.0
Loss: 9.555807082506362e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5636]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6316]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5722.18310546875
KL Divergence: 15.76913833618164
Action 0 - predicted reward: tensor([[2.5174]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9005]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7385.0
Loss: 5726.67822265625
KL Divergence: 15.805768966674805
17299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0290]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0033]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17880.0
Loss: 0.018533051013946533
Action 0 - predicted reward: tensor([[-0.0482]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9373]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18670.0
Loss: 0.033458609133958817
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0423]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.5488]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9630.0
Loss: 0.013276824727654457
Action 0 - predicted reward: tensor([[-1.8638]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9460]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11445.0
Loss: 0.007919540628790855
Greedy
Action 0 - predicted reward: tensor([[0.0033]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.5040]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4330.0
Loss: 0.0032566972076892853
Action 0 - predicted reward: tensor([[0.0113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.6414]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6605.0
Loss: 7.690255188208539e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5769]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6269]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5722.42919921875
KL Divergence: 15.7745361328125
Action 0 - predicted reward: tensor([[2.5244]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9661]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7385.0
Loss: 5725.9658203125
KL Divergence: 15.80851936340332
17399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0988]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8323]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17915.0
Loss: 0.018750904127955437
Action 0 - predicted reward: tensor([[0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.0606]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18775.0
Loss: 0.030936576426029205
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.7607]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9630.0
Loss: 0.01312607154250145
Action 0 - predicted reward: tensor([[0.0119]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.0898]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11480.0
Loss: 0.011846303008496761
Greedy
Action 0 - predicted reward: tensor([[-0.0201]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.0078]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4365.0
Loss: 0.0037690442986786366
Action 0 - predicted reward: tensor([[0.0190]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7564]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6610.0
Loss: 7.560166977782501e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5714]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6286]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5721.14208984375
KL Divergence: 15.769527435302734
Action 0 - predicted reward: tensor([[2.5242]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8248]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7385.0
Loss: 5726.056640625
KL Divergence: 15.805617332458496
17499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.9317]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18075.0
Loss: 0.016438772901892662
Action 0 - predicted reward: tensor([[0.0366]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1374]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18780.0
Loss: 0.026412440463900566
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0277]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9916]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9630.0
Loss: 0.013034109957516193
Action 0 - predicted reward: tensor([[-0.0564]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.5398]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11550.0
Loss: 0.012205791659653187
Greedy
Action 0 - predicted reward: tensor([[-0.8266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9904]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4365.0
Loss: 0.003228351939469576
Action 0 - predicted reward: tensor([[0.4417]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8434]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6610.0
Loss: 7.063736575219082e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5767]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6300]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5721.71923828125
KL Divergence: 15.7706298828125
Action 0 - predicted reward: tensor([[2.5162]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9084]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7385.0
Loss: 5726.87548828125
KL Divergence: 15.799321174621582
17599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3707]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6122]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18215.0
Loss: 0.025307565927505493
Action 0 - predicted reward: tensor([[0.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.3501]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18785.0
Loss: 0.0261753648519516
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0054]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.4557]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9635.0
Loss: 0.012899789959192276
Action 0 - predicted reward: tensor([[-1.8472]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0358]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11585.0
Loss: 0.011933996342122555
Greedy
Action 0 - predicted reward: tensor([[0.0212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.0906]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4365.0
Loss: 0.003262326819822192
Action 0 - predicted reward: tensor([[0.0131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.6837]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6615.0
Loss: 6.175383077788865e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5741]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6286]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5722.46728515625
KL Divergence: 15.763091087341309
Action 0 - predicted reward: tensor([[2.5073]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8007]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7385.0
Loss: 5727.1201171875
KL Divergence: 15.799337387084961
17699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0259]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.6454]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18325.0
Loss: 0.03171919286251068
Action 0 - predicted reward: tensor([[0.2044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9454]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18865.0
Loss: 0.026170998811721802
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9760]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9640.0
Loss: 0.009759442880749702
Action 0 - predicted reward: tensor([[-0.3181]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0273]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11585.0
Loss: 0.011394763365387917
Greedy
Action 0 - predicted reward: tensor([[0.0055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.2104]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4400.0
Loss: 0.0034481764305382967
Action 0 - predicted reward: tensor([[0.0200]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.0023]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 0.0036280439235270023
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5473]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8961]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5722.69970703125
KL Divergence: 15.767221450805664
Action 0 - predicted reward: tensor([[2.5107]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2364]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7385.0
Loss: 5727.4609375
KL Divergence: 15.801033020019531
17799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0751]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5984]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18335.0
Loss: 0.01879429630935192
Action 0 - predicted reward: tensor([[-0.0354]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.8804]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18975.0
Loss: 0.03391950950026512
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0325]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7705]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9640.0
Loss: 0.009709427133202553
Action 0 - predicted reward: tensor([[0.0188]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9814]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11625.0
Loss: 0.012852897867560387
Greedy
Action 0 - predicted reward: tensor([[-0.3476]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0096]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4400.0
Loss: 0.0032295447308570147
Action 0 - predicted reward: tensor([[-0.0122]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.2872]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6665.0
Loss: 0.003512916387990117
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5461]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9819]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5723.314453125
KL Divergence: 15.757128715515137
Action 0 - predicted reward: tensor([[2.5104]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2412]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7390.0
Loss: 5727.693359375
KL Divergence: 15.80836296081543
17899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0758]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9334]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18375.0
Loss: 0.017848148941993713
Action 0 - predicted reward: tensor([[-0.0834]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.3114]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19095.0
Loss: 0.03835463523864746
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0066]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7296]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9675.0
Loss: 0.010583738796412945
Action 0 - predicted reward: tensor([[-0.9666]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8670]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11660.0
Loss: 0.011291819624602795
Greedy
Action 0 - predicted reward: tensor([[0.4229]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9873]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4400.0
Loss: 3.055807974305935e-05
Action 0 - predicted reward: tensor([[0.0018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.8710]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6680.0
Loss: 0.0030713791493326426
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5416]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9896]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5724.34130859375
KL Divergence: 15.758563041687012
Action 0 - predicted reward: tensor([[2.5077]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8587]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7390.0
Loss: 5727.56591796875
KL Divergence: 15.80363941192627
17999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0133]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.4206]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18530.0
Loss: 0.019761456176638603
Action 0 - predicted reward: tensor([[0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.6644]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19170.0
Loss: 0.034180860966444016
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0123]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.4171]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9685.0
Loss: 0.009675129316747189
Action 0 - predicted reward: tensor([[-0.0087]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.9203]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11700.0
Loss: 0.011782399378716946
Greedy
Action 0 - predicted reward: tensor([[-0.6748]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9949]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4400.0
Loss: 2.701162884477526e-05
Action 0 - predicted reward: tensor([[1.4605]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0497]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6680.0
Loss: 0.0034435829147696495
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5571]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6187]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5722.791015625
KL Divergence: 15.747845649719238
Action 0 - predicted reward: tensor([[2.5007]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9643]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7390.0
Loss: 5728.28466796875
KL Divergence: 15.789810180664062
18099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0319]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.1613]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18570.0
Loss: 0.018585480749607086
Action 0 - predicted reward: tensor([[0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1522]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19175.0
Loss: 0.03381691128015518
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0893]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9720.0
Loss: 0.009868613444268703
Action 0 - predicted reward: tensor([[-0.4582]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9832]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11700.0
Loss: 0.0112724294885993
Greedy
Action 0 - predicted reward: tensor([[0.0072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.5000]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4400.0
Loss: 1.5899153368081897e-05
Action 0 - predicted reward: tensor([[0.0011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.0702]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6690.0
Loss: 0.003382153809070587
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5708]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6287]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5721.08740234375
KL Divergence: 15.755301475524902
Action 0 - predicted reward: tensor([[2.5189]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5834]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7390.0
Loss: 5728.0107421875
KL Divergence: 15.794927597045898
18199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3716]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8947]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18680.0
Loss: 0.012068424373865128
Action 0 - predicted reward: tensor([[-0.0019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9377]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19250.0
Loss: 0.03131931647658348
Epsilon Greedy 1%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9720.0
Loss: 0.009746222756803036
Action 0 - predicted reward: tensor([[0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.4776]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11700.0
Loss: 0.011148388497531414
Greedy
Action 0 - predicted reward: tensor([[-0.0161]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.7961]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4400.0
Loss: 1.3120119547238573e-05
Action 0 - predicted reward: tensor([[0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.6356]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6700.0
Loss: 0.0034722203854471445
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5775]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6372]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5722.37109375
KL Divergence: 15.753040313720703
Action 0 - predicted reward: tensor([[2.5132]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5751]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7390.0
Loss: 5728.68212890625
KL Divergence: 15.800115585327148
18299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0116]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.1101]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18765.0
Loss: 0.01353323832154274
Action 0 - predicted reward: tensor([[0.0590]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8956]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19320.0
Loss: 0.028710845857858658
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0936]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0557]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9755.0
Loss: 0.009759285487234592
Action 0 - predicted reward: tensor([[-0.1819]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9126]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11735.0
Loss: 0.01109797041863203
Greedy
Action 0 - predicted reward: tensor([[0.2563]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0431]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4435.0
Loss: 0.004213748965412378
Action 0 - predicted reward: tensor([[-0.0089]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6230]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6700.0
Loss: 0.00332212564535439
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5788]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6275]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5722.2412109375
KL Divergence: 15.752366065979004
Action 0 - predicted reward: tensor([[2.5025]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0827]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7390.0
Loss: 5727.533203125
KL Divergence: 15.791380882263184
18399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0270]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8459]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18850.0
Loss: 0.01266610249876976
Action 0 - predicted reward: tensor([[0.0255]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.8096]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19425.0
Loss: 0.02877519093453884
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0498]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9665]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9755.0
Loss: 0.009677711874246597
Action 0 - predicted reward: tensor([[-0.1151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9720]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11735.0
Loss: 0.010940042324364185
Greedy
Action 0 - predicted reward: tensor([[-1.0094]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0445]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4435.0
Loss: 2.236193722637836e-05
Action 0 - predicted reward: tensor([[1.0861]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0108]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6700.0
Loss: 0.003305787919089198
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5684]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6286]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5721.53173828125
KL Divergence: 15.750646591186523
Action 0 - predicted reward: tensor([[2.5150]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8343]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7395.0
Loss: 5726.4892578125
KL Divergence: 15.798596382141113
18499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0683]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.3241]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18925.0
Loss: 0.008272544480860233
Action 0 - predicted reward: tensor([[0.0013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1512]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19430.0
Loss: 0.023930910974740982
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0299]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9825.0
Loss: 0.010421665385365486
Action 0 - predicted reward: tensor([[0.0383]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.8790]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11770.0
Loss: 0.011259577237069607
Greedy
Action 0 - predicted reward: tensor([[-0.0014]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.8088]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4435.0
Loss: 1.4184388419380412e-05
Action 0 - predicted reward: tensor([[0.0151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.3044]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6705.0
Loss: 0.003288301872089505
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5655]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6164]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5721.37451171875
KL Divergence: 15.746129989624023
Action 0 - predicted reward: tensor([[2.5493]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6127]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7395.0
Loss: 5724.89697265625
KL Divergence: 15.806459426879883
18599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2674]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8719]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18965.0
Loss: 0.007823772728443146
Action 0 - predicted reward: tensor([[0.0116]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.9056]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19515.0
Loss: 0.02733800932765007
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1846]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9711]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9860.0
Loss: 0.006425279192626476
Action 0 - predicted reward: tensor([[-0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.5353]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11805.0
Loss: 0.01100096944719553
Greedy
Action 0 - predicted reward: tensor([[0.0666]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9731]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4435.0
Loss: 1.136961600423092e-05
Action 0 - predicted reward: tensor([[-0.0232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.9339]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6705.0
Loss: 0.003272769972681999
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5651]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6332]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5721.84521484375
KL Divergence: 15.751286506652832
Action 0 - predicted reward: tensor([[2.5331]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9911]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7395.0
Loss: 5724.7744140625
KL Divergence: 15.796924591064453
18699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.5018]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19035.0
Loss: 0.007939656265079975
Action 0 - predicted reward: tensor([[-0.0172]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0654]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19625.0
Loss: 0.02725020982325077
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0041]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9865.0
Loss: 0.006400625221431255
Action 0 - predicted reward: tensor([[-0.3295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9891]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11805.0
Loss: 0.01088699046522379
Greedy
Action 0 - predicted reward: tensor([[-0.0059]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4508]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4435.0
Loss: 9.707277058623731e-06
Action 0 - predicted reward: tensor([[2.4455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9850]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6715.0
Loss: 0.0032809923868626356
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5402]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0441]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5724.35986328125
KL Divergence: 15.746817588806152
Action 0 - predicted reward: tensor([[2.5360]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8405]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7395.0
Loss: 5723.892578125
KL Divergence: 15.803293228149414
18799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8426]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19140.0
Loss: 0.004894248675554991
Action 0 - predicted reward: tensor([[0.0996]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0365]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19735.0
Loss: 0.024411918595433235
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.0751]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9905.0
Loss: 0.006430318579077721
Action 0 - predicted reward: tensor([[-0.0171]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.6138]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11805.0
Loss: 0.0075842938385903835
Greedy
Action 0 - predicted reward: tensor([[-0.0321]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0480]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4435.0
Loss: 8.388577043660916e-06
Action 0 - predicted reward: tensor([[-0.0043]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.9487]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6750.0
Loss: 0.0032715594861656427
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5354]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9842]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5723.666015625
KL Divergence: 15.740190505981445
Action 0 - predicted reward: tensor([[2.5688]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6273]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7395.0
Loss: 5722.55517578125
KL Divergence: 15.799606323242188
18899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0522]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.4073]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19180.0
Loss: 0.004301308188587427
Action 0 - predicted reward: tensor([[-0.0142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.3360]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19770.0
Loss: 0.027485549449920654
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0740]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9985.0
Loss: 0.006677616853266954
Action 0 - predicted reward: tensor([[-1.3474]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9605]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11805.0
Loss: 0.0032369885593652725
Greedy
Action 0 - predicted reward: tensor([[0.0135]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.4963]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4470.0
Loss: 0.0039870720356702805
Action 0 - predicted reward: tensor([[0.0069]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.7758]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6785.0
Loss: 0.003291622968390584
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5300]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0534]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5725.787109375
KL Divergence: 15.738639831542969
Action 0 - predicted reward: tensor([[2.5714]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6370]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7400.0
Loss: 5719.59912109375
KL Divergence: 15.802128791809082
18999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9772]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19195.0
Loss: 0.004255581181496382
Action 0 - predicted reward: tensor([[-0.0452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9996]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19775.0
Loss: 0.027330296114087105
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.7375]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10055.0
Loss: 0.009906771592795849
Action 0 - predicted reward: tensor([[0.5652]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0577]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11805.0
Loss: 0.0031849173828959465
Greedy
Action 0 - predicted reward: tensor([[-0.7838]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0173]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4470.0
Loss: 0.0034978752955794334
Action 0 - predicted reward: tensor([[-0.0029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.7673]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6790.0
Loss: 0.003251343034207821
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5289]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9898]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5725.53515625
KL Divergence: 15.739120483398438
Action 0 - predicted reward: tensor([[2.5534]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0282]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7400.0
Loss: 5722.54931640625
KL Divergence: 15.810272216796875
19099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0776]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1176]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19305.0
Loss: 0.00436200387775898
Action 0 - predicted reward: tensor([[-0.0264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9468]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19815.0
Loss: 0.02280339226126671
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0585]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0331]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10060.0
Loss: 0.006933827884495258
Action 0 - predicted reward: tensor([[-0.0561]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.8049]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11875.0
Loss: 0.003193248063325882
Greedy
Action 0 - predicted reward: tensor([[-1.0656]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4470.0
Loss: 0.0035436339676380157
Action 0 - predicted reward: tensor([[1.3577]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9970]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6825.0
Loss: 0.0033525368198752403
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5552]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6064]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5725.08544921875
KL Divergence: 15.736321449279785
Action 0 - predicted reward: tensor([[2.5527]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8345]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7400.0
Loss: 5721.57568359375
KL Divergence: 15.808297157287598
19199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1466]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19310.0
Loss: 0.004268665798008442
Action 0 - predicted reward: tensor([[0.4468]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9397]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19850.0
Loss: 0.025480646640062332
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0265]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9678]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10065.0
Loss: 0.006380493752658367
Action 0 - predicted reward: tensor([[-1.0101]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0283]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11945.0
Loss: 0.00391817232593894
Greedy
Action 0 - predicted reward: tensor([[-0.1919]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.3701]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4505.0
Loss: 0.003478808794170618
Action 0 - predicted reward: tensor([[0.0460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9847]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6830.0
Loss: 0.00326443649828434
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5525]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6082]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5724.5341796875
KL Divergence: 15.729023933410645
Action 0 - predicted reward: tensor([[2.5824]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6288]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7400.0
Loss: 5721.48583984375
KL Divergence: 15.798606872558594
19299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0898]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0157]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19345.0
Loss: 0.004267355892807245
Action 0 - predicted reward: tensor([[0.1183]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0982]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19920.0
Loss: 0.025322016328573227
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-85.1629]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10070.0
Loss: 0.006381570361554623
Action 0 - predicted reward: tensor([[-0.5059]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0059]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11945.0
Loss: 8.825396071188152e-05
Greedy
Action 0 - predicted reward: tensor([[3.3200e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-74.8670]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4540.0
Loss: 0.0038906363770365715
Action 0 - predicted reward: tensor([[2.0793]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0180]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6830.0
Loss: 0.003255909075960517
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5711]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6212]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5424.931640625
KL Divergence: 15.728601455688477
Action 0 - predicted reward: tensor([[2.6037]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6545]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7400.0
Loss: 5717.62939453125
KL Divergence: 15.800405502319336
19399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.2303]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19385.0
Loss: 0.0042331162840127945
Action 0 - predicted reward: tensor([[-0.0201]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0477]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19995.0
Loss: 0.024610089138150215
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3479]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9939]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10105.0
Loss: 0.0032974081113934517
Action 0 - predicted reward: tensor([[-1.0707]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0192]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11985.0
Loss: 0.0013274456141516566
Greedy
Action 0 - predicted reward: tensor([[-0.4522]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0592]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4540.0
Loss: 0.0035489671863615513
Action 0 - predicted reward: tensor([[-0.0054]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.5072]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6830.0
Loss: 0.003242738777771592
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5338]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8746]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5426.50634765625
KL Divergence: 15.7351655960083
Action 0 - predicted reward: tensor([[2.5908]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6471]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7405.0
Loss: 5719.08544921875
KL Divergence: 15.807693481445312
19499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0638]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.7758]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19635.0
Loss: 0.010721378028392792
Action 0 - predicted reward: tensor([[0.0121]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.2011]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20145.0
Loss: 0.03608640283346176
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0112]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.1374]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10105.0
Loss: 0.0032028257846832275
Action 0 - predicted reward: tensor([[0.0441]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9797]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11990.0
Loss: 0.00017329792899545282
Greedy
Action 0 - predicted reward: tensor([[0.0966]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9935]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4540.0
Loss: 0.003453184152022004
Action 0 - predicted reward: tensor([[0.0104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.0185]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6835.0
Loss: 0.003220743266865611
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5277]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9331]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5425.904296875
KL Divergence: 15.741292953491211
Action 0 - predicted reward: tensor([[2.5612]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6345]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7405.0
Loss: 5720.6796875
KL Divergence: 15.79909896850586
19599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.0124]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19750.0
Loss: 0.013723189942538738
Action 0 - predicted reward: tensor([[-0.0430]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2559]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20225.0
Loss: 0.03502301126718521
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0218]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0055]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10105.0
Loss: 0.0032012707088142633
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11990.0
Loss: 3.754673889488913e-05
Greedy
Action 0 - predicted reward: tensor([[0.0280]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.5866]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4540.0
Loss: 0.003377581015229225
Action 0 - predicted reward: tensor([[3.0496]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0338]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6840.0
Loss: 0.0032020725775510073
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5269]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0049]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5427.046875
KL Divergence: 15.730021476745605
Action 0 - predicted reward: tensor([[2.5580]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0194]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7405.0
Loss: 5722.01123046875
KL Divergence: 15.796185493469238
19699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0115]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.2332]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19750.0
Loss: 0.01230106595903635
Action 0 - predicted reward: tensor([[0.0028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.5564]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20375.0
Loss: 0.037233684211969376
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3395]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9763]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10140.0
Loss: 0.003361815819516778
Action 0 - predicted reward: tensor([[0.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.8315]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12025.0
Loss: 0.003645435208454728
Greedy
Action 0 - predicted reward: tensor([[-0.2247]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0107]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4540.0
Loss: 0.0033675392623990774
Action 0 - predicted reward: tensor([[0.0005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.0902]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6845.0
Loss: 0.0031891055405139923
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5303]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2060]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5426.9638671875
KL Divergence: 15.733055114746094
Action 0 - predicted reward: tensor([[2.5939]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6493]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7405.0
Loss: 5716.7236328125
KL Divergence: 15.800708770751953
19799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0694]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9259]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19825.0
Loss: 0.012061595916748047
Action 0 - predicted reward: tensor([[-0.0285]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.5015]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20380.0
Loss: 0.03455378860235214
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1651]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0038]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10210.0
Loss: 0.006012838799506426
Action 0 - predicted reward: tensor([[-0.4060]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0405]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12100.0
Loss: 0.0056463442742824554
Greedy
Action 0 - predicted reward: tensor([[-0.1745]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0010]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4575.0
Loss: 0.005534780211746693
Action 0 - predicted reward: tensor([[0.0487]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5162]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6850.0
Loss: 0.003185170004144311
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5189]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1527]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5427.7763671875
KL Divergence: 15.719017028808594
Action 0 - predicted reward: tensor([[2.5754]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1163]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7405.0
Loss: 5716.8876953125
KL Divergence: 15.79530143737793
19899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0531]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9503]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19940.0
Loss: 0.00816944520920515
Action 0 - predicted reward: tensor([[-0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8303]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20450.0
Loss: 0.03834542632102966
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0588]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9116]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10210.0
Loss: 0.003286491148173809
Action 0 - predicted reward: tensor([[0.2334]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.4750]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12140.0
Loss: 0.005078176036477089
Greedy
Action 0 - predicted reward: tensor([[0.0541]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.9889]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4575.0
Loss: 0.003452953416854143
Action 0 - predicted reward: tensor([[-0.0016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.8656]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6850.0
Loss: 0.0031819408759474754
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5253]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9810]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5427.33154296875
KL Divergence: 15.724974632263184
Action 0 - predicted reward: tensor([[2.5728]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2622]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7405.0
Loss: 5719.02001953125
KL Divergence: 15.795036315917969
19999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0341]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0062]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19985.0
Loss: 0.009268240071833134
Action 0 - predicted reward: tensor([[-0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9338]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20560.0
Loss: 0.041190460324287415
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1717]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10245.0
Loss: 0.0034697442315518856
Action 0 - predicted reward: tensor([[-0.9815]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0036]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12145.0
Loss: 0.003954592160880566
Greedy
Action 0 - predicted reward: tensor([[0.0059]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.9206]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4575.0
Loss: 0.0034646186977624893
Action 0 - predicted reward: tensor([[-0.0100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6974]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6850.0
Loss: 0.0031706795562058687
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5389]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8773]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5425.96630859375
KL Divergence: 15.727702140808105
Action 0 - predicted reward: tensor([[2.5714]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6321]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7410.0
Loss: 5720.90576171875
KL Divergence: 15.782541275024414
20099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1311]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0470]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20055.0
Loss: 0.010446203872561455
Action 0 - predicted reward: tensor([[-0.0089]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.8056]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20565.0
Loss: 0.04048480838537216
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.8404]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10315.0
Loss: 0.0032922762911766768
Action 0 - predicted reward: tensor([[0.0849]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.6381]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12220.0
Loss: 0.01309459563344717
Greedy
Action 0 - predicted reward: tensor([[0.0104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.5625]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4575.0
Loss: 0.0034120562486350536
Action 0 - predicted reward: tensor([[1.6902]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0176]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6855.0
Loss: 0.0031764209270477295
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5401]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0524]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5424.84716796875
KL Divergence: 15.728498458862305
Action 0 - predicted reward: tensor([[2.5579]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0000]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 5721.53076171875
KL Divergence: 15.789617538452148
20199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.9748]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20090.0
Loss: 0.007670511491596699
Action 0 - predicted reward: tensor([[-0.0129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.9919]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20660.0
Loss: 0.036638498306274414
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0039]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-74.2423]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10350.0
Loss: 0.004325343295931816
Action 0 - predicted reward: tensor([[-2.0145]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9953]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12230.0
Loss: 0.012737896293401718
Greedy
Action 0 - predicted reward: tensor([[0.0976]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0053]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4575.0
Loss: 0.0034116446040570736
Action 0 - predicted reward: tensor([[-0.0487]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.0206]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6855.0
Loss: 0.00317021319642663
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5539]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2188]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5422.00439453125
KL Divergence: 15.723780632019043
Action 0 - predicted reward: tensor([[2.5578]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8979]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 5720.666015625
KL Divergence: 15.782816886901855
20299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0794]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.3781]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20175.0
Loss: 0.007654315326362848
Action 0 - predicted reward: tensor([[0.0011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9509]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20765.0
Loss: 0.03766901418566704
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0053]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.1799]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10350.0
Loss: 0.003293923567980528
Action 0 - predicted reward: tensor([[-0.0184]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.3559]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12265.0
Loss: 0.013373121619224548
Greedy
Action 0 - predicted reward: tensor([[-0.5979]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0045]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4575.0
Loss: 0.003472062759101391
Action 0 - predicted reward: tensor([[-0.0009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.2168]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6860.0
Loss: 0.0031621784437447786
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5713]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1849]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5418.41259765625
KL Divergence: 15.732394218444824
Action 0 - predicted reward: tensor([[2.5652]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0482]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 5718.91943359375
KL Divergence: 15.790478706359863
20399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9567]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20220.0
Loss: 0.007582089863717556
Action 0 - predicted reward: tensor([[0.0076]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.9258]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20870.0
Loss: 0.03907731547951698
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6453]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10385.0
Loss: 0.003170279785990715
Action 0 - predicted reward: tensor([[1.9398]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0053]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12340.0
Loss: 0.011567618697881699
Greedy
Action 0 - predicted reward: tensor([[-0.4102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0212]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4575.0
Loss: 0.0034357085824012756
Action 0 - predicted reward: tensor([[1.0654]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8765]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6860.0
Loss: 0.003168937284499407
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6004]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6480]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5419.48681640625
KL Divergence: 15.720175743103027
Action 0 - predicted reward: tensor([[2.5638]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1578]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 5715.92578125
KL Divergence: 15.795035362243652
20499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1924]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.7817]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20225.0
Loss: 0.009946214966475964
Action 0 - predicted reward: tensor([[-0.1123]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9066]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20985.0
Loss: 0.04268057644367218
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0101]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.5720]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10385.0
Loss: 0.003252588212490082
Action 0 - predicted reward: tensor([[0.0016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.3033]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12380.0
Loss: 0.011406684294342995
Greedy
Action 0 - predicted reward: tensor([[-1.1478]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0172]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4575.0
Loss: 0.0032968055456876755
Action 0 - predicted reward: tensor([[0.3995]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0155]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6865.0
Loss: 0.003152014221996069
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5739]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1612]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5416.83251953125
KL Divergence: 15.728432655334473
Action 0 - predicted reward: tensor([[2.5741]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9663]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 5715.8994140625
KL Divergence: 15.796292304992676
20599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1054]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9730]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20305.0
Loss: 0.011623604223132133
Action 0 - predicted reward: tensor([[0.0480]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0691]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21100.0
Loss: 0.04277234897017479
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.2404]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10385.0
Loss: 0.0032303647603839636
Action 0 - predicted reward: tensor([[0.0382]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.9522]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12425.0
Loss: 0.011259541846811771
Greedy
Action 0 - predicted reward: tensor([[0.0101]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.1426]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4575.0
Loss: 0.0033362123649567366
Action 0 - predicted reward: tensor([[-0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.9983]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6870.0
Loss: 0.0031499790493398905
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5980]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6655]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5415.11328125
KL Divergence: 15.733847618103027
Action 0 - predicted reward: tensor([[2.6040]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6603]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7410.0
Loss: 5715.3232421875
KL Divergence: 15.799159049987793
20699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.4719]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20350.0
Loss: 0.014235353097319603
Action 0 - predicted reward: tensor([[0.0729]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0493]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21195.0
Loss: 0.041361115872859955
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.5925]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10420.0
Loss: 0.0032412789296358824
Action 0 - predicted reward: tensor([[-1.0691]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9226]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12425.0
Loss: 0.011290940456092358
Greedy
Action 0 - predicted reward: tensor([[-0.5306]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9126]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4575.0
Loss: 0.003300440963357687
Action 0 - predicted reward: tensor([[0.0074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.4600]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6875.0
Loss: 0.003152837511152029
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6114]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6673]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5414.296875
KL Divergence: 15.73648738861084
Action 0 - predicted reward: tensor([[2.6099]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6698]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7410.0
Loss: 5712.86669921875
KL Divergence: 15.794474601745605
20799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5516]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0564]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20385.0
Loss: 0.011092870496213436
Action 0 - predicted reward: tensor([[-0.0102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.8371]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21375.0
Loss: 0.046562083065509796
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0519]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9820]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10455.0
Loss: 0.0031927491072565317
Action 0 - predicted reward: tensor([[-0.7983]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8297]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12495.0
Loss: 0.016204148530960083
Greedy
Action 0 - predicted reward: tensor([[0.0205]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-68.0148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4575.0
Loss: 0.0033339837100356817
Action 0 - predicted reward: tensor([[1.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0842]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6885.0
Loss: 0.0031535557936877012
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5917]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2046]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5414.22705078125
KL Divergence: 15.732389450073242
Action 0 - predicted reward: tensor([[2.6146]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6789]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7410.0
Loss: 5709.7685546875
KL Divergence: 15.803613662719727
20899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4485]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9468]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20470.0
Loss: 0.010980220511555672
Action 0 - predicted reward: tensor([[0.1617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1867]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21445.0
Loss: 0.04436745494604111
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0225]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9417]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10490.0
Loss: 0.00490555539727211
Action 0 - predicted reward: tensor([[-0.0484]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.0878]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12530.0
Loss: 0.015011562965810299
Greedy
Action 0 - predicted reward: tensor([[0.1749]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0293]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4575.0
Loss: 0.003290139837190509
Action 0 - predicted reward: tensor([[1.2123]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0035]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6920.0
Loss: 0.005400759167969227
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5952]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0013]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5411.66259765625
KL Divergence: 15.74287223815918
Action 0 - predicted reward: tensor([[2.6148]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0993]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 5705.27001953125
KL Divergence: 15.805821418762207
20999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.3682]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20505.0
Loss: 0.012332220561802387
Action 0 - predicted reward: tensor([[-0.0117]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.0267]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21485.0
Loss: 0.04282306507229805
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0096]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.7010]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10525.0
Loss: 0.00757485581561923
Action 0 - predicted reward: tensor([[0.0856]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.5359]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12605.0
Loss: 0.015574787743389606
Greedy
Action 0 - predicted reward: tensor([[-0.1901]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9779]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4610.0
Loss: 0.0066545805893838406
Action 0 - predicted reward: tensor([[-0.0464]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.1421]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6925.0
Loss: 0.0038891511503607035
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6208]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6805]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5413.67578125
KL Divergence: 15.7483491897583
Action 0 - predicted reward: tensor([[2.6186]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0016]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 5405.22607421875
KL Divergence: 15.807421684265137
21099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0096]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0526]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20550.0
Loss: 0.010857800953090191
Action 0 - predicted reward: tensor([[-0.0108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9260]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21605.0
Loss: 0.04545411840081215
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-88.1710]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10560.0
Loss: 0.008201769553124905
Action 0 - predicted reward: tensor([[0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.9861]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12640.0
Loss: 0.012204569764435291
Greedy
Action 0 - predicted reward: tensor([[-0.7426]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1053]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4610.0
Loss: 0.003575593465939164
Action 0 - predicted reward: tensor([[-0.0323]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.7109]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6935.0
Loss: 0.0033900875132530928
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6136]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6807]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5413.7138671875
KL Divergence: 15.742181777954102
Action 0 - predicted reward: tensor([[2.6528]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7108]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7410.0
Loss: 5405.431640625
KL Divergence: 15.806804656982422
21199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0636]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9829]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20595.0
Loss: 0.010794734582304955
Action 0 - predicted reward: tensor([[0.0097]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9861]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21620.0
Loss: 0.0439378023147583
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0573]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9742]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10560.0
Loss: 0.008052468299865723
Action 0 - predicted reward: tensor([[-0.6608]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1909]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12715.0
Loss: 0.013980258256196976
Greedy
Action 0 - predicted reward: tensor([[-0.2738]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0069]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4610.0
Loss: 0.0033946079201996326
Action 0 - predicted reward: tensor([[1.4521]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0175]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6935.0
Loss: 0.003177944105118513
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6077]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4765]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5415.89501953125
KL Divergence: 15.739391326904297
Action 0 - predicted reward: tensor([[2.6297]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9004]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 5405.16796875
KL Divergence: 15.812816619873047
21299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0802]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0070]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20705.0
Loss: 0.010367557406425476
Action 0 - predicted reward: tensor([[0.0191]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.8350]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21730.0
Loss: 0.048731982707977295
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0516]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9723]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10595.0
Loss: 0.007954246364533901
Action 0 - predicted reward: tensor([[-0.3127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1523]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12715.0
Loss: 0.012124867178499699
Greedy
Action 0 - predicted reward: tensor([[-0.2045]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-77.8940]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4610.0
Loss: 1.4602262126572896e-05
Action 0 - predicted reward: tensor([[0.6662]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0296]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6940.0
Loss: 0.0031520244665443897
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5915]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0415]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5414.98486328125
KL Divergence: 15.737542152404785
Action 0 - predicted reward: tensor([[2.6251]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1088]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 5405.67822265625
KL Divergence: 15.803531646728516
21399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1623]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20920.0
Loss: 0.014916288666427135
Action 0 - predicted reward: tensor([[0.0360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.7317]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21775.0
Loss: 0.047216299921274185
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0141]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-69.6377]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10595.0
Loss: 0.007965357974171638
Action 0 - predicted reward: tensor([[-0.0442]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0404]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12785.0
Loss: 0.012648284435272217
Greedy
Action 0 - predicted reward: tensor([[-0.0011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.4581]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4610.0
Loss: 1.035416698869085e-05
Action 0 - predicted reward: tensor([[0.0192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-73.9988]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6955.0
Loss: 0.0031433033291250467
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5893]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0561]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5414.97314453125
KL Divergence: 15.730364799499512
Action 0 - predicted reward: tensor([[2.6446]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7072]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7415.0
Loss: 5405.75244140625
KL Divergence: 15.808592796325684
21499.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20935.0
Loss: 0.014356520026922226
Action 0 - predicted reward: tensor([[-0.0139]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0287]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21815.0
Loss: 0.04360877349972725
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.1208]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10595.0
Loss: 0.007882850244641304
Action 0 - predicted reward: tensor([[0.0078]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0213]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12820.0
Loss: 0.012086856178939342
Greedy
Action 0 - predicted reward: tensor([[-0.8426]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9406]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4610.0
Loss: 8.712785529496614e-06
Action 0 - predicted reward: tensor([[1.2277]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0204]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6955.0
Loss: 1.5531142707914114e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6033]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6688]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5413.66552734375
KL Divergence: 15.73271656036377
Action 0 - predicted reward: tensor([[2.6357]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6990]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7420.0
Loss: 5402.970703125
KL Divergence: 15.805388450622559
21599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1301]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9627]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20970.0
Loss: 0.014066219329833984
Action 0 - predicted reward: tensor([[0.0337]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.7065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21855.0
Loss: 0.048273783177137375
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0481]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0210]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10595.0
Loss: 0.007753405254334211
Action 0 - predicted reward: tensor([[-0.7156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0955]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12925.0
Loss: 0.016157984733581543
Greedy
Action 0 - predicted reward: tensor([[0.2045]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0129]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4610.0
Loss: 8.073428034549579e-06
Action 0 - predicted reward: tensor([[-0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-80.0358]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6960.0
Loss: 1.0975101758958772e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6229]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6770]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5412.5478515625
KL Divergence: 15.730279922485352
Action 0 - predicted reward: tensor([[2.6546]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7201]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7420.0
Loss: 5399.1455078125
KL Divergence: 15.81553840637207
21699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0826]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.9960]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21005.0
Loss: 0.014167183078825474
Action 0 - predicted reward: tensor([[-0.0429]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9775]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21980.0
Loss: 0.05254137143492699
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-86.3147]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10595.0
Loss: 0.0077593824826180935
Action 0 - predicted reward: tensor([[-0.9849]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9477]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12960.0
Loss: 0.0194140262901783
Greedy
Action 0 - predicted reward: tensor([[1.3298]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0210]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4610.0
Loss: 7.2508978519181255e-06
Action 0 - predicted reward: tensor([[1.9723]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9974]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6965.0
Loss: 9.493080142419785e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6281]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6778]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5412.5
KL Divergence: 15.73676872253418
Action 0 - predicted reward: tensor([[2.6659]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7240]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7420.0
Loss: 5399.61474609375
KL Divergence: 15.808867454528809
21799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0505]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0880]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21050.0
Loss: 0.014780594035983086
Action 0 - predicted reward: tensor([[0.1016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.6475]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21985.0
Loss: 0.0516170896589756
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.1547]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10665.0
Loss: 0.012994306161999702
Action 0 - predicted reward: tensor([[-0.0267]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.5542]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13000.0
Loss: 0.017721282318234444
Greedy
Action 0 - predicted reward: tensor([[0.0017]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-90.1460]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4610.0
Loss: 6.870011020509992e-06
Action 0 - predicted reward: tensor([[-0.0176]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.6130]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6970.0
Loss: 7.983207979123108e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6005]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9104]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5414.47607421875
KL Divergence: 15.724579811096191
Action 0 - predicted reward: tensor([[2.6465]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1587]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7420.0
Loss: 5395.8095703125
KL Divergence: 15.804572105407715
21899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1384]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0420]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21060.0
Loss: 0.011983557604253292
Action 0 - predicted reward: tensor([[0.0184]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0148]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21995.0
Loss: 0.051084984093904495
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.2278]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10700.0
Loss: 0.008061421103775501
Action 0 - predicted reward: tensor([[1.6387]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0083]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13035.0
Loss: 0.021622585132718086
Greedy
Action 0 - predicted reward: tensor([[-0.5123]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9949]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4645.0
Loss: 1.0528327948122751e-05
Action 0 - predicted reward: tensor([[-0.0066]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.5071]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 1.54482877405826e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6072]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6743]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5414.392578125
KL Divergence: 15.730165481567383
Action 0 - predicted reward: tensor([[2.6394]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0679]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5701.62890625
KL Divergence: 15.805583953857422
21999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2797]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0089]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21065.0
Loss: 0.011046105995774269
Action 0 - predicted reward: tensor([[-0.1904]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9580]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22035.0
Loss: 0.04441676288843155
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.2657]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10700.0
Loss: 0.0077268630266189575
Action 0 - predicted reward: tensor([[0.2739]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.7108]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13040.0
Loss: 0.021302398294210434
Greedy
Action 0 - predicted reward: tensor([[-1.1141]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0061]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4645.0
Loss: 7.524034572270466e-06
Action 0 - predicted reward: tensor([[-0.0035]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.9089]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 1.1641155651886947e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6066]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6775]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5412.81884765625
KL Divergence: 15.736473083496094
Action 0 - predicted reward: tensor([[2.6486]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7090]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7455.0
Loss: 5705.42236328125
KL Divergence: 15.815420150756836
22099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0384]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21080.0
Loss: 0.011408709920942783
Action 0 - predicted reward: tensor([[-0.0233]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9640]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22105.0
Loss: 0.04749107360839844
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0417]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10700.0
Loss: 0.007757598999887705
Action 0 - predicted reward: tensor([[0.3883]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0161]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13115.0
Loss: 0.031301528215408325
Greedy
Action 0 - predicted reward: tensor([[-0.0099]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.2794]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4645.0
Loss: 6.456505616370123e-06
Action 0 - predicted reward: tensor([[0.0022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9760]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 1.0244598342978861e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6195]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6693]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5414.2294921875
KL Divergence: 15.736263275146484
Action 0 - predicted reward: tensor([[2.6323]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1392]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5704.86767578125
KL Divergence: 15.796926498413086
22199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1703]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9116]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21195.0
Loss: 0.010747583582997322
Action 0 - predicted reward: tensor([[0.0015]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.3263]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22105.0
Loss: 0.043414756655693054
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.9442]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10700.0
Loss: 0.007746129762381315
Action 0 - predicted reward: tensor([[0.4694]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0178]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13120.0
Loss: 0.024299586191773415
Greedy
Action 0 - predicted reward: tensor([[-0.3357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9977]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4645.0
Loss: 6.02342743150075e-06
Action 0 - predicted reward: tensor([[0.0064]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7353]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6990.0
Loss: 8.824542419461068e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6067]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6687]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5418.4833984375
KL Divergence: 15.733848571777344
Action 0 - predicted reward: tensor([[2.6254]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0070]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5704.525390625
KL Divergence: 15.799031257629395
22299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1744]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.9490]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21300.0
Loss: 0.009767303243279457
Action 0 - predicted reward: tensor([[-0.0511]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9631]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22150.0
Loss: 0.043317023664712906
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0068]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0638]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10740.0
Loss: 0.007760434411466122
Action 0 - predicted reward: tensor([[0.0104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.6621]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13120.0
Loss: 0.024115359410643578
Greedy
Action 0 - predicted reward: tensor([[0.0154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.2524]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4645.0
Loss: 1.0686866517062299e-05
Action 0 - predicted reward: tensor([[-0.0008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.5000]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6990.0
Loss: 7.651568012079224e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6016]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6590]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5417.99853515625
KL Divergence: 15.720780372619629
Action 0 - predicted reward: tensor([[2.6277]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9980]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5704.01123046875
KL Divergence: 15.801470756530762
22399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0873]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0303]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21410.0
Loss: 0.007642594166100025
Action 0 - predicted reward: tensor([[-0.0319]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8898]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22225.0
Loss: 0.04417413845658302
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.2720]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10740.0
Loss: 0.007943176664412022
Action 0 - predicted reward: tensor([[0.5348]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0238]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13155.0
Loss: 0.027526216581463814
Greedy
Action 0 - predicted reward: tensor([[0.4009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0082]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4645.0
Loss: 6.5511330831213854e-06
Action 0 - predicted reward: tensor([[1.2832]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0178]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6990.0
Loss: 9.986634722736198e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6053]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6591]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5418.54052734375
KL Divergence: 15.720414161682129
Action 0 - predicted reward: tensor([[2.6507]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7052]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7455.0
Loss: 5706.26025390625
KL Divergence: 15.806929588317871
22499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0383]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9971]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21445.0
Loss: 0.009668361395597458
Action 0 - predicted reward: tensor([[0.0134]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0140]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22265.0
Loss: 0.04502393305301666
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0621]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0174]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10810.0
Loss: 0.01147038396447897
Action 0 - predicted reward: tensor([[-0.9755]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0989]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13155.0
Loss: 0.025690710172057152
Greedy
Action 0 - predicted reward: tensor([[0.1623]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9769]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4645.0
Loss: 4.970593181496952e-06
Action 0 - predicted reward: tensor([[0.9844]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9732]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6995.0
Loss: 8.40373741084477e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5822]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0345]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5417.10107421875
KL Divergence: 15.725282669067383
Action 0 - predicted reward: tensor([[2.6194]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9603]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5706.09326171875
KL Divergence: 15.806955337524414
22599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0752]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.0342]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21550.0
Loss: 0.016180431470274925
Action 0 - predicted reward: tensor([[-0.0206]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.6996]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22380.0
Loss: 0.044023189693689346
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.3691]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10885.0
Loss: 0.016343945637345314
Action 0 - predicted reward: tensor([[1.1295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1458]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13155.0
Loss: 0.025697674602270126
Greedy
Action 0 - predicted reward: tensor([[-0.7319]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0705]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4680.0
Loss: 0.0033508390188217163
Action 0 - predicted reward: tensor([[1.1967]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9946]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6995.0
Loss: 7.3966193667729385e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5800]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0518]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5417.810546875
KL Divergence: 15.73346996307373
Action 0 - predicted reward: tensor([[2.6192]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9637]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5706.31640625
KL Divergence: 15.80346393585205
22699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1597]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.6162]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21590.0
Loss: 0.015948252752423286
Action 0 - predicted reward: tensor([[-0.0080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9795]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22425.0
Loss: 0.04333704710006714
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9749]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10885.0
Loss: 0.012978789396584034
Action 0 - predicted reward: tensor([[0.2692]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0166]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13225.0
Loss: 0.024546397849917412
Greedy
Action 0 - predicted reward: tensor([[-0.5714]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2227]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4680.0
Loss: 0.0022678908426314592
Action 0 - predicted reward: tensor([[2.4809]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9973]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6995.0
Loss: 8.191649612854235e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5827]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1153]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5417.83740234375
KL Divergence: 15.73555850982666
Action 0 - predicted reward: tensor([[2.6265]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6810]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7455.0
Loss: 5708.44921875
KL Divergence: 15.802149772644043
22799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9763]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21670.0
Loss: 0.017565743997693062
Action 0 - predicted reward: tensor([[-0.0065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0000]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22465.0
Loss: 0.03657635301351547
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0085]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10920.0
Loss: 0.013816121965646744
Action 0 - predicted reward: tensor([[1.2224]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9768]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13260.0
Loss: 0.02044031210243702
Greedy
Action 0 - predicted reward: tensor([[-1.0492]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0555]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4715.0
Loss: 0.00020382413640618324
Action 0 - predicted reward: tensor([[1.6725]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9944]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7005.0
Loss: 7.491213636967586e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6139]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6654]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5413.87646484375
KL Divergence: 15.736790657043457
Action 0 - predicted reward: tensor([[2.6183]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9994]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5709.3212890625
KL Divergence: 15.809430122375488
22899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0876]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1098]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21775.0
Loss: 0.022450631484389305
Action 0 - predicted reward: tensor([[0.0174]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.2849]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22535.0
Loss: 0.034592289477586746
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.0516]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10920.0
Loss: 0.013550267554819584
Action 0 - predicted reward: tensor([[-0.5555]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0992]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13300.0
Loss: 0.016736725345253944
Greedy
Action 0 - predicted reward: tensor([[0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.9744]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4715.0
Loss: 4.013063880847767e-05
Action 0 - predicted reward: tensor([[0.9464]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9911]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7005.0
Loss: 6.695223419228569e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6296]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6820]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5408.16943359375
KL Divergence: 15.73785400390625
Action 0 - predicted reward: tensor([[2.6066]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1638]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5708.103515625
KL Divergence: 15.797303199768066
22999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1542]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8306]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 21810.0
Loss: 0.023450590670108795
Action 0 - predicted reward: tensor([[-0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.0148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22640.0
Loss: 0.031615253537893295
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0099]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.5805]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10925.0
Loss: 0.013363494537770748
Action 0 - predicted reward: tensor([[-0.0455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.3773]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13335.0
Loss: 0.01820891536772251
Greedy
Action 0 - predicted reward: tensor([[-0.6886]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0064]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4715.0
Loss: 2.7347097784513608e-05
Action 0 - predicted reward: tensor([[-0.0008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.6330]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7010.0
Loss: 6.445117378461873e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6334]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6997]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5406.2646484375
KL Divergence: 15.735506057739258
Action 0 - predicted reward: tensor([[2.5981]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6625]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7455.0
Loss: 5715.32177734375
KL Divergence: 15.797857284545898
23099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0404]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1013]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21850.0
Loss: 0.017393557354807854
Action 0 - predicted reward: tensor([[-0.0271]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2698]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 22785.0
Loss: 0.036569058895111084
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.2052]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10960.0
Loss: 0.010484598577022552
Action 0 - predicted reward: tensor([[0.9367]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0234]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13405.0
Loss: 0.02457764931023121
Greedy
Action 0 - predicted reward: tensor([[1.4590]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0247]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4715.0
Loss: 2.0802310245926492e-05
Action 0 - predicted reward: tensor([[-0.0142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.0220]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7015.0
Loss: 5.7570305216358975e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6631]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7071]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5404.25146484375
KL Divergence: 15.754912376403809
Action 0 - predicted reward: tensor([[2.5914]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0679]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5712.05615234375
KL Divergence: 15.795351028442383
23199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0691]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.2791]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21885.0
Loss: 0.01840636506676674
Action 0 - predicted reward: tensor([[0.0158]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4809]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22795.0
Loss: 0.027962330728769302
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0236]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9563]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10960.0
Loss: 0.009860116057097912
Action 0 - predicted reward: tensor([[-0.6689]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9988]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13440.0
Loss: 0.028219951316714287
Greedy
Action 0 - predicted reward: tensor([[-0.7140]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9162]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4715.0
Loss: 2.858901279978454e-05
Action 0 - predicted reward: tensor([[1.3665]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9932]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7020.0
Loss: 5.28257760379347e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6534]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7104]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5404.56103515625
KL Divergence: 15.741706848144531
Action 0 - predicted reward: tensor([[2.6094]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6748]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7455.0
Loss: 5711.91015625
KL Divergence: 15.801733016967773
23299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9038]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21890.0
Loss: 0.014711694791913033
Action 0 - predicted reward: tensor([[0.0526]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0567]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22945.0
Loss: 0.03445294126868248
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0712]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11030.0
Loss: 0.01799369417130947
Action 0 - predicted reward: tensor([[0.3226]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9729]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13475.0
Loss: 0.03224839270114899
Greedy
Action 0 - predicted reward: tensor([[-0.0102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.4668]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4715.0
Loss: 1.7609798305784352e-05
Action 0 - predicted reward: tensor([[-0.1461]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0282]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7020.0
Loss: 4.880471351498272e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6281]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2491]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5406.73046875
KL Divergence: 15.740083694458008
Action 0 - predicted reward: tensor([[2.6008]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0022]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5712.31787109375
KL Divergence: 15.796494483947754
23399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0289]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21930.0
Loss: 0.013715270906686783
Action 0 - predicted reward: tensor([[-0.0779]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.7021]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22960.0
Loss: 0.03289789706468582
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0238]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.7084]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11035.0
Loss: 0.013421907089650631
Action 0 - predicted reward: tensor([[-0.1373]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.5167]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13550.0
Loss: 0.03507116436958313
Greedy
Action 0 - predicted reward: tensor([[-0.4933]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9884]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4715.0
Loss: 1.3739572750637308e-05
Action 0 - predicted reward: tensor([[1.2390]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9967]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7020.0
Loss: 5.2567943384929094e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6463]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7003]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5407.55029296875
KL Divergence: 15.735746383666992
Action 0 - predicted reward: tensor([[2.6025]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6623]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7455.0
Loss: 5719.0146484375
KL Divergence: 15.78898811340332
23499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1939]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9023]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21965.0
Loss: 0.017344286665320396
Action 0 - predicted reward: tensor([[-0.0716]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9995]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23000.0
Loss: 0.03253904730081558
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-124.6786]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11105.0
Loss: 0.013191153295338154
Action 0 - predicted reward: tensor([[-0.4307]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0117]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13585.0
Loss: 0.0304795503616333
Greedy
Action 0 - predicted reward: tensor([[-0.0078]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.0016]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4715.0
Loss: 1.0909419870586134e-05
Action 0 - predicted reward: tensor([[-0.0035]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.2556]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7020.0
Loss: 5.136887921253219e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6199]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3413]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5406.2802734375
KL Divergence: 15.73642349243164
Action 0 - predicted reward: tensor([[2.5828]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8993]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5714.74072265625
KL Divergence: 15.791946411132812
23599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0238]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9921]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22150.0
Loss: 0.02189672365784645
Action 0 - predicted reward: tensor([[-0.0770]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0954]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23000.0
Loss: 0.02757604606449604
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0255]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9937]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11175.0
Loss: 0.013206466101109982
Action 0 - predicted reward: tensor([[0.1079]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9891]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13620.0
Loss: 0.030101405456662178
Greedy
Action 0 - predicted reward: tensor([[0.2533]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9908]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4715.0
Loss: 9.862416845862754e-06
Action 0 - predicted reward: tensor([[-0.0051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.7512]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7020.0
Loss: 7.257106517499778e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6419]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7021]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5402.7109375
KL Divergence: 15.747099876403809
Action 0 - predicted reward: tensor([[2.5880]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0312]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5715.2333984375
KL Divergence: 15.797988891601562
23699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0336]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22150.0
Loss: 0.0210752971470356
Action 0 - predicted reward: tensor([[-0.0782]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8291]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23140.0
Loss: 0.034511059522628784
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0535]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9892]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11180.0
Loss: 0.013098674826323986
Action 0 - predicted reward: tensor([[-0.2008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9063]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13620.0
Loss: 0.02711334452033043
Greedy
Action 0 - predicted reward: tensor([[-0.1739]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.2174]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4715.0
Loss: 1.083954794012243e-05
Action 0 - predicted reward: tensor([[0.0072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.5082]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7025.0
Loss: 4.447327683010371e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6574]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7211]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5398.24462890625
KL Divergence: 15.75463581085205
Action 0 - predicted reward: tensor([[2.6094]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6627]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7455.0
Loss: 5714.75048828125
KL Divergence: 15.800567626953125
23799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1628]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7846]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22300.0
Loss: 0.022698398679494858
Action 0 - predicted reward: tensor([[0.0166]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.7023]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23325.0
Loss: 0.03785385563969612
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.0684]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11180.0
Loss: 0.01304304227232933
Action 0 - predicted reward: tensor([[-0.7329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1397]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13620.0
Loss: 0.026314228773117065
Greedy
Action 0 - predicted reward: tensor([[-1.2453]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9569]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4750.0
Loss: 3.847013431368396e-05
Action 0 - predicted reward: tensor([[0.0018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.3074]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7025.0
Loss: 4.094180440006312e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6425]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1796]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5399.91552734375
KL Divergence: 15.746024131774902
Action 0 - predicted reward: tensor([[2.5881]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7895]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5716.8359375
KL Divergence: 15.79296875
23899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0141]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9542]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22425.0
Loss: 0.022512679919600487
Action 0 - predicted reward: tensor([[0.0667]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0377]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23400.0
Loss: 0.03306282311677933
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0166]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11180.0
Loss: 0.010075364261865616
Action 0 - predicted reward: tensor([[-0.5999]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9981]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13665.0
Loss: 0.02626108191907406
Greedy
Action 0 - predicted reward: tensor([[-1.7439]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0027]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4750.0
Loss: 1.196069297293434e-05
Action 0 - predicted reward: tensor([[0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.0794]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7030.0
Loss: 3.6431140415515983e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6476]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3760]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5397.20947265625
KL Divergence: 15.7449369430542
Action 0 - predicted reward: tensor([[2.6072]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6605]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7455.0
Loss: 5715.47265625
KL Divergence: 15.797765731811523
23999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0297]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.9788]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22465.0
Loss: 0.02063589356839657
Action 0 - predicted reward: tensor([[-0.1302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9627]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23520.0
Loss: 0.03414026275277138
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.6342]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11180.0
Loss: 0.009904108010232449
Action 0 - predicted reward: tensor([[-0.1655]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9223]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13665.0
Loss: 0.027338145300745964
Greedy
Action 0 - predicted reward: tensor([[-0.0042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.3848]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4750.0
Loss: 9.484878319199197e-06
Action 0 - predicted reward: tensor([[1.5711]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9998]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7030.0
Loss: 3.3507860734971473e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6710]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7277]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5400.04052734375
KL Divergence: 15.74564266204834
Action 0 - predicted reward: tensor([[2.5977]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6652]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7455.0
Loss: 5712.83984375
KL Divergence: 15.796207427978516
24099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1274]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.3796]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22640.0
Loss: 0.02529020980000496
Action 0 - predicted reward: tensor([[-0.2050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6481]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23565.0
Loss: 0.03436809778213501
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9917]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11215.0
Loss: 0.009845854714512825
Action 0 - predicted reward: tensor([[-0.0089]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.2169]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13665.0
Loss: 0.022914420813322067
Greedy
Action 0 - predicted reward: tensor([[0.1903]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.7694]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4750.0
Loss: 8.097827048914041e-06
Action 0 - predicted reward: tensor([[1.7188]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0069]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7030.0
Loss: 3.0838373277219944e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6330]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3163]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5404.55908203125
KL Divergence: 15.742134094238281
Action 0 - predicted reward: tensor([[2.5982]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0627]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 5712.32763671875
KL Divergence: 15.795820236206055
24199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0758]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1012]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22675.0
Loss: 0.024654220789670944
Action 0 - predicted reward: tensor([[-0.0110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9996]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23670.0
Loss: 0.03477154299616814
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.0117]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11215.0
Loss: 0.009775184094905853
Action 0 - predicted reward: tensor([[0.5530]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8137]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13745.0
Loss: 0.028774168342351913
Greedy
Action 0 - predicted reward: tensor([[0.6589]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9778]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4750.0
Loss: 7.317549261642853e-06
Action 0 - predicted reward: tensor([[0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.6058]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7030.0
Loss: 3.1166634926194092e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6579]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7102]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5401.25390625
KL Divergence: 15.740594863891602
Action 0 - predicted reward: tensor([[2.6060]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0426]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7460.0
Loss: 5710.453125
KL Divergence: 15.803688049316406
24299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0769]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8074]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22750.0
Loss: 0.024671321734786034
Action 0 - predicted reward: tensor([[-0.0295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.8936]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23745.0
Loss: 0.03433869406580925
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0343]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6664]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11215.0
Loss: 0.009785430505871773
Action 0 - predicted reward: tensor([[-0.3721]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8684]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13745.0
Loss: 0.026287920773029327
Greedy
Action 0 - predicted reward: tensor([[-0.0044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.7030]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4750.0
Loss: 6.7998430495208595e-06
Action 0 - predicted reward: tensor([[0.0030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.1159]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7030.0
Loss: 2.87635361928551e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6216]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1447]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5409.52197265625
KL Divergence: 15.748051643371582
Action 0 - predicted reward: tensor([[2.6209]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6823]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7465.0
Loss: 5710.87255859375
KL Divergence: 15.810517311096191
24399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.8008]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22865.0
Loss: 0.02782939374446869
Action 0 - predicted reward: tensor([[-0.0846]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.5143]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23750.0
Loss: 0.031346917152404785
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.5289]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11250.0
Loss: 0.006497349124401808
Action 0 - predicted reward: tensor([[-0.8629]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0326]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13745.0
Loss: 0.026110472157597542
Greedy
Action 0 - predicted reward: tensor([[1.5588]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0032]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4750.0
Loss: 6.427449079637881e-06
Action 0 - predicted reward: tensor([[0.0036]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.5607]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7030.0
Loss: 2.687549795155064e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6137]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2088]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5410.9892578125
KL Divergence: 15.742735862731934
Action 0 - predicted reward: tensor([[2.6054]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0013]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7465.0
Loss: 5709.28564453125
KL Divergence: 15.799131393432617
24499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0181]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8120]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22870.0
Loss: 0.027581028640270233
Action 0 - predicted reward: tensor([[-0.1429]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.8795]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23785.0
Loss: 0.031143728643655777
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0430]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.3575]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11250.0
Loss: 0.006332298740744591
Action 0 - predicted reward: tensor([[-0.1297]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.1831]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13745.0
Loss: 0.025907810777425766
Greedy
Action 0 - predicted reward: tensor([[-0.0096]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.2806]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4785.0
Loss: 0.0008659605518914759
Action 0 - predicted reward: tensor([[2.0370]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9943]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7030.0
Loss: 2.5799934064707486e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6340]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6904]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5411.28369140625
KL Divergence: 15.72956371307373
Action 0 - predicted reward: tensor([[2.5985]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0125]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7465.0
Loss: 5715.51708984375
KL Divergence: 15.78715705871582
24599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0582]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.5539]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22875.0
Loss: 0.02032071352005005
Action 0 - predicted reward: tensor([[-0.1871]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8790]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23905.0
Loss: 0.03532813489437103
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-154.4917]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11290.0
Loss: 0.003279938828200102
Action 0 - predicted reward: tensor([[-0.0395]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.5543]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13790.0
Loss: 0.028949344530701637
Greedy
Action 0 - predicted reward: tensor([[0.7083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9546]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4785.0
Loss: 2.625253182486631e-05
Action 0 - predicted reward: tensor([[1.7982]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9866]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7035.0
Loss: 2.567008323239861e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5995]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2625]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5415.560546875
KL Divergence: 15.737939834594727
Action 0 - predicted reward: tensor([[2.5871]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9920]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7465.0
Loss: 5716.4345703125
KL Divergence: 15.796104431152344
24699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0046]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.1523]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22880.0
Loss: 0.021155597642064095
Action 0 - predicted reward: tensor([[-0.0311]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.8868]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23940.0
Loss: 0.037033747881650925
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0832]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9905]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11290.0
Loss: 0.003145468421280384
Action 0 - predicted reward: tensor([[-0.0459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0108]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13860.0
Loss: 0.03187564015388489
Greedy
Action 0 - predicted reward: tensor([[-1.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0578]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4785.0
Loss: 1.214391795656411e-05
Action 0 - predicted reward: tensor([[-0.0048]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.8856]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 2.2645981516689062e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5942]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4785]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5413.38720703125
KL Divergence: 15.724971771240234
Action 0 - predicted reward: tensor([[2.5825]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0783]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7465.0
Loss: 5716.86328125
KL Divergence: 15.789814949035645
24799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0511]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0304]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22955.0
Loss: 0.020411396399140358
Action 0 - predicted reward: tensor([[0.0455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0800]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24010.0
Loss: 0.0398116298019886
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9722]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11290.0
Loss: 0.0031427813228219748
Action 0 - predicted reward: tensor([[-0.0857]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.1049]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13895.0
Loss: 0.020168457180261612
Greedy
Action 0 - predicted reward: tensor([[-0.0015]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-69.5707]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4785.0
Loss: 8.369663191842847e-06
Action 0 - predicted reward: tensor([[1.4481]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0163]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7040.0
Loss: 3.6228921089787036e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6215]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6724]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5413.66748046875
KL Divergence: 15.73024845123291
Action 0 - predicted reward: tensor([[2.5757]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8799]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7465.0
Loss: 5718.0556640625
KL Divergence: 15.794027328491211
24899.
Epsilon Greedy 5%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 23005.0
Loss: 0.02422860637307167
Action 0 - predicted reward: tensor([[-0.1351]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9710]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24020.0
Loss: 0.03711528331041336
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0278]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0155]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11290.0
Loss: 0.0031393321696668863
Action 0 - predicted reward: tensor([[-0.1762]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.2499]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14000.0
Loss: 0.020974047482013702
Greedy
Action 0 - predicted reward: tensor([[-0.1230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-74.3054]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4785.0
Loss: 6.986056632740656e-06
Action 0 - predicted reward: tensor([[0.0081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.5172]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 3.353119609528221e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6002]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2378]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5414.66455078125
KL Divergence: 15.734403610229492
Action 0 - predicted reward: tensor([[2.5782]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9173]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7465.0
Loss: 5716.72705078125
KL Divergence: 15.798445701599121
24999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0393]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.0707]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23050.0
Loss: 0.02450353465974331
Action 0 - predicted reward: tensor([[-0.1467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8934]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24055.0
Loss: 0.040739793330430984
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0426]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11290.0
Loss: 0.003137052757665515
Action 0 - predicted reward: tensor([[2.4484]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9844]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14045.0
Loss: 0.017250359058380127
Greedy
Action 0 - predicted reward: tensor([[-0.7980]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9661]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4785.0
Loss: 6.497455160570098e-06
Action 0 - predicted reward: tensor([[0.0011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.2201]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 3.2576851936028106e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5965]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1564]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5414.80810546875
KL Divergence: 15.732816696166992
Action 0 - predicted reward: tensor([[2.6045]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6592]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7465.0
Loss: 5715.25634765625
KL Divergence: 15.795000076293945
25099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.7058]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23205.0
Loss: 0.025963814929127693
Action 0 - predicted reward: tensor([[0.0945]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0433]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24200.0
Loss: 0.04501407966017723
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0015]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.3248]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11290.0
Loss: 0.0031575520988553762
Action 0 - predicted reward: tensor([[-0.1350]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.3405]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14080.0
Loss: 0.013522240333259106
Greedy
Action 0 - predicted reward: tensor([[0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.6327]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4785.0
Loss: 6.03523676545592e-06
Action 0 - predicted reward: tensor([[0.0005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.1899]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 3.0840353701933054e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5997]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2956]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5413.8251953125
KL Divergence: 15.728764533996582
Action 0 - predicted reward: tensor([[2.6018]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6675]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7465.0
Loss: 5715.7275390625
KL Divergence: 15.800100326538086
25199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0216]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.8439]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23315.0
Loss: 0.024856923148036003
Action 0 - predicted reward: tensor([[0.0672]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7919]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24240.0
Loss: 0.04422434791922569
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0594]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9861]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11290.0
Loss: 0.0031321614515036345
Action 0 - predicted reward: tensor([[1.0180]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9872]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14080.0
Loss: 0.013247516937553883
Greedy
Action 0 - predicted reward: tensor([[0.0098]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-86.5143]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4785.0
Loss: 5.585281087405747e-06
Action 0 - predicted reward: tensor([[-0.0008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.2222]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 2.8278254831093363e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6010]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1641]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5413.4853515625
KL Divergence: 15.730348587036133
Action 0 - predicted reward: tensor([[2.6123]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6672]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7465.0
Loss: 5713.59521484375
KL Divergence: 15.799160957336426
25299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0559]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9481]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23390.0
Loss: 0.020848875865340233
Action 0 - predicted reward: tensor([[-0.0257]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0493]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24350.0
Loss: 0.052584875375032425
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9859]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11290.0
Loss: 0.0031281302217394114
Action 0 - predicted reward: tensor([[0.6633]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9038]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14120.0
Loss: 0.013623163104057312
Greedy
Action 0 - predicted reward: tensor([[0.0041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9881]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4785.0
Loss: 5.341136784409173e-06
Action 0 - predicted reward: tensor([[0.0060]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.1120]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7045.0
Loss: 2.6918173716694582e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6274]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6807]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5411.25048828125
KL Divergence: 15.73060131072998
Action 0 - predicted reward: tensor([[2.5999]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6651]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7470.0
Loss: 5716.49560546875
KL Divergence: 15.79675006866455
25399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0506]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9314]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23425.0
Loss: 0.018545188009738922
Action 0 - predicted reward: tensor([[0.0072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.9391]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24420.0
Loss: 0.04692430794239044
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0563]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9734]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11290.0
Loss: 0.0031266091391444206
Action 0 - predicted reward: tensor([[0.0563]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0381]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14120.0
Loss: 0.012887783348560333
Greedy
Action 0 - predicted reward: tensor([[-0.3556]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9549]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4785.0
Loss: 5.027927727496717e-06
Action 0 - predicted reward: tensor([[2.1407]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0023]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7050.0
Loss: 2.7045616661780514e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6027]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1430]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5413.18115234375
KL Divergence: 15.735540390014648
Action 0 - predicted reward: tensor([[2.5760]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9594]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7470.0
Loss: 5719.67626953125
KL Divergence: 15.786909103393555
25499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0060]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.1228]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23500.0
Loss: 0.017038455232977867
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1162]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24460.0
Loss: 0.04979931563138962
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.6872]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11290.0
Loss: 0.003122922033071518
Action 0 - predicted reward: tensor([[2.1768]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9911]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14120.0
Loss: 0.00958334468305111
Greedy
Action 0 - predicted reward: tensor([[0.5350]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0090]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4785.0
Loss: 5.21218998983386e-06
Action 0 - predicted reward: tensor([[-0.0062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.2436]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7050.0
Loss: 2.446231974317925e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6012]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1622]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5412.6650390625
KL Divergence: 15.738941192626953
Action 0 - predicted reward: tensor([[2.5946]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6450]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7470.0
Loss: 5718.3154296875
KL Divergence: 15.783625602722168
25599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0132]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23505.0
Loss: 0.01685415394604206
Action 0 - predicted reward: tensor([[-0.0459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9291]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24465.0
Loss: 0.04578253999352455
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-6.4135e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.6165]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11325.0
Loss: 0.0032086819410324097
Action 0 - predicted reward: tensor([[0.1262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0129]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14190.0
Loss: 0.013775884173810482
Greedy
Action 0 - predicted reward: tensor([[0.0004]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.8701]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4820.0
Loss: 4.6843379095662385e-05
Action 0 - predicted reward: tensor([[-0.0126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.6575]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7055.0
Loss: 2.663183749973541e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6268]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6868]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5409.43603515625
KL Divergence: 15.734323501586914
Action 0 - predicted reward: tensor([[2.5717]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1153]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7470.0
Loss: 5714.296875
KL Divergence: 15.802937507629395
25699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3893]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9687]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23515.0
Loss: 0.016726747155189514
Action 0 - predicted reward: tensor([[-0.0430]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0429]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24540.0
Loss: 0.04707188531756401
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0559]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9417]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11325.0
Loss: 7.967631972860545e-06
Action 0 - predicted reward: tensor([[-0.3244]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0393]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14190.0
Loss: 0.012997937388718128
Greedy
Action 0 - predicted reward: tensor([[0.6369]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0112]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4820.0
Loss: 1.082061862689443e-05
Action 0 - predicted reward: tensor([[-0.0011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.7396]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 2.5071781237784307e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6089]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0313]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5411.23779296875
KL Divergence: 15.742120742797852
Action 0 - predicted reward: tensor([[2.5646]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9477]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7470.0
Loss: 5721.16162109375
KL Divergence: 15.789426803588867
25799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0898]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9708]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23675.0
Loss: 0.021403634920716286
Action 0 - predicted reward: tensor([[0.0164]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.3495]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24580.0
Loss: 0.04339037835597992
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.1381]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11360.0
Loss: 7.376275334536331e-06
Action 0 - predicted reward: tensor([[0.0407]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.3327]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14225.0
Loss: 0.014629377983510494
Greedy
Action 0 - predicted reward: tensor([[-0.0123]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-81.1921]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4820.0
Loss: 7.72102521295892e-06
Action 0 - predicted reward: tensor([[-0.0040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.5148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7065.0
Loss: 2.5926499347406207e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6291]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6881]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5410.873046875
KL Divergence: 15.734216690063477
Action 0 - predicted reward: tensor([[2.5649]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9622]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7475.0
Loss: 5719.42919921875
KL Divergence: 15.789051055908203
25899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.9656]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 23720.0
Loss: 0.020547140389680862
Action 0 - predicted reward: tensor([[-0.1924]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8866]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24615.0
Loss: 0.04411981999874115
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0048]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.9892]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11360.0
Loss: 5.304907062964048e-06
Action 0 - predicted reward: tensor([[0.0109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6670]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14230.0
Loss: 0.01302885077893734
Greedy
Action 0 - predicted reward: tensor([[0.1554]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9550]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4820.0
Loss: 6.512620984722162e-06
Action 0 - predicted reward: tensor([[-0.0019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.0874]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7070.0
Loss: 2.5821652798185823e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6402]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6855]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5408.4755859375
KL Divergence: 15.737500190734863
Action 0 - predicted reward: tensor([[2.5623]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0475]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7475.0
Loss: 5721.47412109375
KL Divergence: 15.79172134399414
25999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0004]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1655]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 23840.0
Loss: 0.025561831891536713
Action 0 - predicted reward: tensor([[-0.0673]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0349]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24615.0
Loss: 0.03985203430056572
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0054]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.8566]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11395.0
Loss: 1.143494046118576e-05
Action 0 - predicted reward: tensor([[-0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7933]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14335.0
Loss: 0.013462928123772144
Greedy
Action 0 - predicted reward: tensor([[0.0548]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9853]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4820.0
Loss: 5.5817813517933246e-06
Action 0 - predicted reward: tensor([[0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.1220]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7070.0
Loss: 2.4085911718429998e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6376]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6968]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5407.32080078125
KL Divergence: 15.735213279724121
Action 0 - predicted reward: tensor([[2.5852]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6387]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5720.76318359375
KL Divergence: 15.778249740600586
26099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.8820]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24050.0
Loss: 0.02950075827538967
Action 0 - predicted reward: tensor([[0.1170]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0207]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24655.0
Loss: 0.03930157795548439
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.8117]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11395.0
Loss: 6.7036871769232675e-06
Action 0 - predicted reward: tensor([[-0.0010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.7869]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14370.0
Loss: 0.015390658751130104
Greedy
Action 0 - predicted reward: tensor([[0.0039]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.9100]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4820.0
Loss: 4.91176251671277e-06
Action 0 - predicted reward: tensor([[1.1540]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0148]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7070.0
Loss: 2.369784624534077e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6433]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6950]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5408.5048828125
KL Divergence: 15.738856315612793
Action 0 - predicted reward: tensor([[2.5860]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6365]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5721.857421875
KL Divergence: 15.784296989440918
26199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0957]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2134]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24155.0
Loss: 0.03167492523789406
Action 0 - predicted reward: tensor([[-0.0321]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.5836]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24695.0
Loss: 0.03945618122816086
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0043]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-106.2803]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11395.0
Loss: 5.560993031394901e-06
Action 0 - predicted reward: tensor([[-1.6391]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9527]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14370.0
Loss: 0.010041323490440845
Greedy
Action 0 - predicted reward: tensor([[0.0098]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-100.2230]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4820.0
Loss: 4.352254109107889e-06
Action 0 - predicted reward: tensor([[1.1966]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0025]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7070.0
Loss: 2.304604095115792e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6207]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1419]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5410.9892578125
KL Divergence: 15.736144065856934
Action 0 - predicted reward: tensor([[2.5584]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3321]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7510.0
Loss: 5719.4443359375
KL Divergence: 15.78302001953125
26299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.3591]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24230.0
Loss: 0.027808403596282005
Action 0 - predicted reward: tensor([[-0.0110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.5527]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24735.0
Loss: 0.03902216628193855
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0756]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0309]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11430.0
Loss: 0.0002503023424651474
Action 0 - predicted reward: tensor([[0.0302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14370.0
Loss: 0.009862042963504791
Greedy
Action 0 - predicted reward: tensor([[-0.5634]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9563]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4855.0
Loss: 0.004616562742739916
Action 0 - predicted reward: tensor([[0.0042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.0839]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7070.0
Loss: 2.2157882995088585e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6204]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6879]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 5406.21484375
KL Divergence: 15.729637145996094
Action 0 - predicted reward: tensor([[2.5886]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6404]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5718.341796875
KL Divergence: 15.792938232421875
26399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2570]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9935]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24235.0
Loss: 0.023681702092289925
Action 0 - predicted reward: tensor([[-0.0379]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0531]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24755.0
Loss: 0.03875218704342842
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0017]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.9904]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11465.0
Loss: 9.225123358191922e-05
Action 0 - predicted reward: tensor([[-1.1284]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0016]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14370.0
Loss: 0.01006033830344677
Greedy
Action 0 - predicted reward: tensor([[0.0108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.6904]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4855.0
Loss: 3.447948984103277e-05
Action 0 - predicted reward: tensor([[0.0009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.8892]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7075.0
Loss: 2.1991772882756777e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6224]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1379]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 5405.77490234375
KL Divergence: 15.735078811645508
Action 0 - predicted reward: tensor([[2.5860]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6463]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5719.64306640625
KL Divergence: 15.790143013000488
26499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0708]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24320.0
Loss: 0.02368968166410923
Action 0 - predicted reward: tensor([[0.1874]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0096]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24765.0
Loss: 0.03926244005560875
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0202]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9968]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11470.0
Loss: 5.8487562455411535e-06
Action 0 - predicted reward: tensor([[0.2536]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0157]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14370.0
Loss: 0.009704322554171085
Greedy
Action 0 - predicted reward: tensor([[-0.0026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.0920]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4890.0
Loss: 1.564588637847919e-05
Action 0 - predicted reward: tensor([[-0.0015]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.2913]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7080.0
Loss: 1.942102244356647e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6358]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6967]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5704.5107421875
KL Divergence: 15.737645149230957
Action 0 - predicted reward: tensor([[2.5698]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9306]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7510.0
Loss: 5718.40478515625
KL Divergence: 15.795214653015137
26599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0169]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.5833]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24325.0
Loss: 0.02327735535800457
Action 0 - predicted reward: tensor([[-0.1266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8746]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24810.0
Loss: 0.03858862817287445
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0512]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9494]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11575.0
Loss: 0.005492842756211758
Action 0 - predicted reward: tensor([[-0.0342]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.7227]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14370.0
Loss: 0.009739981032907963
Greedy
Action 0 - predicted reward: tensor([[0.0428]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.9905]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4890.0
Loss: 9.02903138921829e-06
Action 0 - predicted reward: tensor([[1.3599]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0016]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 1.9770893686654745e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6251]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6952]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5708.37939453125
KL Divergence: 15.739097595214844
Action 0 - predicted reward: tensor([[2.5708]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6296]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5720.3662109375
KL Divergence: 15.78942584991455
26699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1702]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0075]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24365.0
Loss: 0.022829586640000343
Action 0 - predicted reward: tensor([[0.0261]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.9956]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24885.0
Loss: 0.040995042771101
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-93.0882]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11575.0
Loss: 0.004487372934818268
Action 0 - predicted reward: tensor([[0.6023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0380]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14370.0
Loss: 0.007797339931130409
Greedy
Action 0 - predicted reward: tensor([[-1.8111]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0231]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4890.0
Loss: 8.490197615174111e-06
Action 0 - predicted reward: tensor([[0.7127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9889]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 2.09595964406617e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6153]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3148]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5708.82958984375
KL Divergence: 15.72874641418457
Action 0 - predicted reward: tensor([[2.5674]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9258]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7510.0
Loss: 5717.83203125
KL Divergence: 15.798942565917969
26799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0116]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.8288]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24445.0
Loss: 0.019962772727012634
Action 0 - predicted reward: tensor([[-0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.1668]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24995.0
Loss: 0.028058281168341637
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9842]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11620.0
Loss: 0.0043822345323860645
Action 0 - predicted reward: tensor([[-0.0074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.8187]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14370.0
Loss: 0.0034749782644212246
Greedy
Action 0 - predicted reward: tensor([[-0.2252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4890.0
Loss: 7.209747764136409e-06
Action 0 - predicted reward: tensor([[0.6360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9757]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 1.9366971173440106e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6340]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6883]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5708.8291015625
KL Divergence: 15.736246109008789
Action 0 - predicted reward: tensor([[2.5884]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6475]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5718.904296875
KL Divergence: 15.782746315002441
26899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0333]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.3960]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24560.0
Loss: 0.0221742931753397
Action 0 - predicted reward: tensor([[0.0906]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.4863]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25110.0
Loss: 0.027259444817900658
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.1123]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11620.0
Loss: 0.004340853076428175
Action 0 - predicted reward: tensor([[2.2060]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9639]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14370.0
Loss: 0.003395367180928588
Greedy
Action 0 - predicted reward: tensor([[0.0055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0093]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4890.0
Loss: 5.568566848523915e-06
Action 0 - predicted reward: tensor([[0.9254]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0128]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 1.8702963870964595e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6367]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6885]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5710.072265625
KL Divergence: 15.738762855529785
Action 0 - predicted reward: tensor([[2.5869]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6411]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5718.68603515625
KL Divergence: 15.785835266113281
26999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-76.9670]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24740.0
Loss: 0.028729120269417763
Action 0 - predicted reward: tensor([[0.0271]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0472]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25190.0
Loss: 0.020725062116980553
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0857]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0084]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11620.0
Loss: 0.004404050298035145
Action 0 - predicted reward: tensor([[0.0097]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.8221]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14445.0
Loss: 0.0036429285537451506
Greedy
Action 0 - predicted reward: tensor([[-1.2034]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0096]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4890.0
Loss: 5.039299594500335e-06
Action 0 - predicted reward: tensor([[1.4048]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9899]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 1.758364533088752e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6097]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6702]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5715.5390625
KL Divergence: 15.731505393981934
Action 0 - predicted reward: tensor([[2.5695]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0769]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7510.0
Loss: 5718.787109375
KL Divergence: 15.780617713928223
27099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0480]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.4794]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24785.0
Loss: 0.02925892174243927
Action 0 - predicted reward: tensor([[0.0598]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.7508]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25260.0
Loss: 0.02637859806418419
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.0864]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11620.0
Loss: 0.004298807121813297
Action 0 - predicted reward: tensor([[0.1194]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9305]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14480.0
Loss: 0.0034059835597872734
Greedy
Action 0 - predicted reward: tensor([[-0.0166]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.5176]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4890.0
Loss: 5.251081347523723e-06
Action 0 - predicted reward: tensor([[0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.5386]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7090.0
Loss: 1.7290441292061587e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5838]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2023]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5716.63232421875
KL Divergence: 15.721858978271484
Action 0 - predicted reward: tensor([[2.5871]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6487]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5717.60302734375
KL Divergence: 15.78220272064209
27199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0145]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9392]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24785.0
Loss: 0.02759985812008381
Action 0 - predicted reward: tensor([[0.0220]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.0337]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25330.0
Loss: 0.02090422250330448
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0519]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11620.0
Loss: 0.003203308442607522
Action 0 - predicted reward: tensor([[-0.0148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.5479]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14480.0
Loss: 0.0001209915426443331
Greedy
Action 0 - predicted reward: tensor([[-0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-73.2786]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4925.0
Loss: 7.188175459305057e-06
Action 0 - predicted reward: tensor([[1.3671]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9950]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7095.0
Loss: 1.670511892370996e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5770]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2904]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5719.31396484375
KL Divergence: 15.725431442260742
Action 0 - predicted reward: tensor([[2.6007]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6499]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5717.8544921875
KL Divergence: 15.787341117858887
27299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1236]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0110]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 24825.0
Loss: 0.02702447585761547
Action 0 - predicted reward: tensor([[-0.0525]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6373]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25400.0
Loss: 0.0208208616822958
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-6.2943e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.0611]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11620.0
Loss: 0.003213637974113226
Action 0 - predicted reward: tensor([[-0.0046]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.6490]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14480.0
Loss: 2.653817500686273e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-75.8698]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4925.0
Loss: 5.374289230530849e-06
Action 0 - predicted reward: tensor([[0.9543]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0039]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7095.0
Loss: 1.6580302144575398e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5816]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6389]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5719.2568359375
KL Divergence: 15.723886489868164
Action 0 - predicted reward: tensor([[2.5921]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6534]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7510.0
Loss: 5716.5439453125
KL Divergence: 15.78588581085205
27399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0348]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.7116]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 24870.0
Loss: 0.026677705347537994
Action 0 - predicted reward: tensor([[0.3205]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3361]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25545.0
Loss: 0.029106808826327324
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0057]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.9654]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11655.0
Loss: 0.0032339401077479124
Action 0 - predicted reward: tensor([[-0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.6812]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14480.0
Loss: 1.8285552869201638e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.5802]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4960.0
Loss: 0.003548938548192382
Action 0 - predicted reward: tensor([[-1.0729e-06]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.2628]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7095.0
Loss: 1.5758820381961414e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5826]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6421]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5720.349609375
KL Divergence: 15.710254669189453
Action 0 - predicted reward: tensor([[2.5781]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6479]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7515.0
Loss: 5717.89306640625
KL Divergence: 15.791003227233887
27499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4178]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9324]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25050.0
Loss: 0.031651001423597336
Action 0 - predicted reward: tensor([[-0.0855]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0878]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25590.0
Loss: 0.028120538219809532
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.0999]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11690.0
Loss: 0.003341466886922717
Action 0 - predicted reward: tensor([[0.0033]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.5700]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14515.0
Loss: 4.726288534584455e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0210]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.1852]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4960.0
Loss: 0.0033504723105579615
Action 0 - predicted reward: tensor([[0.0089]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.8135]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7095.0
Loss: 1.5458821280844859e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5834]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6393]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5719.29931640625
KL Divergence: 15.720494270324707
Action 0 - predicted reward: tensor([[2.5802]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9859]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7515.0
Loss: 5715.52001953125
KL Divergence: 15.793171882629395
27599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1502]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.7823]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25160.0
Loss: 0.03348034247756004
Action 0 - predicted reward: tensor([[-0.0365]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0031]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25625.0
Loss: 0.028710167855024338
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-119.8154]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11690.0
Loss: 0.003192054806277156
Action 0 - predicted reward: tensor([[0.0073]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.5951]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14515.0
Loss: 1.7673979527899064e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0033]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.6316]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4960.0
Loss: 0.003345950972288847
Action 0 - predicted reward: tensor([[-0.1917]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9937]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7100.0
Loss: 1.5243316511259764e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5602]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2948]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5721.39794921875
KL Divergence: 15.719078063964844
Action 0 - predicted reward: tensor([[2.6059]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6593]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7515.0
Loss: 5717.541015625
KL Divergence: 15.792654037475586
27699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.1229]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25275.0
Loss: 0.03411192074418068
Action 0 - predicted reward: tensor([[-0.0312]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0005]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25695.0
Loss: 0.03126228600740433
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.9903]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11690.0
Loss: 0.0031800305005162954
Action 0 - predicted reward: tensor([[0.4947]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0034]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14515.0
Loss: 1.3887783097743522e-05
Greedy
Action 0 - predicted reward: tensor([[0.7642]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9702]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4995.0
Loss: 0.006387891713529825
Action 0 - predicted reward: tensor([[0.0047]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.9956]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7100.0
Loss: 1.4663920637758565e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5552]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1992]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5722.26953125
KL Divergence: 15.71608829498291
Action 0 - predicted reward: tensor([[2.5931]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6459]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7515.0
Loss: 5720.2060546875
KL Divergence: 15.786028861999512
27799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9850]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25350.0
Loss: 0.02682945318520069
Action 0 - predicted reward: tensor([[0.0959]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0099]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25835.0
Loss: 0.03787701576948166
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0407]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0080]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11690.0
Loss: 0.003173308912664652
Action 0 - predicted reward: tensor([[0.0068]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.8902]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14550.0
Loss: 1.345614691672381e-05
Greedy
Action 0 - predicted reward: tensor([[0.1413]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-89.6264]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4995.0
Loss: 0.0036138915456831455
Action 0 - predicted reward: tensor([[0.0120]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.6077]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7100.0
Loss: 1.4529400687024463e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5768]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6302]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5720.27978515625
KL Divergence: 15.72781753540039
Action 0 - predicted reward: tensor([[2.5690]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0772]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7515.0
Loss: 5719.91064453125
KL Divergence: 15.790021896362305
27899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.6577]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7050]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25500.0
Loss: 0.0318261981010437
Action 0 - predicted reward: tensor([[-0.0475]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0285]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25875.0
Loss: 0.034152328968048096
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.1951]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11690.0
Loss: 0.0031567243859171867
Action 0 - predicted reward: tensor([[0.7275]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0093]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14555.0
Loss: 1.1340166565787513e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0135]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-83.2058]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4995.0
Loss: 0.0033806301653385162
Action 0 - predicted reward: tensor([[0.0152]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.7154]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7100.0
Loss: 1.4041596614333685e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5871]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6277]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5721.37744140625
KL Divergence: 15.718648910522461
Action 0 - predicted reward: tensor([[2.5656]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6331]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7520.0
Loss: 5718.69384765625
KL Divergence: 15.797417640686035
27999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0153]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.1461]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25570.0
Loss: 0.035244982689619064
Action 0 - predicted reward: tensor([[0.1603]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25910.0
Loss: 0.0339663065969944
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0015]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.8552]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11695.0
Loss: 0.0035080446396023035
Action 0 - predicted reward: tensor([[-0.8751]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0172]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14560.0
Loss: 1.1841262676171027e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0188]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.8014]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4995.0
Loss: 0.003275955095887184
Action 0 - predicted reward: tensor([[-7.9632e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.7606]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7105.0
Loss: 1.9038984646613244e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5848]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6295]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5720.24365234375
KL Divergence: 15.723641395568848
Action 0 - predicted reward: tensor([[2.5865]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6451]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7520.0
Loss: 5717.154296875
KL Divergence: 15.784958839416504
28099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0576]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5540]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 25750.0
Loss: 0.039975933730602264
Action 0 - predicted reward: tensor([[-0.1050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.3847]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26070.0
Loss: 0.03776080533862114
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11695.0
Loss: 0.0031467967201024294
Action 0 - predicted reward: tensor([[0.4550]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9834]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14560.0
Loss: 1.011757103697164e-05
Greedy
Action 0 - predicted reward: tensor([[0.0060]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.0646]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5030.0
Loss: 0.004016745835542679
Action 0 - predicted reward: tensor([[3.3617e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.4488]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7105.0
Loss: 1.7802074125938816e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5614]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9836]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5720.1943359375
KL Divergence: 15.728490829467773
Action 0 - predicted reward: tensor([[2.5900]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6468]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7520.0
Loss: 5718.9091796875
KL Divergence: 15.788261413574219
28199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0409]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9305]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25790.0
Loss: 0.03724575787782669
Action 0 - predicted reward: tensor([[-0.0455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2049]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26110.0
Loss: 0.03701983764767647
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1707]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9995]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11695.0
Loss: 0.0031472849659621716
Action 0 - predicted reward: tensor([[-0.5752]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0047]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14595.0
Loss: 0.00013598993245977908
Greedy
Action 0 - predicted reward: tensor([[-0.0074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3744]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5030.0
Loss: 0.0033480115234851837
Action 0 - predicted reward: tensor([[2.3544]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9895]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7115.0
Loss: 1.7075689129342209e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5675]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9421]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5717.1494140625
KL Divergence: 15.731379508972168
Action 0 - predicted reward: tensor([[2.5661]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0799]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7520.0
Loss: 5720.72509765625
KL Divergence: 15.78120231628418
28299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2901]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9855]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25865.0
Loss: 0.03855445235967636
Action 0 - predicted reward: tensor([[-0.5225]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7679]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26145.0
Loss: 0.04132598266005516
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0043]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.4126]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11695.0
Loss: 0.003138499101623893
Action 0 - predicted reward: tensor([[-0.0490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.8010]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14735.0
Loss: 0.0007234924705699086
Greedy
Action 0 - predicted reward: tensor([[0.3497]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0061]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5030.0
Loss: 0.0032367852982133627
Action 0 - predicted reward: tensor([[-7.3791e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.2462]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7125.0
Loss: 1.629171492822934e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5801]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3168]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5716.08447265625
KL Divergence: 15.723715782165527
Action 0 - predicted reward: tensor([[2.5735]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6323]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7520.0
Loss: 5721.42822265625
KL Divergence: 15.794139862060547
28399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1423]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9884]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 25880.0
Loss: 0.03108450211584568
Action 0 - predicted reward: tensor([[-0.0263]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9446]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26225.0
Loss: 0.03998739644885063
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0122]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9785]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11695.0
Loss: 0.003132512792944908
Action 0 - predicted reward: tensor([[0.0211]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.3350]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 14780.0
Loss: 0.005720346700400114
Greedy
Action 0 - predicted reward: tensor([[-0.0080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.7653]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5030.0
Loss: 0.0032525050919502974
Action 0 - predicted reward: tensor([[1.6970]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7125.0
Loss: 1.590890633451636e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5846]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9953]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5713.8818359375
KL Divergence: 15.72641372680664
Action 0 - predicted reward: tensor([[2.5517]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9585]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7520.0
Loss: 5722.23779296875
KL Divergence: 15.773051261901855
28499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0058]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.4076]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 25915.0
Loss: 0.02668151631951332
Action 0 - predicted reward: tensor([[-0.0365]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1563]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26230.0
Loss: 0.039680179208517075
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.0599]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11730.0
Loss: 0.003193733049556613
Action 0 - predicted reward: tensor([[-0.2180]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0191]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14820.0
Loss: 0.00010783343896036968
Greedy
Action 0 - predicted reward: tensor([[-0.3410]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0135]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5030.0
Loss: 0.003212007926777005
Action 0 - predicted reward: tensor([[2.1139]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0001]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7125.0
Loss: 1.5273836879714509e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5927]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6529]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5716.6201171875
KL Divergence: 15.730785369873047
Action 0 - predicted reward: tensor([[2.5694]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6211]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7520.0
Loss: 5724.01806640625
KL Divergence: 15.777134895324707
28599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0255]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9716]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26090.0
Loss: 0.04006357491016388
Action 0 - predicted reward: tensor([[0.0122]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.0741]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26270.0
Loss: 0.03957101330161095
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0210]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2203]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11730.0
Loss: 0.003170605981722474
Action 0 - predicted reward: tensor([[8.7202e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.8521]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14825.0
Loss: 0.00011773470760090277
Greedy
Action 0 - predicted reward: tensor([[0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-73.0677]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5030.0
Loss: 0.0032337948214262724
Action 0 - predicted reward: tensor([[-0.0030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0395]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7125.0
Loss: 1.4812773088124231e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5762]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3163]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5717.88916015625
KL Divergence: 15.72195053100586
Action 0 - predicted reward: tensor([[2.5665]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6200]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7525.0
Loss: 5720.5478515625
KL Divergence: 15.789868354797363
28699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0551]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.8769]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26135.0
Loss: 0.037959035485982895
Action 0 - predicted reward: tensor([[-0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.2885]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26355.0
Loss: 0.04362954944372177
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2780]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11770.0
Loss: 0.003241318976506591
Action 0 - predicted reward: tensor([[0.0863]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9058]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14825.0
Loss: 1.6986312402877957e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0238]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.8885]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 0.0032547409646213055
Action 0 - predicted reward: tensor([[0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.2606]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7125.0
Loss: 2.2948261175770313e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5994]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6587]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5714.3154296875
KL Divergence: 15.727192878723145
Action 0 - predicted reward: tensor([[2.5774]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6289]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7525.0
Loss: 5721.8837890625
KL Divergence: 15.782251358032227
28799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1613]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.2589]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26285.0
Loss: 0.038764916360378265
Action 0 - predicted reward: tensor([[0.0373]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9845]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26395.0
Loss: 0.03963514417409897
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9935]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11875.0
Loss: 0.004093508701771498
Action 0 - predicted reward: tensor([[0.0150]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-91.1359]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14825.0
Loss: 1.2722835890599526e-05
Greedy
Action 0 - predicted reward: tensor([[0.0046]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.9440]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 0.0032157832756638527
Action 0 - predicted reward: tensor([[-0.0036]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.1275]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7130.0
Loss: 1.6927489241425064e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6156]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6665]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5714.3125
KL Divergence: 15.730681419372559
Action 0 - predicted reward: tensor([[2.5837]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6392]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7525.0
Loss: 5719.93798828125
KL Divergence: 15.784587860107422
28899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.6563]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26360.0
Loss: 0.03652931749820709
Action 0 - predicted reward: tensor([[-0.0246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.8900]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26395.0
Loss: 0.03666730225086212
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0047]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.9984]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11875.0
Loss: 0.0032306371722370386
Action 0 - predicted reward: tensor([[-1.3696]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0642]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14860.0
Loss: 0.0004163752601016313
Greedy
Action 0 - predicted reward: tensor([[1.8825]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9506]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
Loss: 0.0031995321623981
Action 0 - predicted reward: tensor([[0.8934]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0054]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7140.0
Loss: 1.7120021311711753e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5996]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6666]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5712.6552734375
KL Divergence: 15.724441528320312
Action 0 - predicted reward: tensor([[2.5928]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6523]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7525.0
Loss: 5716.30908203125
KL Divergence: 15.793499946594238
28999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0705]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.9228]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26435.0
Loss: 0.03818705677986145
Action 0 - predicted reward: tensor([[-0.0063]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.7245]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26455.0
Loss: 0.03593100979924202
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.0245]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11950.0
Loss: 5.6718592531979084e-05
Action 0 - predicted reward: tensor([[0.1487]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0191]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14895.0
Loss: 2.1364814529079013e-05
Greedy
Action 0 - predicted reward: tensor([[-0.8981]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9923]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
Loss: 0.0031872547697275877
Action 0 - predicted reward: tensor([[-0.0041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2349]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7140.0
Loss: 1.5752484614495188e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6203]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6755]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5712.01416015625
KL Divergence: 15.726907730102539
Action 0 - predicted reward: tensor([[2.5694]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0115]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7525.0
Loss: 5718.73486328125
KL Divergence: 15.793259620666504
29099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.7810]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26510.0
Loss: 0.03744471073150635
Action 0 - predicted reward: tensor([[-0.0119]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.8417]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26525.0
Loss: 0.038204293698072433
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0417]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9881]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11950.0
Loss: 1.1270717550360132e-05
Action 0 - predicted reward: tensor([[-0.0210]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.6774]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14895.0
Loss: 1.2038418390147854e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.8413]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 0.0031840000301599503
Action 0 - predicted reward: tensor([[-0.0042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.1147]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7140.0
Loss: 1.4573382713933825e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5954]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0757]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5712.720703125
KL Divergence: 15.73074722290039
Action 0 - predicted reward: tensor([[2.5613]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8787]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7530.0
Loss: 5721.86572265625
KL Divergence: 15.787453651428223
29199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9501]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26545.0
Loss: 0.03641507029533386
Action 0 - predicted reward: tensor([[-0.0469]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.3301]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26595.0
Loss: 0.036994077265262604
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0269]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9946]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11950.0
Loss: 8.18187982076779e-06
Action 0 - predicted reward: tensor([[-0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4947]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14895.0
Loss: 1.0126113920705393e-05
Greedy
Action 0 - predicted reward: tensor([[1.2780]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0638]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
Loss: 0.0031872130930423737
Action 0 - predicted reward: tensor([[1.6737]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9994]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7140.0
Loss: 1.4269463690652628e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6185]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6685]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5712.26025390625
KL Divergence: 15.721275329589844
Action 0 - predicted reward: tensor([[2.5529]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9826]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7530.0
Loss: 5722.630859375
KL Divergence: 15.780462265014648
29299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0047]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26650.0
Loss: 0.044587913900613785
Action 0 - predicted reward: tensor([[0.0386]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.4171]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26700.0
Loss: 0.03840312361717224
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0507]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0149]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11985.0
Loss: 0.00018500683654565364
Action 0 - predicted reward: tensor([[0.0137]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.0088]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14930.0
Loss: 1.2212882211315446e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0226]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.7984]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 0.003155058715492487
Action 0 - predicted reward: tensor([[0.1721]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9717]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7145.0
Loss: 1.2776431503880303e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5976]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6615]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5713.63720703125
KL Divergence: 15.719292640686035
Action 0 - predicted reward: tensor([[2.5549]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9951]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7530.0
Loss: 5721.025390625
KL Divergence: 15.781307220458984
29399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1554]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.7290]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26690.0
Loss: 0.0446120947599411
Action 0 - predicted reward: tensor([[-0.0142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.9037]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26740.0
Loss: 0.040083304047584534
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9930]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11985.0
Loss: 0.00010575514897936955
Action 0 - predicted reward: tensor([[-0.3186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9684]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14965.0
Loss: 2.7021234927815385e-05
Greedy
Action 0 - predicted reward: tensor([[0.0132]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.3494]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 0.0031519653275609016
Action 0 - predicted reward: tensor([[-0.0016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.9740]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7150.0
Loss: 1.2163135352238896e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5938]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2413]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5714.88916015625
KL Divergence: 15.733116149902344
Action 0 - predicted reward: tensor([[2.5576]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1315]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7530.0
Loss: 5719.5556640625
KL Divergence: 15.78840446472168
29499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0486]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9936]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26730.0
Loss: 0.04496515914797783
Action 0 - predicted reward: tensor([[-0.1624]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.7505]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26815.0
Loss: 0.044336412101984024
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.0491]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11985.0
Loss: 0.0001130394302890636
Action 0 - predicted reward: tensor([[-0.9408]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0076]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14965.0
Loss: 1.1061441000492778e-05
Greedy
Action 0 - predicted reward: tensor([[-0.0131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.0753]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 0.0032327461522072554
Action 0 - predicted reward: tensor([[0.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.0055]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7150.0
Loss: 1.2400853393046418e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6167]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6673]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5713.49462890625
KL Divergence: 15.726881980895996
Action 0 - predicted reward: tensor([[2.5905]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6459]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7530.0
Loss: 5717.69873046875
KL Divergence: 15.784017562866211
29599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0396]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.8604]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26780.0
Loss: 0.04375116154551506
Action 0 - predicted reward: tensor([[-0.0152]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0469]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26885.0
Loss: 0.043151699006557465
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0577]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9542]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12055.0
Loss: 0.00375166954472661
Action 0 - predicted reward: tensor([[-0.1050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9923]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14965.0
Loss: 8.585599971411284e-06
Greedy
Action 0 - predicted reward: tensor([[-0.0049]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.4061]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 0.003174121491611004
Action 0 - predicted reward: tensor([[0.6882]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9992]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7150.0
Loss: 1.3921965091867605e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5967]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0817]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5710.6748046875
KL Divergence: 15.729433059692383
Action 0 - predicted reward: tensor([[2.5655]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9547]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7530.0
Loss: 5721.10986328125
KL Divergence: 15.780060768127441
29699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.9187]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 26855.0
Loss: 0.04389047995209694
Action 0 - predicted reward: tensor([[-1.4591]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3353]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 26925.0
Loss: 0.049028586596250534
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0097]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.2760]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12095.0
Loss: 0.00015664270904380828
Action 0 - predicted reward: tensor([[-0.4741]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9906]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14965.0
Loss: 7.4604163273761515e-06
Greedy
Action 0 - predicted reward: tensor([[-0.1637]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9392]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
Loss: 0.0031516405288130045
Action 0 - predicted reward: tensor([[-0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.8339]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7150.0
Loss: 1.207956643156649e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6232]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6799]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5707.4462890625
KL Divergence: 15.729801177978516
Action 0 - predicted reward: tensor([[2.5568]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8424]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7530.0
Loss: 5720.59326171875
KL Divergence: 15.78791618347168
29799.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 26935.0
Loss: 0.046621255576610565
Action 0 - predicted reward: tensor([[0.0133]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0484]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27035.0
Loss: 0.04634421318769455
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0858]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0028]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12130.0
Loss: 0.00016225545550696552
Action 0 - predicted reward: tensor([[0.1977]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1349]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14970.0
Loss: 6.691223461530171e-06
Greedy
Action 0 - predicted reward: tensor([[0.6191]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0023]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
Loss: 6.689712336083176e-06
Action 0 - predicted reward: tensor([[-7.4267e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.7820]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7150.0
Loss: 1.1438290812293417e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5903]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3121]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5715.07080078125
KL Divergence: 15.7255220413208
Action 0 - predicted reward: tensor([[2.5519]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8949]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7535.0
Loss: 5722.7509765625
KL Divergence: 15.789205551147461
29899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.3314]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27010.0
Loss: 0.05229242891073227
Action 0 - predicted reward: tensor([[0.0638]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.7302]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27120.0
Loss: 0.04612430930137634
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0060]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.2288]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12130.0
Loss: 4.8300178605131805e-05
Action 0 - predicted reward: tensor([[-0.8932]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9995]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14970.0
Loss: 7.0336573116946965e-06
Greedy
Action 0 - predicted reward: tensor([[0.0095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.1169]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 5.746915121562779e-06
Action 0 - predicted reward: tensor([[0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.1213]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7155.0
Loss: 1.2617366564882104e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6034]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6645]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5715.4267578125
KL Divergence: 15.7257080078125
Action 0 - predicted reward: tensor([[2.5724]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6201]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7535.0
Loss: 5722.83984375
KL Divergence: 15.778940200805664
29999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4185]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1063]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27070.0
Loss: 0.05061936005949974
Action 0 - predicted reward: tensor([[0.0668]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0263]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27230.0
Loss: 0.039322275668382645
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0152]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.5422]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12130.0
Loss: 1.6866260921233334e-05
Action 0 - predicted reward: tensor([[-0.4235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9925]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14970.0
Loss: 6.014134214638034e-06
Greedy
Action 0 - predicted reward: tensor([[-0.1911]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0103]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
Loss: 4.888207513431553e-06
Action 0 - predicted reward: tensor([[1.3920]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9957]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7155.0
Loss: 1.3988324099045713e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5851]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0404]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5714.41845703125
KL Divergence: 15.721450805664062
Action 0 - predicted reward: tensor([[2.5657]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6237]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7535.0
Loss: 5722.3310546875
KL Divergence: 15.791461944580078
30099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1147]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0287]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27225.0
Loss: 0.04500395432114601
Action 0 - predicted reward: tensor([[0.1761]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6284]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27235.0
Loss: 0.039004821330308914
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.5662]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12130.0
Loss: 1.135397997131804e-05
Action 0 - predicted reward: tensor([[0.0680]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1226]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15040.0
Loss: 0.0055022877641022205
Greedy
Action 0 - predicted reward: tensor([[0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.2909]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 4.957662895321846e-06
Action 0 - predicted reward: tensor([[0.0038]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.2181]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 1.2636164683499373e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6050]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6576]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5715.42041015625
KL Divergence: 15.731231689453125
Action 0 - predicted reward: tensor([[2.5507]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7980]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7535.0
Loss: 5422.6171875
KL Divergence: 15.787556648254395
30199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9106]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27300.0
Loss: 0.043604228645563126
Action 0 - predicted reward: tensor([[-0.0435]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0087]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27305.0
Loss: 0.03929073363542557
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0959]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12235.0
Loss: 0.005656527820974588
Action 0 - predicted reward: tensor([[-0.0100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-73.8204]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15085.0
Loss: 0.0033740950748324394
Greedy
Action 0 - predicted reward: tensor([[-0.1728]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0047]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
Loss: 1.4075600120122544e-05
Action 0 - predicted reward: tensor([[-0.0005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.8660]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 1.1740187346731545e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6035]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6652]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5712.6455078125
KL Divergence: 15.733559608459473
Action 0 - predicted reward: tensor([[2.5526]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9770]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7535.0
Loss: 5421.0048828125
KL Divergence: 15.788599967956543
30299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1970]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8997]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27440.0
Loss: 0.047994643449783325
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 27445.0
Loss: 0.040825456380844116
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0094]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.2640]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12305.0
Loss: 0.004749513231217861
Action 0 - predicted reward: tensor([[-0.0103]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.6109]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15120.0
Loss: 0.003291392233222723
Greedy
Action 0 - predicted reward: tensor([[-1.7610]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9852]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
Loss: 6.23974028712837e-06
Action 0 - predicted reward: tensor([[0.0005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-66.5332]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 1.135823481490661e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5902]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1961]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5708.59619140625
KL Divergence: 15.721952438354492
Action 0 - predicted reward: tensor([[2.5707]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6335]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7535.0
Loss: 5421.587890625
KL Divergence: 15.778477668762207
30399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0365]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.7191]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27475.0
Loss: 0.04852883517742157
Action 0 - predicted reward: tensor([[-0.0165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.3193]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27600.0
Loss: 0.052474841475486755
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0707]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1261]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12340.0
Loss: 0.001461727311834693
Action 0 - predicted reward: tensor([[-0.2815]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0127]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15125.0
Loss: 0.0032856566831469536
Greedy
Action 0 - predicted reward: tensor([[-0.0265]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.5727]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 4.386423825053498e-06
Action 0 - predicted reward: tensor([[0.0001]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.4343]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 1.1089088047810947e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5884]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0729]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5709.9072265625
KL Divergence: 15.731791496276855
Action 0 - predicted reward: tensor([[2.5736]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6298]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 5423.28466796875
KL Divergence: 15.783757209777832
30499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9722]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27515.0
Loss: 0.04671049863100052
Action 0 - predicted reward: tensor([[0.0207]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0001]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27670.0
Loss: 0.04481551796197891
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.7598]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12375.0
Loss: 0.0006533336127176881
Action 0 - predicted reward: tensor([[1.4696]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9654]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15125.0
Loss: 0.003226111875846982
Greedy
Action 0 - predicted reward: tensor([[-0.4512]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9873]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5065.0
Loss: 3.6665260267909616e-06
Action 0 - predicted reward: tensor([[0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.5159]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 1.0758503776742145e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5923]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1403]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5707.89501953125
KL Divergence: 15.730779647827148
Action 0 - predicted reward: tensor([[2.5767]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6280]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 5424.583984375
KL Divergence: 15.775155067443848
30599.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27595.0
Loss: 0.04868346452713013
Action 0 - predicted reward: tensor([[-0.1018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.1743]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27820.0
Loss: 0.036296650767326355
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.6710]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 12480.0
Loss: 0.006446551531553268
Action 0 - predicted reward: tensor([[-0.6946]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0625]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15160.0
Loss: 0.0032218657433986664
Greedy
Action 0 - predicted reward: tensor([[0.0051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.9362]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5135.0
Loss: 0.0037709781900048256
Action 0 - predicted reward: tensor([[0.0069]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.4112]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7165.0
Loss: 1.203399165206065e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5963]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1123]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5409.57568359375
KL Divergence: 15.728858947753906
Action 0 - predicted reward: tensor([[2.5668]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6306]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 5425.455078125
KL Divergence: 15.783676147460938
30699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0311]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0911]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27665.0
Loss: 0.050730619579553604
Action 0 - predicted reward: tensor([[0.0186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.7947]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 27870.0
Loss: 0.040657706558704376
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0823]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12480.0
Loss: 5.1723607612075284e-05
Action 0 - predicted reward: tensor([[-0.9140]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9365]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15165.0
Loss: 0.0032131962943822145
Greedy
Action 0 - predicted reward: tensor([[-0.0141]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.2165]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5135.0
Loss: 0.003745491150766611
Action 0 - predicted reward: tensor([[2.2540]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0031]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7170.0
Loss: 1.12110660666076e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6005]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2358]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5409.08984375
KL Divergence: 15.728230476379395
Action 0 - predicted reward: tensor([[2.5738]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6265]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 5425.37060546875
KL Divergence: 15.782389640808105
30799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0303]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0382]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27735.0
Loss: 0.0489981435239315
Action 0 - predicted reward: tensor([[-0.0379]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.6786]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27985.0
Loss: 0.03794776275753975
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0568]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9548]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12515.0
Loss: 0.0032953135669231415
Action 0 - predicted reward: tensor([[-1.5016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9944]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15165.0
Loss: 0.003064736956730485
Greedy
Action 0 - predicted reward: tensor([[-0.0185]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-75.5044]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5170.0
Loss: 0.003618342336267233
Action 0 - predicted reward: tensor([[1.3286]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0006]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7170.0
Loss: 1.1811869171651779e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6302]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6812]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5410.13330078125
KL Divergence: 15.738748550415039
Action 0 - predicted reward: tensor([[2.5476]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2633]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7540.0
Loss: 5427.09423828125
KL Divergence: 15.776137351989746
30899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0727]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.5169]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27735.0
Loss: 0.047003164887428284
Action 0 - predicted reward: tensor([[-0.0646]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9666]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28095.0
Loss: 0.044028591364622116
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0316]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.8396]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12620.0
Loss: 0.0025470806285738945
Action 0 - predicted reward: tensor([[-0.0217]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.9912]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15235.0
Loss: 0.0032094696070998907
Greedy
Action 0 - predicted reward: tensor([[-1.0132]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9784]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5170.0
Loss: 0.0035228095948696136
Action 0 - predicted reward: tensor([[0.0102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.8678]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7175.0
Loss: 1.104772536564269e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6164]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6868]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5409.04931640625
KL Divergence: 15.732954025268555
Action 0 - predicted reward: tensor([[2.5447]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9930]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7540.0
Loss: 5426.5654296875
KL Divergence: 15.781886100769043
30999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8295]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27845.0
Loss: 0.05411872640252113
Action 0 - predicted reward: tensor([[-0.0223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0121]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28205.0
Loss: 0.04473073408007622
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3840]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9852]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12795.0
Loss: 0.004051561933010817
Action 0 - predicted reward: tensor([[-2.3339]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9939]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15235.0
Loss: 0.003181189065799117
Greedy
Action 0 - predicted reward: tensor([[-0.1537]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0791]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5170.0
Loss: 0.003439989872276783
Action 0 - predicted reward: tensor([[-0.0200]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.6657]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7205.0
Loss: 1.1527999959071167e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6220]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6616]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5407.88525390625
KL Divergence: 15.739089965820312
Action 0 - predicted reward: tensor([[2.5463]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6119]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 5427.6748046875
KL Divergence: 15.79105281829834
31099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.0512]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 27895.0
Loss: 0.042215846478939056
Action 0 - predicted reward: tensor([[-0.0073]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8684]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28250.0
Loss: 0.0404868945479393
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0162]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.4015]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12835.0
Loss: 0.008193975314497948
Action 0 - predicted reward: tensor([[-1.5499]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9949]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15235.0
Loss: 0.003192436881363392
Greedy
Action 0 - predicted reward: tensor([[1.3513]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9593]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5170.0
Loss: 0.0034369686618447304
Action 0 - predicted reward: tensor([[0.0779]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8053]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7215.0
Loss: 1.3175122148822993e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6074]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8846]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5405.80615234375
KL Divergence: 15.73985767364502
Action 0 - predicted reward: tensor([[2.5629]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6134]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 5427.43701171875
KL Divergence: 15.783713340759277
31199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0037]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 27965.0
Loss: 0.04246639460325241
Action 0 - predicted reward: tensor([[-0.0234]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.8555]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 28255.0
Loss: 0.038423385471105576
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0815]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0408]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12835.0
Loss: 0.00761817954480648
Action 0 - predicted reward: tensor([[-0.0041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.7230]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15235.0
Loss: 0.003176686353981495
Greedy
Action 0 - predicted reward: tensor([[0.7994]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9993]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5170.0
Loss: 0.0033989602234214544
Action 0 - predicted reward: tensor([[1.8644]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0149]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7220.0
Loss: 1.3825663245370379e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6135]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2464]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5404.13916015625
KL Divergence: 15.73561954498291
Action 0 - predicted reward: tensor([[2.5584]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6135]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 5427.9443359375
KL Divergence: 15.775245666503906
31299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.5688]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 28120.0
Loss: 0.047436296939849854
Action 0 - predicted reward: tensor([[0.0803]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9047]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28330.0
Loss: 0.037353627383708954
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0228]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9999]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12905.0
Loss: 0.007523314096033573
Action 0 - predicted reward: tensor([[0.0057]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0003]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15240.0
Loss: 0.003170562442392111
Greedy
Action 0 - predicted reward: tensor([[0.0057]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-92.2100]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5170.0
Loss: 0.003367082215845585
Action 0 - predicted reward: tensor([[2.4798]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9877]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7220.0
Loss: 1.4968740060794516e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6143]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5582]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5401.64111328125
KL Divergence: 15.732795715332031
Action 0 - predicted reward: tensor([[2.5346]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0015]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7540.0
Loss: 5428.2978515625
KL Divergence: 15.777533531188965
31399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0396]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0326]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28130.0
Loss: 0.04309142753481865
Action 0 - predicted reward: tensor([[0.0690]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.0552]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 28440.0
Loss: 0.037518210709095
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0854]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0432]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12915.0
Loss: 0.007152297534048557
Action 0 - predicted reward: tensor([[-0.0022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.0985]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15240.0
Loss: 0.003185954177752137
Greedy
Action 0 - predicted reward: tensor([[-0.9455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9394]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.00334723899140954
Action 0 - predicted reward: tensor([[-0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.3699]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7220.0
Loss: 1.4201251588019659e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6181]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2004]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5401.53466796875
KL Divergence: 15.738752365112305
Action 0 - predicted reward: tensor([[2.5430]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5958]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 5428.50830078125
KL Divergence: 15.777515411376953
31499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0845]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0245]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28170.0
Loss: 0.03514191880822182
Action 0 - predicted reward: tensor([[-0.0398]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0036]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28515.0
Loss: 0.038176216185092926
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0128]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.9753]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12915.0
Loss: 0.006843527778983116
Action 0 - predicted reward: tensor([[-0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.7316]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15240.0
Loss: 0.00317397341132164
Greedy
Action 0 - predicted reward: tensor([[-0.2719]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0104]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.003313964232802391
Action 0 - predicted reward: tensor([[-0.0019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.6198]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7220.0
Loss: 1.376146087750385e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6227]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2320]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5399.513671875
KL Divergence: 15.729195594787598
Action 0 - predicted reward: tensor([[2.5331]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8619]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7545.0
Loss: 5428.59619140625
KL Divergence: 15.776531219482422
31599.
Epsilon Greedy 5%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28205.0
Loss: 0.03575825318694115
Action 0 - predicted reward: tensor([[-0.0113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.8236]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 28555.0
Loss: 0.042908210307359695
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2285]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1867]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12915.0
Loss: 0.0069421399384737015
Action 0 - predicted reward: tensor([[-0.1913]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9951]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15240.0
Loss: 0.0031663160771131516
Greedy
Action 0 - predicted reward: tensor([[-1.4253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0146]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.003289435524493456
Action 0 - predicted reward: tensor([[0.0055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6835]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7220.0
Loss: 1.2653912335736095e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6530]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7061]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5398.654296875
KL Divergence: 15.734285354614258
Action 0 - predicted reward: tensor([[2.5555]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6031]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7545.0
Loss: 5429.37255859375
KL Divergence: 15.774370193481445
31699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3462]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2623]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28310.0
Loss: 0.030272724106907845
Action 0 - predicted reward: tensor([[0.0098]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8779]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28635.0
Loss: 0.03697057440876961
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0484]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12915.0
Loss: 0.006888978648930788
Action 0 - predicted reward: tensor([[0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-85.0293]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15280.0
Loss: 0.0032694644760340452
Greedy
Action 0 - predicted reward: tensor([[0.0135]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7706]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.003272451926022768
Action 0 - predicted reward: tensor([[2.4763]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0100]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7225.0
Loss: 1.298151801165659e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6300]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0182]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5398.40673828125
KL Divergence: 15.740939140319824
Action 0 - predicted reward: tensor([[2.5464]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5977]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7545.0
Loss: 5428.55029296875
KL Divergence: 15.772294044494629
31799.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 28415.0
Loss: 0.028475245460867882
Action 0 - predicted reward: tensor([[-0.0310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.0741]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 28685.0
Loss: 0.03790944442152977
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0119]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.5266]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12915.0
Loss: 0.006871209945529699
Action 0 - predicted reward: tensor([[-0.0609]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9869]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15315.0
Loss: 0.0032150791957974434
Greedy
Action 0 - predicted reward: tensor([[-0.1210]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9942]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.003267101477831602
Action 0 - predicted reward: tensor([[1.3033]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9947]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7225.0
Loss: 1.2594459803949576e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6589]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7138]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5400.54345703125
KL Divergence: 15.740596771240234
Action 0 - predicted reward: tensor([[2.5244]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0696]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7550.0
Loss: 5428.84521484375
KL Divergence: 15.771207809448242
31899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9339]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28490.0
Loss: 0.028000574558973312
Action 0 - predicted reward: tensor([[0.0190]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9688]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28755.0
Loss: 0.03619850054383278
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0257]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9962]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12915.0
Loss: 0.0068617938086390495
Action 0 - predicted reward: tensor([[-0.1159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0497]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15350.0
Loss: 0.0031883527990430593
Greedy
Action 0 - predicted reward: tensor([[-0.0145]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.7635]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.003248890396207571
Action 0 - predicted reward: tensor([[2.0294]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9941]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7225.0
Loss: 1.2154113164797309e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6603]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7129]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5400.83984375
KL Divergence: 15.737913131713867
Action 0 - predicted reward: tensor([[2.5215]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8783]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7550.0
Loss: 5428.19384765625
KL Divergence: 15.77103042602539
31999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1794]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2723]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28780.0
Loss: 0.03457638993859291
Action 0 - predicted reward: tensor([[0.1023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.7123]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 28845.0
Loss: 0.03693995624780655
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-105.4835]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12990.0
Loss: 0.010815896093845367
Action 0 - predicted reward: tensor([[-0.9182]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9995]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15385.0
Loss: 0.005239242222160101
Greedy
Action 0 - predicted reward: tensor([[0.0212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.0276]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.0032375010196119547
Action 0 - predicted reward: tensor([[1.7868]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9995]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7235.0
Loss: 1.1470407343949773e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6353]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1851]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5399.88525390625
KL Divergence: 15.739096641540527
Action 0 - predicted reward: tensor([[2.5439]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5925]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7550.0
Loss: 5429.74658203125
KL Divergence: 15.77546215057373
32099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3865]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.2991]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 28820.0
Loss: 0.0325317345559597
Action 0 - predicted reward: tensor([[0.0763]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.9169]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 28960.0
Loss: 0.03489632159471512
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0179]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12990.0
Loss: 0.010453657247126102
Action 0 - predicted reward: tensor([[-0.0557]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.7780]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15385.0
Loss: 0.003175443271175027
Greedy
Action 0 - predicted reward: tensor([[-0.4486]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.0032263377215713263
Action 0 - predicted reward: tensor([[-0.0010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.3623]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7235.0
Loss: 1.099860128306318e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6359]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3075]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5406.1640625
KL Divergence: 15.742533683776855
Action 0 - predicted reward: tensor([[2.5354]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5879]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7550.0
Loss: 5429.103515625
KL Divergence: 15.776410102844238
32199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2115]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9521]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28860.0
Loss: 0.03314989432692528
Action 0 - predicted reward: tensor([[-0.0266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.2230]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29065.0
Loss: 0.0338849239051342
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0560]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0289]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12990.0
Loss: 0.010290726087987423
Action 0 - predicted reward: tensor([[0.0135]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.8286]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15425.0
Loss: 0.0031706560403108597
Greedy
Action 0 - predicted reward: tensor([[-0.0148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.2746]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.0032152573112398386
Action 0 - predicted reward: tensor([[0.3151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9863]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7235.0
Loss: 1.0476194347575074e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6339]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0845]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5406.099609375
KL Divergence: 15.737386703491211
Action 0 - predicted reward: tensor([[2.5314]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5894]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7550.0
Loss: 5430.47802734375
KL Divergence: 15.776070594787598
32299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8077]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 28940.0
Loss: 0.02508024126291275
Action 0 - predicted reward: tensor([[0.0119]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.7254]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29105.0
Loss: 0.030652496963739395
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.5801]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13060.0
Loss: 0.01029014028608799
Action 0 - predicted reward: tensor([[-0.8668]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9889]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15495.0
Loss: 0.003617500187829137
Greedy
Action 0 - predicted reward: tensor([[0.2413]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0289]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.003201889805495739
Action 0 - predicted reward: tensor([[0.0016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.4426]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7235.0
Loss: 1.0836072306119604e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6551]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7130]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5407.5615234375
KL Divergence: 15.746976852416992
Action 0 - predicted reward: tensor([[2.5211]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5789]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7550.0
Loss: 5432.41064453125
KL Divergence: 15.775568962097168
32399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0337]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7858]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 29015.0
Loss: 0.024412745609879494
Action 0 - predicted reward: tensor([[-0.0275]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0302]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 29150.0
Loss: 0.026373134925961494
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0358]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0402]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13095.0
Loss: 0.01061966922134161
Action 0 - predicted reward: tensor([[-0.0176]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-110.9118]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15495.0
Loss: 0.0031837511342018843
Greedy
Action 0 - predicted reward: tensor([[-0.0107]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.1566]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.0031799464486539364
Action 0 - predicted reward: tensor([[0.0009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.1507]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7235.0
Loss: 1.0539849881752161e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6287]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1350]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5409.76806640625
KL Divergence: 15.747274398803711
Action 0 - predicted reward: tensor([[2.5067]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9335]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7550.0
Loss: 5430.62255859375
KL Divergence: 15.769745826721191
32499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1626]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9700]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 29120.0
Loss: 0.029727745801210403
Action 0 - predicted reward: tensor([[0.0075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0314]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 29195.0
Loss: 0.026375088840723038
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0248]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9747]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13130.0
Loss: 0.0109242619946599
Action 0 - predicted reward: tensor([[0.3885]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0041]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15505.0
Loss: 0.0031489382963627577
Greedy
Action 0 - predicted reward: tensor([[0.2932]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9729]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.0031745971646159887
Action 0 - predicted reward: tensor([[1.8337]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9960]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7245.0
Loss: 1.0405146895209327e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6570]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7103]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5410.78369140625
KL Divergence: 15.734875679016113
Action 0 - predicted reward: tensor([[2.5033]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7646]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7550.0
Loss: 5429.64013671875
KL Divergence: 15.7595796585083
32599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2776]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.4230]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29160.0
Loss: 0.0245946254581213
Action 0 - predicted reward: tensor([[-0.0420]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.8070]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29230.0
Loss: 0.026152947917580605
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0385]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0032]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13135.0
Loss: 0.010242129676043987
Action 0 - predicted reward: tensor([[-0.7357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0293]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15540.0
Loss: 0.0030656568706035614
Greedy
Action 0 - predicted reward: tensor([[-0.0162]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.4764]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.003167577553540468
Action 0 - predicted reward: tensor([[0.7600]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0234]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7250.0
Loss: 9.89644945548207e-07
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6537]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7060]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5407.49169921875
KL Divergence: 15.741933822631836
Action 0 - predicted reward: tensor([[2.5012]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8742]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7550.0
Loss: 5429.67529296875
KL Divergence: 15.769356727600098
32699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0928]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8877]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29230.0
Loss: 0.02469286508858204
Action 0 - predicted reward: tensor([[0.0918]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.9157]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29235.0
Loss: 0.026162341237068176
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9724]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13210.0
Loss: 0.01440183911472559
Action 0 - predicted reward: tensor([[0.6401]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9774]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15540.0
Loss: 0.0031429859809577465
Greedy
Action 0 - predicted reward: tensor([[0.0029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.1705]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.0031572377774864435
Action 0 - predicted reward: tensor([[1.2050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9958]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7250.0
Loss: 9.826394489209633e-07
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6221]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2070]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5408.5302734375
KL Divergence: 15.738537788391113
Action 0 - predicted reward: tensor([[2.5012]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9335]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7550.0
Loss: 5430.3076171875
KL Divergence: 15.769332885742188
32799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0120]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.8817]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29355.0
Loss: 0.024226244539022446
Action 0 - predicted reward: tensor([[-0.0377]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8604]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 29305.0
Loss: 0.029662100598216057
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0038]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.4327]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13210.0
Loss: 0.013513014651834965
Action 0 - predicted reward: tensor([[0.0119]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.4408]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15540.0
Loss: 0.0031386439222842455
Greedy
Action 0 - predicted reward: tensor([[-0.2817]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9698]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.00315613835118711
Action 0 - predicted reward: tensor([[0.0033]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9396]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7255.0
Loss: 9.152188908956305e-07
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6335]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6981]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6655.0
Loss: 5412.875
KL Divergence: 15.736698150634766
Action 0 - predicted reward: tensor([[2.4990]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8419]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7550.0
Loss: 5429.7236328125
KL Divergence: 15.767507553100586
32899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0103]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.7588]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29360.0
Loss: 0.024062586948275566
Action 0 - predicted reward: tensor([[-0.0197]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.8932]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29380.0
Loss: 0.02636251039803028
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.5820]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13250.0
Loss: 0.013738644309341908
Action 0 - predicted reward: tensor([[-0.0301]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9939]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15545.0
Loss: 0.0031369358766824007
Greedy
Action 0 - predicted reward: tensor([[-0.0041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.5657]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.0031514575239270926
Action 0 - predicted reward: tensor([[0.8754]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0048]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7255.0
Loss: 9.041936550602259e-07
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6231]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3272]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5413.185546875
KL Divergence: 15.738282203674316
Action 0 - predicted reward: tensor([[2.5144]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5712]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7555.0
Loss: 5430.62548828125
KL Divergence: 15.77456283569336
32999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0078]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9032]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 29430.0
Loss: 0.023586012423038483
Action 0 - predicted reward: tensor([[-0.0384]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0340]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 29385.0
Loss: 0.02623745985329151
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0116]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0100]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13250.0
Loss: 0.013556147925555706
Action 0 - predicted reward: tensor([[0.0038]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.1279]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15545.0
Loss: 0.0031400169245898724
Greedy
Action 0 - predicted reward: tensor([[0.8704]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9581]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.0031513783615082502
Action 0 - predicted reward: tensor([[0.0029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-90.5321]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7255.0
Loss: 9.47189221278677e-07
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6180]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1695]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5415.11962890625
KL Divergence: 15.73422908782959
Action 0 - predicted reward: tensor([[2.4960]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0220]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7555.0
Loss: 5432.0380859375
KL Divergence: 15.771747589111328
33099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.0659]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 29475.0
Loss: 0.02388634905219078
Action 0 - predicted reward: tensor([[0.0692]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9919]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 29455.0
Loss: 0.02067411132156849
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.7883]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13250.0
Loss: 0.013569328002631664
Action 0 - predicted reward: tensor([[-0.0129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.5831]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15545.0
Loss: 0.0031474605202674866
Greedy
Action 0 - predicted reward: tensor([[0.5928]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0050]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5175.0
Loss: 0.003142580157145858
Action 0 - predicted reward: tensor([[1.1809]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0071]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7255.0
Loss: 8.920914638110844e-07
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.6130]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3167]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 5414.908203125
KL Divergence: 15.729175567626953
Action 0 - predicted reward: tensor([[2.5137]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5642]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7555.0
Loss: 5431.1787109375
KL Divergence: 15.770930290222168
