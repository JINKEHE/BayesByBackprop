Use GPU: False
1.0.1.post2
99.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1897]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2172]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 200.0
Loss: 1.431538701057434
Action 0 - predicted reward: tensor([[-1.2313]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.5962]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 375.0
Loss: 0.6958301663398743
Action 0 - predicted reward: tensor([[-0.4234]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4344]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 315.0
Loss: 0.6599401235580444
Action 0 - predicted reward: tensor([[-1.3316]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.4212]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 425.0
Loss: 1.9682852029800415
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-1.5310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.9691]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 490.0
Loss: 2.2426373958587646
Action 0 - predicted reward: tensor([[-0.3242]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3711]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 290.0
Loss: 0.7326379418373108
Action 0 - predicted reward: tensor([[-0.0189]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0164]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 225.0
Loss: 0.014765019528567791
Action 0 - predicted reward: tensor([[-0.9840]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.0611]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 375.0
Loss: 0.8676269054412842
Greedy
Action 0 - predicted reward: tensor([[-0.8141]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.2000]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 390.0
Loss: 1.0636951923370361
Action 0 - predicted reward: tensor([[-1.1505]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.2157]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 280.0
Loss: 0.3681822121143341
Action 0 - predicted reward: tensor([[0.5042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.5662]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 320.0
Loss: 2.611004114151001
Action 0 - predicted reward: tensor([[-0.9821]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.0378]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 555.0
Loss: 3.050130605697632
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.7959]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7968]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 415.0
Loss: 57529.109375
KL Divergence: 821.4031372070312
Action 0 - predicted reward: tensor([[-1.1207]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1254]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 545.0
Loss: 85157.2421875
KL Divergence: 824.2286987304688
Action 0 - predicted reward: tensor([[-1.1875]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1934]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 430.0
Loss: 58775.765625
KL Divergence: 822.286376953125
Action 0 - predicted reward: tensor([[-1.5048]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.5193]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 435.0
Loss: 47964.078125
KL Divergence: 822.1572875976562
199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[5.0448]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0812]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 445.0
Loss: 0.9178194999694824
Action 0 - predicted reward: tensor([[-3.2901]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.7119]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 650.0
Loss: 0.2620789408683777
Action 0 - predicted reward: tensor([[-0.6371]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7486]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 535.0
Loss: 0.19368982315063477
Action 0 - predicted reward: tensor([[-0.9174]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.4396]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 715.0
Loss: 1.0377681255340576
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.8776]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.6089]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 755.0
Loss: 0.22105033695697784
Action 0 - predicted reward: tensor([[-2.8646]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.2533]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 570.0
Loss: 0.2831271290779114
Action 0 - predicted reward: tensor([[0.0198]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0082]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 540.0
Loss: 0.20313087105751038
Action 0 - predicted reward: tensor([[-0.6547]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.8352]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 600.0
Loss: 0.2868712544441223
Greedy
Action 0 - predicted reward: tensor([[0.2075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.1753]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 645.0
Loss: 0.21041983366012573
Action 0 - predicted reward: tensor([[-0.2396]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4261]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 535.0
Loss: 0.1715301126241684
Action 0 - predicted reward: tensor([[2.0961]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2811]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 550.0
Loss: 0.9643277525901794
Action 0 - predicted reward: tensor([[2.2842]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.0109]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 845.0
Loss: 0.6075171828269958
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.7221]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7214]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 720.0
Loss: 40287.93359375
KL Divergence: 407.6075134277344
Action 0 - predicted reward: tensor([[-1.1746]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1775]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 915.0
Loss: 58356.74609375
KL Divergence: 408.32928466796875
Action 0 - predicted reward: tensor([[-0.6791]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6796]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 735.0
Loss: 35407.49609375
KL Divergence: 407.56414794921875
Action 0 - predicted reward: tensor([[-0.7525]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7595]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 670.0
Loss: 33256.26953125
KL Divergence: 406.5008544921875
299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[2.2457]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.8903]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 700.0
Loss: 0.09078551083803177
Action 0 - predicted reward: tensor([[0.2775]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1435]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 970.0
Loss: 0.04820370301604271
Action 0 - predicted reward: tensor([[0.2040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2001]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 780.0
Loss: 0.1260618418455124
Action 0 - predicted reward: tensor([[1.0220]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0046]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 980.0
Loss: 0.07040296494960785
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2091]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.0446]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1040.0
Loss: 0.03588663786649704
Action 0 - predicted reward: tensor([[0.5409]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4994]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 810.0
Loss: 0.17667454481124878
Action 0 - predicted reward: tensor([[-0.1488]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1735]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 820.0
Loss: 0.1393854171037674
Action 0 - predicted reward: tensor([[-0.3099]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7499]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 820.0
Loss: 0.1770809292793274
Greedy
Action 0 - predicted reward: tensor([[0.1442]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8613]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 900.0
Loss: 0.019392235204577446
Action 0 - predicted reward: tensor([[0.2845]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0350]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 820.0
Loss: 0.08363465964794159
Action 0 - predicted reward: tensor([[0.8581]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.9868]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 770.0
Loss: 0.5022425055503845
Action 0 - predicted reward: tensor([[0.7840]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.7484]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1090.0
Loss: 0.3148708939552307
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.4688]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4717]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 945.0
Loss: 29488.3359375
KL Divergence: 270.94537353515625
Action 0 - predicted reward: tensor([[-1.0374]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0378]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1255.0
Loss: 55667.05859375
KL Divergence: 271.0538635253906
Action 0 - predicted reward: tensor([[-0.3813]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3825]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 985.0
Loss: 28770.236328125
KL Divergence: 270.3578796386719
Action 0 - predicted reward: tensor([[-0.5858]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5911]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1005.0
Loss: 25814.642578125
KL Divergence: 269.92230224609375
399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3923]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3677]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 830.0
Loss: 0.043524932116270065
Action 0 - predicted reward: tensor([[-2.8423]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.0048]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1230.0
Loss: 0.010868320241570473
Action 0 - predicted reward: tensor([[-0.2287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.0889]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 990.0
Loss: 0.09053047001361847
Action 0 - predicted reward: tensor([[-1.4937]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.5754]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1470.0
Loss: 0.736681342124939
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2906]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3841]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1325.0
Loss: 0.22876767814159393
Action 0 - predicted reward: tensor([[-0.6726]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.8690]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1055.0
Loss: 0.11384494602680206
Action 0 - predicted reward: tensor([[-0.1147]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1466]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1045.0
Loss: 0.10627284646034241
Action 0 - predicted reward: tensor([[0.3266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1095.0
Loss: 0.20516452193260193
Greedy
Action 0 - predicted reward: tensor([[0.0890]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.4552]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1135.0
Loss: 0.01036588940769434
Action 0 - predicted reward: tensor([[0.0693]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1196]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1030.0
Loss: 0.03211415186524391
Action 0 - predicted reward: tensor([[0.9425]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.7747]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 920.0
Loss: 0.14779815077781677
Action 0 - predicted reward: tensor([[0.6559]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3457]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1465.0
Loss: 0.34563079476356506
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.4196]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4203]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1265.0
Loss: 31386.501953125
KL Divergence: 202.39865112304688
Action 0 - predicted reward: tensor([[-1.1843]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1839]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1780.0
Loss: 67255.6875
KL Divergence: 202.6587371826172
Action 0 - predicted reward: tensor([[-0.1638]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1640]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1190.0
Loss: 23834.193359375
KL Divergence: 202.2218017578125
Action 0 - predicted reward: tensor([[-0.4370]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4377]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1265.0
Loss: 19521.03125
KL Divergence: 201.51156616210938
499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3815]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4223]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 960.0
Loss: 0.031492311507463455
Action 0 - predicted reward: tensor([[0.1260]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1208]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1500.0
Loss: 0.016679132357239723
Action 0 - predicted reward: tensor([[0.4812]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4593]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1265.0
Loss: 0.09032569825649261
Action 0 - predicted reward: tensor([[0.5009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2496]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1555.0
Loss: 0.2093994915485382
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.7692]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.7078]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1715.0
Loss: 0.4180396795272827
Action 0 - predicted reward: tensor([[0.4207]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.9350]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1315.0
Loss: 0.05456315726041794
Action 0 - predicted reward: tensor([[0.2387]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2215]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1290.0
Loss: 0.13083547353744507
Action 0 - predicted reward: tensor([[0.0408]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.3289]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1375.0
Loss: 0.0351874977350235
Greedy
Action 0 - predicted reward: tensor([[0.0869]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.0225]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1380.0
Loss: 0.010806807316839695
Action 0 - predicted reward: tensor([[-0.8293]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.7978]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1310.0
Loss: 0.006769439671188593
Action 0 - predicted reward: tensor([[0.9927]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6095]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 995.0
Loss: 0.03132554888725281
Action 0 - predicted reward: tensor([[-0.1215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.3622]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1690.0
Loss: 0.2241230458021164
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3008]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3048]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1515.0
Loss: 26810.400390625
KL Divergence: 161.3815155029297
Action 0 - predicted reward: tensor([[-1.2188]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.2227]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2120.0
Loss: 65341.83984375
KL Divergence: 162.03915405273438
Action 0 - predicted reward: tensor([[-0.3540]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3540]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1575.0
Loss: 32541.587890625
KL Divergence: 161.3636474609375
Action 0 - predicted reward: tensor([[-0.2812]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2821]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1465.0
Loss: 15870.173828125
KL Divergence: 160.93299865722656
599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0701]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.4113]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1070.0
Loss: 0.09702683985233307
Action 0 - predicted reward: tensor([[0.3578]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.3735]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1640.0
Loss: 0.11517344415187836
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1495.0
Loss: 0.0300386231392622
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1645.0
Loss: 0.1676885038614273
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.9462]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.5821]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1925.0
Loss: 0.18309062719345093
Action 0 - predicted reward: tensor([[0.0153]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3285]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1510.0
Loss: 0.02247278019785881
Action 0 - predicted reward: tensor([[-0.3121]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4494]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1450.0
Loss: 0.14062319695949554
Action 0 - predicted reward: tensor([[0.1226]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1881]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1660.0
Loss: 0.00822539534419775
Greedy
Action 0 - predicted reward: tensor([[0.1865]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.9316]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1560.0
Loss: 0.052038323134183884
Action 0 - predicted reward: tensor([[-0.0891]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3275]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1545.0
Loss: 0.006109597627073526
Action 0 - predicted reward: tensor([[-0.1767]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9245]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1205.0
Loss: 0.10425549000501633
Action 0 - predicted reward: tensor([[-0.0014]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.5739]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1840.0
Loss: 0.038160111755132675
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3389]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3411]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1850.0
Loss: 33147.3046875
KL Divergence: 134.27992248535156
Action 0 - predicted reward: tensor([[-1.3210]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.3274]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2505.0
Loss: 63882.859375
KL Divergence: 134.6802215576172
Action 0 - predicted reward: tensor([[-0.4710]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4709]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1930.0
Loss: 36535.4609375
KL Divergence: 134.25123596191406
Action 0 - predicted reward: tensor([[-0.1825]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1855]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1715.0
Loss: 13473.93359375
KL Divergence: 133.77737426757812
699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0207]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.1426]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1255.0
Loss: 0.03673133999109268
Action 0 - predicted reward: tensor([[0.1278]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.5289]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1900.0
Loss: 0.07014122605323792
Action 0 - predicted reward: tensor([[0.0796]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1450]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1865.0
Loss: 0.16459444165229797
Action 0 - predicted reward: tensor([[-1.1919]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.7957]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1700.0
Loss: 0.037506118416786194
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0495]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3292]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2020.0
Loss: 0.11389623582363129
Action 0 - predicted reward: tensor([[-2.0229]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.1489]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1740.0
Loss: 0.03770747035741806
Action 0 - predicted reward: tensor([[1.1893]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.9763]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1645.0
Loss: 0.22826160490512848
Action 0 - predicted reward: tensor([[0.1869]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1052]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1930.0
Loss: 0.004187031649053097
Greedy
Action 0 - predicted reward: tensor([[-0.1292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1110]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1785.0
Loss: 0.031155869364738464
Action 0 - predicted reward: tensor([[-0.1278]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.3969]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1795.0
Loss: 0.003526791464537382
Action 0 - predicted reward: tensor([[0.9561]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.8930]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1395.0
Loss: 0.11156895011663437
Action 0 - predicted reward: tensor([[-0.4538]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.7304]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2055.0
Loss: 0.14976255595684052
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.2596]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2570]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2135.0
Loss: 33743.6953125
KL Divergence: 114.83677673339844
Action 0 - predicted reward: tensor([[-1.2112]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.2111]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2865.0
Loss: 62796.703125
KL Divergence: 115.16081237792969
Action 0 - predicted reward: tensor([[-0.2968]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2997]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2165.0
Loss: 33513.02734375
KL Divergence: 114.92221069335938
Action 0 - predicted reward: tensor([[-0.2661]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2683]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2100.0
Loss: 17927.55078125
KL Divergence: 114.55770111083984
799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.6236]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.2405]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1470.0
Loss: 0.08387507498264313
Action 0 - predicted reward: tensor([[-0.1432]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.1056]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1985.0
Loss: 0.10733995586633682
Action 0 - predicted reward: tensor([[0.1236]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.6997]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2080.0
Loss: 0.05665533244609833
Action 0 - predicted reward: tensor([[-0.0823]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1855.0
Loss: 0.022311676293611526
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1537]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7876]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2180.0
Loss: 0.09190458059310913
Action 0 - predicted reward: tensor([[-0.1291]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.8917]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1895.0
Loss: 0.004258149303495884
Action 0 - predicted reward: tensor([[-0.0622]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.1607]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1765.0
Loss: 0.20272548496723175
Action 0 - predicted reward: tensor([[0.1679]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0047]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2265.0
Loss: 0.05474327132105827
Greedy
Action 0 - predicted reward: tensor([[0.1381]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.7785]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1965.0
Loss: 0.07922019064426422
Action 0 - predicted reward: tensor([[-0.0247]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.0240]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2010.0
Loss: 0.0018787295557558537
Action 0 - predicted reward: tensor([[0.1960]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.2060]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1505.0
Loss: 0.030311331152915955
Action 0 - predicted reward: tensor([[-0.3205]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4903]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2240.0
Loss: 0.08375834673643112
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.2988]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3017]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2470.0
Loss: 36847.1875
KL Divergence: 100.34304809570312
Action 0 - predicted reward: tensor([[-1.0315]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0333]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3075.0
Loss: 55007.78515625
KL Divergence: 100.5071792602539
Action 0 - predicted reward: tensor([[-0.3103]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3115]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2480.0
Loss: 32419.939453125
KL Divergence: 100.38215637207031
Action 0 - predicted reward: tensor([[-0.1665]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1684]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2290.0
Loss: 16031.8310546875
KL Divergence: 100.07098388671875
899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2813]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2974]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1720.0
Loss: 0.09090573340654373
Action 0 - predicted reward: tensor([[0.1630]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.6414]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2000.0
Loss: 0.02848156914114952
Action 0 - predicted reward: tensor([[-0.2457]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.0303]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2195.0
Loss: 0.03949979320168495
Action 0 - predicted reward: tensor([[-0.1995]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.2950]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2155.0
Loss: 0.07103101164102554
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0909]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.6985]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2250.0
Loss: 0.08160486817359924
Action 0 - predicted reward: tensor([[-0.1138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.0074]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1990.0
Loss: 0.04613950103521347
Action 0 - predicted reward: tensor([[-1.7516]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.6186]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1805.0
Loss: 0.0742434486746788
Action 0 - predicted reward: tensor([[0.3027]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.3482]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2535.0
Loss: 0.06314217299222946
Greedy
Action 0 - predicted reward: tensor([[-0.2259]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.6713]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2155.0
Loss: 0.057307541370391846
Action 0 - predicted reward: tensor([[0.0096]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.3588]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2260.0
Loss: 0.0010078110499307513
Action 0 - predicted reward: tensor([[-0.3693]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.9499]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1700.0
Loss: 0.11984401941299438
Action 0 - predicted reward: tensor([[-0.0961]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2119]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2290.0
Loss: 0.04512816667556763
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.4668]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4669]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2920.0
Loss: 41334.05078125
KL Divergence: 89.11302947998047
Action 0 - predicted reward: tensor([[-0.8442]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8454]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3285.0
Loss: 51396.58203125
KL Divergence: 89.10249328613281
Action 0 - predicted reward: tensor([[-0.2021]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2037]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2660.0
Loss: 30445.505859375
KL Divergence: 89.13311004638672
Action 0 - predicted reward: tensor([[-0.2154]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2180]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2615.0
Loss: 19462.8671875
KL Divergence: 88.79409790039062
999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.7822]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1825.0
Loss: 0.06317289918661118
Action 0 - predicted reward: tensor([[-0.1761]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9778]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2065.0
Loss: 0.02027990110218525
Action 0 - predicted reward: tensor([[-0.3449]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4429]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2295.0
Loss: 0.027395611628890038
Action 0 - predicted reward: tensor([[-0.5187]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7904]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2345.0
Loss: 0.09577763825654984
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3764]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.8205]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2330.0
Loss: 0.054518572986125946
Action 0 - predicted reward: tensor([[0.0019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.5871]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2095.0
Loss: 0.038489919155836105
Action 0 - predicted reward: tensor([[-0.9407]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5712]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1875.0
Loss: 0.054157353937625885
Action 0 - predicted reward: tensor([[0.4231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2765.0
Loss: 0.029064102098345757
Greedy
Action 0 - predicted reward: tensor([[-0.5478]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.8621]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2215.0
Loss: 0.02467161975800991
Action 0 - predicted reward: tensor([[0.0203]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3575]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2520.0
Loss: 0.0005977759137749672
Action 0 - predicted reward: tensor([[-0.8276]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.6166]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1945.0
Loss: 0.06713610142469406
Action 0 - predicted reward: tensor([[-0.6981]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.8545]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2430.0
Loss: 0.0739060565829277
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.4219]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4244]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3260.0
Loss: 40017.5078125
KL Divergence: 80.05438995361328
Action 0 - predicted reward: tensor([[-0.7471]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7499]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3490.0
Loss: 45771.93359375
KL Divergence: 80.02495574951172
Action 0 - predicted reward: tensor([[-0.0770]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0760]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2895.0
Loss: 28461.279296875
KL Divergence: 80.02760314941406
Action 0 - predicted reward: tensor([[-0.2085]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2116]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2860.0
Loss: 19898.884765625
KL Divergence: 79.83385467529297
1099.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2040.0
Loss: 0.0838300958275795
Action 0 - predicted reward: tensor([[-0.0670]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2657]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2155.0
Loss: 0.02301018498837948
Action 0 - predicted reward: tensor([[0.3378]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.1625]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2370.0
Loss: 0.018478799611330032
Action 0 - predicted reward: tensor([[0.0987]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.4859]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2430.0
Loss: 0.10507210344076157
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1683]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.7903]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2495.0
Loss: 0.08343619108200073
Action 0 - predicted reward: tensor([[-0.0702]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.4384]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2120.0
Loss: 0.012698146514594555
Action 0 - predicted reward: tensor([[-0.5873]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6557]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1965.0
Loss: 0.03460937738418579
Action 0 - predicted reward: tensor([[0.1953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.0043]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2895.0
Loss: 0.023855216801166534
Greedy
Action 0 - predicted reward: tensor([[-0.1506]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.0302]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2305.0
Loss: 0.04917890951037407
Action 0 - predicted reward: tensor([[0.0758]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2669]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2760.0
Loss: 0.00043808642658405006
Action 0 - predicted reward: tensor([[0.2921]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9628]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2155.0
Loss: 0.09047118574380875
Action 0 - predicted reward: tensor([[0.0363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2629]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2540.0
Loss: 0.07271089404821396
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3268]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3292]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3500.0
Loss: 38060.703125
KL Divergence: 72.706787109375
Action 0 - predicted reward: tensor([[-0.5512]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5517]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3670.0
Loss: 44917.890625
KL Divergence: 72.63465881347656
Action 0 - predicted reward: tensor([[-0.0620]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0632]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3135.0
Loss: 27495.8203125
KL Divergence: 72.65965270996094
Action 0 - predicted reward: tensor([[-0.1166]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1161]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3050.0
Loss: 19555.298828125
KL Divergence: 72.50041198730469
1199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.8034]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.0281]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2155.0
Loss: 0.04256802424788475
Action 0 - predicted reward: tensor([[-0.9146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.2943]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2460.0
Loss: 0.09528712928295135
Action 0 - predicted reward: tensor([[0.0900]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4461]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2570.0
Loss: 0.060014840215444565
Action 0 - predicted reward: tensor([[0.6489]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.1271]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2445.0
Loss: 0.02781098335981369
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0356]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2495.0
Loss: 0.05883539095520973
Action 0 - predicted reward: tensor([[0.0483]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.8596]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2250.0
Loss: 0.017669230699539185
Action 0 - predicted reward: tensor([[0.6702]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.6734]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2025.0
Loss: 0.022036941722035408
Action 0 - predicted reward: tensor([[-0.0287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7335]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3045.0
Loss: 0.010770521126687527
Greedy
Action 0 - predicted reward: tensor([[0.1070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.4444]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2310.0
Loss: 0.002402358688414097
Action 0 - predicted reward: tensor([[-0.0292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2509]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2980.0
Loss: 0.0003535616269800812
Action 0 - predicted reward: tensor([[0.2197]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.1897]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2300.0
Loss: 0.10235164314508438
Action 0 - predicted reward: tensor([[-0.3655]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.9504]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2685.0
Loss: 0.10344405472278595
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3287]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3326]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3765.0
Loss: 39242.4609375
KL Divergence: 66.55787658691406
Action 0 - predicted reward: tensor([[-0.4450]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4461]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3825.0
Loss: 41732.19140625
KL Divergence: 66.47029113769531
Action 0 - predicted reward: tensor([[-0.0463]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0518]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3355.0
Loss: 26124.787109375
KL Divergence: 66.52609252929688
Action 0 - predicted reward: tensor([[-0.1159]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1170]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3305.0
Loss: 21712.142578125
KL Divergence: 66.3309326171875
1299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5812]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2305.0
Loss: 0.04349782317876816
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2550.0
Loss: 0.041784461587667465
Action 0 - predicted reward: tensor([[-0.2943]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.1782]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2700.0
Loss: 0.03478485345840454
Action 0 - predicted reward: tensor([[0.0395]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1861]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2575.0
Loss: 0.04181031882762909
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1703]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.1933]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2640.0
Loss: 0.08330799639225006
Action 0 - predicted reward: tensor([[0.0150]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1078]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2320.0
Loss: 0.005764540284872055
Action 0 - predicted reward: tensor([[-0.3175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.3311]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2095.0
Loss: 0.031471483409404755
Action 0 - predicted reward: tensor([[-0.0105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7756]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3070.0
Loss: 0.0009562930790707469
Greedy
Action 0 - predicted reward: tensor([[-0.0642]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2345]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2310.0
Loss: 0.00045852380571886897
Action 0 - predicted reward: tensor([[-0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1721]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3240.0
Loss: 0.00030870549380779266
Action 0 - predicted reward: tensor([[-0.0175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.1428]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2475.0
Loss: 0.12544412910938263
Action 0 - predicted reward: tensor([[0.0100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.6718]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2800.0
Loss: 0.0809260681271553
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.2945]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2973]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4080.0
Loss: 39971.296875
KL Divergence: 61.37901306152344
Action 0 - predicted reward: tensor([[-0.3635]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3652]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3995.0
Loss: 39157.53515625
KL Divergence: 61.2515869140625
Action 0 - predicted reward: tensor([[-0.0062]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0124]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3545.0
Loss: 24735.005859375
KL Divergence: 61.36207962036133
Action 0 - predicted reward: tensor([[-0.0522]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0530]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3560.0
Loss: 22627.396484375
KL Divergence: 61.186580657958984
1399.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2415.0
Loss: 0.04770928621292114
Action 0 - predicted reward: tensor([[-0.0585]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4651]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2625.0
Loss: 0.021053113043308258
Action 0 - predicted reward: tensor([[0.1237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0512]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2815.0
Loss: 0.04152685031294823
Action 0 - predicted reward: tensor([[0.0419]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9830]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2650.0
Loss: 0.0377705916762352
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-71.0414]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2675.0
Loss: 0.06358491629362106
Action 0 - predicted reward: tensor([[-0.0021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2918]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2430.0
Loss: 0.02456972375512123
Action 0 - predicted reward: tensor([[0.0289]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0280]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2225.0
Loss: 0.040272120386362076
Action 0 - predicted reward: tensor([[0.1842]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.2331]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3330.0
Loss: 0.04310443997383118
Greedy
Action 0 - predicted reward: tensor([[-0.2707]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.8531]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2420.0
Loss: 0.020266883075237274
Action 0 - predicted reward: tensor([[-0.0989]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1983]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3515.0
Loss: 0.0002893172495532781
Action 0 - predicted reward: tensor([[0.1160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4981]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2545.0
Loss: 0.07645648717880249
Action 0 - predicted reward: tensor([[-0.1028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.3179]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2905.0
Loss: 0.06746633350849152
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.2650]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2650]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4290.0
Loss: 38927.66796875
KL Divergence: 56.924644470214844
Action 0 - predicted reward: tensor([[-0.2524]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2512]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4175.0
Loss: 36898.91796875
KL Divergence: 56.7692985534668
Action 0 - predicted reward: tensor([[0.0436]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0403]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3765.0
Loss: 23473.216796875
KL Divergence: 56.90496063232422
Action 0 - predicted reward: tensor([[-0.0002]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0013]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3805.0
Loss: 24418.748046875
KL Divergence: 56.736934661865234
1499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5184]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.1137]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2460.0
Loss: 0.0362747386097908
Action 0 - predicted reward: tensor([[-0.1513]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.3102]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2770.0
Loss: 0.040236398577690125
Action 0 - predicted reward: tensor([[-0.0293]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.2377]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2995.0
Loss: 0.05536703020334244
Action 0 - predicted reward: tensor([[-0.4518]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2364]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2725.0
Loss: 0.043202225118875504
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3343]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.9822]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2780.0
Loss: 0.06693839281797409
Action 0 - predicted reward: tensor([[-0.0277]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1710]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2515.0
Loss: 0.017467880621552467
Action 0 - predicted reward: tensor([[-0.0286]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.4840]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2330.0
Loss: 0.04149053245782852
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3430.0
Loss: 0.04016164690256119
Greedy
Action 0 - predicted reward: tensor([[-0.0513]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5308]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2425.0
Loss: 0.0140900369733572
Action 0 - predicted reward: tensor([[-0.0215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0949]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3765.0
Loss: 0.00026051196618936956
Action 0 - predicted reward: tensor([[-0.7718]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8776]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2615.0
Loss: 0.07857199758291245
Action 0 - predicted reward: tensor([[-0.2171]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.8336]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3115.0
Loss: 0.06568992137908936
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.1462]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1477]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4395.0
Loss: 36319.30859375
KL Divergence: 53.02113723754883
Action 0 - predicted reward: tensor([[-0.1629]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1643]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4310.0
Loss: 35061.5078125
KL Divergence: 52.932777404785156
Action 0 - predicted reward: tensor([[0.0702]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0693]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4030.0
Loss: 23057.931640625
KL Divergence: 53.04039001464844
Action 0 - predicted reward: tensor([[-0.0035]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0059]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4075.0
Loss: 26373.962890625
KL Divergence: 52.897422790527344
1599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5797]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0311]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2570.0
Loss: 0.024665379896759987
Action 0 - predicted reward: tensor([[0.3521]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.1159]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2915.0
Loss: 0.031070146709680557
Action 0 - predicted reward: tensor([[-0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0801]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3080.0
Loss: 0.04367063194513321
Action 0 - predicted reward: tensor([[0.0699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3133]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2805.0
Loss: 0.03764359652996063
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2227]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8124]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2860.0
Loss: 0.046200551092624664
Action 0 - predicted reward: tensor([[0.0558]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2898]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2590.0
Loss: 0.029780227690935135
Action 0 - predicted reward: tensor([[0.5548]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.8792]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2405.0
Loss: 0.05096244812011719
Action 0 - predicted reward: tensor([[-0.2459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.8982]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3690.0
Loss: 0.06451307237148285
Greedy
Action 0 - predicted reward: tensor([[0.0564]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.9245]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2600.0
Loss: 0.03122023493051529
Action 0 - predicted reward: tensor([[-0.0292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2163]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4040.0
Loss: 0.00023829143901821226
Action 0 - predicted reward: tensor([[-0.1918]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.9940]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2765.0
Loss: 0.05856654793024063
Action 0 - predicted reward: tensor([[-0.1833]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.4959]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3255.0
Loss: 0.03211953863501549
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.1477]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1548]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4605.0
Loss: 36217.85546875
KL Divergence: 49.67094802856445
Action 0 - predicted reward: tensor([[-0.1056]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1068]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4485.0
Loss: 33200.37890625
KL Divergence: 49.55988693237305
Action 0 - predicted reward: tensor([[0.0740]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0722]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4275.0
Loss: 22432.4453125
KL Divergence: 49.689971923828125
Action 0 - predicted reward: tensor([[0.0688]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0703]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4285.0
Loss: 27661.359375
KL Divergence: 49.54598617553711
1699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.6768]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2710.0
Loss: 0.028337858617305756
Action 0 - predicted reward: tensor([[-0.3805]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.8116]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3130.0
Loss: 0.1833336502313614
Action 0 - predicted reward: tensor([[-0.0635]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.1942]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3160.0
Loss: 0.05822007730603218
Action 0 - predicted reward: tensor([[0.0163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.5449]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2855.0
Loss: 0.026688989251852036
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0980]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3040.0
Loss: 0.04622893035411835
Action 0 - predicted reward: tensor([[-0.0852]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.4874]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2665.0
Loss: 0.047094088047742844
Action 0 - predicted reward: tensor([[0.0452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.4320]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2425.0
Loss: 0.03121854178607464
Action 0 - predicted reward: tensor([[-0.0236]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.9021]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3860.0
Loss: 0.04816892370581627
Greedy
Action 0 - predicted reward: tensor([[-0.0511]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9873]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2705.0
Loss: 0.033526379615068436
Action 0 - predicted reward: tensor([[-0.0029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.1528]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4260.0
Loss: 0.00022195163182914257
Action 0 - predicted reward: tensor([[-0.1261]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4054]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2870.0
Loss: 0.052558157593011856
Action 0 - predicted reward: tensor([[-0.3595]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6563]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3360.0
Loss: 0.025495609268546104
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.0470]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0431]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4705.0
Loss: 34108.20703125
KL Divergence: 46.693172454833984
Action 0 - predicted reward: tensor([[-0.0146]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0159]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4625.0
Loss: 31246.0078125
KL Divergence: 46.61415100097656
Action 0 - predicted reward: tensor([[0.0737]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0604]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4470.0
Loss: 21967.42578125
KL Divergence: 46.711944580078125
Action 0 - predicted reward: tensor([[0.0790]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0760]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4435.0
Loss: 27436.80859375
KL Divergence: 46.57370376586914
1799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.2030]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2790.0
Loss: 0.01796847954392433
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3215.0
Loss: 0.0982813909649849
Action 0 - predicted reward: tensor([[0.0805]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0495]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3300.0
Loss: 0.047925155609846115
Action 0 - predicted reward: tensor([[-4.8988]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.4437]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2930.0
Loss: 0.4682588279247284
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8440]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3115.0
Loss: 0.04022103548049927
Action 0 - predicted reward: tensor([[-0.0910]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.1946]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2675.0
Loss: 0.02375936135649681
Action 0 - predicted reward: tensor([[0.0539]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.3474]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2440.0
Loss: 0.025774462148547173
Action 0 - predicted reward: tensor([[0.0574]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7218]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4055.0
Loss: 0.05419714003801346
Greedy
Action 0 - predicted reward: tensor([[-0.0832]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.8745]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2745.0
Loss: 0.024493996053934097
Action 0 - predicted reward: tensor([[0.0120]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0712]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4480.0
Loss: 0.0002071755297947675
Action 0 - predicted reward: tensor([[-0.2349]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5107]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2870.0
Loss: 0.040946949273347855
Action 0 - predicted reward: tensor([[0.1038]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.8656]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3500.0
Loss: 0.07901160418987274
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.0599]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0585]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4800.0
Loss: 32807.27734375
KL Divergence: 44.05978775024414
Action 0 - predicted reward: tensor([[0.0277]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0104]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4835.0
Loss: 30606.16015625
KL Divergence: 43.97010040283203
Action 0 - predicted reward: tensor([[0.1486]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1507]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4615.0
Loss: 20991.20703125
KL Divergence: 44.07783889770508
Action 0 - predicted reward: tensor([[0.1782]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1798]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4545.0
Loss: 26551.8359375
KL Divergence: 43.948944091796875
1899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3473]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4523]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2870.0
Loss: 0.026163313537836075
Action 0 - predicted reward: tensor([[0.4325]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.7159]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3255.0
Loss: 0.04291268438100815
Action 0 - predicted reward: tensor([[0.1442]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9934]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3395.0
Loss: 0.04284638166427612
Action 0 - predicted reward: tensor([[2.1569]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8879]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2935.0
Loss: 0.08133987337350845
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1396]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2152]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3185.0
Loss: 0.0527157299220562
Action 0 - predicted reward: tensor([[0.0371]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.4720]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2725.0
Loss: 0.038049548864364624
Action 0 - predicted reward: tensor([[0.4109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.1173]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2480.0
Loss: 0.02890263870358467
Action 0 - predicted reward: tensor([[0.1103]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.7249]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4140.0
Loss: 0.06496015191078186
Greedy
Action 0 - predicted reward: tensor([[-0.0323]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1456]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2745.0
Loss: 0.020450346171855927
Action 0 - predicted reward: tensor([[-0.0546]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.1238]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4830.0
Loss: 0.020377278327941895
Action 0 - predicted reward: tensor([[-0.2585]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.7245]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3045.0
Loss: 0.06202533468604088
Action 0 - predicted reward: tensor([[-0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8313]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3615.0
Loss: 0.030991964042186737
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.0847]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0671]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4915.0
Loss: 31851.25390625
KL Divergence: 41.712432861328125
Action 0 - predicted reward: tensor([[0.0663]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0716]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4935.0
Loss: 29526.94921875
KL Divergence: 41.64049530029297
Action 0 - predicted reward: tensor([[0.2037]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1804]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4755.0
Loss: 19967.447265625
KL Divergence: 41.73142623901367
Action 0 - predicted reward: tensor([[0.2077]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2027]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4720.0
Loss: 26925.5390625
KL Divergence: 41.57601547241211
1999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7827]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2915.0
Loss: 0.017675071954727173
Action 0 - predicted reward: tensor([[-0.6292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6347]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3445.0
Loss: 0.558005690574646
Action 0 - predicted reward: tensor([[-0.2112]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6979]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3450.0
Loss: 0.04995131865143776
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3050.0
Loss: 0.045491866767406464
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0639]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6117]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3220.0
Loss: 0.04474914073944092
Action 0 - predicted reward: tensor([[-0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.4880]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2835.0
Loss: 0.019440069794654846
Action 0 - predicted reward: tensor([[0.1555]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9250]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2485.0
Loss: 0.017994655296206474
Action 0 - predicted reward: tensor([[0.1284]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3341]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4210.0
Loss: 0.053332772105932236
Greedy
Action 0 - predicted reward: tensor([[0.0300]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.2082]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2745.0
Loss: 0.020128414034843445
Action 0 - predicted reward: tensor([[-0.0467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.1630]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5080.0
Loss: 0.019117459654808044
Action 0 - predicted reward: tensor([[0.0429]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3730]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3085.0
Loss: 0.06309979408979416
Action 0 - predicted reward: tensor([[0.0212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1721]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3650.0
Loss: 0.025430358946323395
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.1839]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1584]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5065.0
Loss: 31299.310546875
KL Divergence: 39.575103759765625
Action 0 - predicted reward: tensor([[0.1597]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1571]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5040.0
Loss: 28338.34765625
KL Divergence: 39.5019645690918
Action 0 - predicted reward: tensor([[0.2717]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2706]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4920.0
Loss: 19362.11328125
KL Divergence: 39.59046173095703
Action 0 - predicted reward: tensor([[0.2917]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2851]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4760.0
Loss: 26298.837890625
KL Divergence: 39.47929382324219
2099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1064]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7529]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3055.0
Loss: 0.04389167204499245
Action 0 - predicted reward: tensor([[0.2327]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0528]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3495.0
Loss: 0.022274643182754517
Action 0 - predicted reward: tensor([[-0.0083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.6512]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3595.0
Loss: 0.0782390832901001
Action 0 - predicted reward: tensor([[-0.6560]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9127]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3165.0
Loss: 0.05480516329407692
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-1.0125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-69.1338]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3330.0
Loss: 0.05333647131919861
Action 0 - predicted reward: tensor([[0.0195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.2934]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2875.0
Loss: 0.03563877195119858
Action 0 - predicted reward: tensor([[-0.3449]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.1689]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2555.0
Loss: 0.017148159444332123
Action 0 - predicted reward: tensor([[0.1346]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3148]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4410.0
Loss: 0.17758916318416595
Greedy
Action 0 - predicted reward: tensor([[0.0503]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.8050]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2815.0
Loss: 0.01714027114212513
Action 0 - predicted reward: tensor([[0.0556]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4466]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5375.0
Loss: 0.017734991386532784
Action 0 - predicted reward: tensor([[0.3151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.6623]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3190.0
Loss: 0.06280416995286942
Action 0 - predicted reward: tensor([[0.1495]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.1186]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3685.0
Loss: 0.04515931010246277
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.2594]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2398]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5135.0
Loss: 30155.126953125
KL Divergence: 37.64281463623047
Action 0 - predicted reward: tensor([[0.2313]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2406]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5130.0
Loss: 27553.0
KL Divergence: 37.589752197265625
Action 0 - predicted reward: tensor([[0.2675]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2247]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5020.0
Loss: 18459.578125
KL Divergence: 37.67748260498047
Action 0 - predicted reward: tensor([[0.3660]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3687]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4855.0
Loss: 25843.314453125
KL Divergence: 37.56206512451172
2199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4335]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.4638]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3100.0
Loss: 0.024203503504395485
Action 0 - predicted reward: tensor([[0.2223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.0012]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3600.0
Loss: 0.041712574660778046
Action 0 - predicted reward: tensor([[-0.3895]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0896]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3610.0
Loss: 0.0653175413608551
Action 0 - predicted reward: tensor([[-0.0132]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.5426]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3205.0
Loss: 0.04646672308444977
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0345]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2140]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3370.0
Loss: 0.05830101668834686
Action 0 - predicted reward: tensor([[-0.0274]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9824]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2950.0
Loss: 0.03584977611899376
Action 0 - predicted reward: tensor([[-0.1440]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7020]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2595.0
Loss: 0.01567876525223255
Action 0 - predicted reward: tensor([[-0.0502]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8852]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4460.0
Loss: 0.11575759947299957
Greedy
Action 0 - predicted reward: tensor([[0.0474]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1457]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2815.0
Loss: 0.018407782539725304
Action 0 - predicted reward: tensor([[-0.1703]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8263]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5650.0
Loss: 0.016296282410621643
Action 0 - predicted reward: tensor([[0.3550]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.5244]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3260.0
Loss: 0.06893449276685715
Action 0 - predicted reward: tensor([[0.0586]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.1257]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3720.0
Loss: 0.030327962711453438
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.3428]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3498]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5180.0
Loss: 29127.916015625
KL Divergence: 35.898834228515625
Action 0 - predicted reward: tensor([[0.3070]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0332]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5200.0
Loss: 26811.251953125
KL Divergence: 35.843406677246094
Action 0 - predicted reward: tensor([[0.3625]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3651]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5115.0
Loss: 18137.3203125
KL Divergence: 35.93571472167969
Action 0 - predicted reward: tensor([[0.4768]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4810]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4885.0
Loss: 24849.712890625
KL Divergence: 35.82634735107422
2299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7195]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3215.0
Loss: 0.04263739660382271
Action 0 - predicted reward: tensor([[-0.1592]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9299]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3750.0
Loss: 0.06097693368792534
Action 0 - predicted reward: tensor([[-0.4299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7739]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3800.0
Loss: 0.061170488595962524
Action 0 - predicted reward: tensor([[-0.1810]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9046]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3330.0
Loss: 0.03536117821931839
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0169]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.3731]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3370.0
Loss: 0.047045569866895676
Action 0 - predicted reward: tensor([[-0.0013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.3685]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3020.0
Loss: 0.031296368688344955
Action 0 - predicted reward: tensor([[-0.1300]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.2106]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2605.0
Loss: 0.012690894305706024
Action 0 - predicted reward: tensor([[-0.1969]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3970]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4505.0
Loss: 0.07224349677562714
Greedy
Action 0 - predicted reward: tensor([[-0.0454]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0844]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2815.0
Loss: 0.01625439152121544
Action 0 - predicted reward: tensor([[0.0292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.6746]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5955.0
Loss: 0.01456900592893362
Action 0 - predicted reward: tensor([[0.2004]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.7473]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3330.0
Loss: 0.05174671486020088
Action 0 - predicted reward: tensor([[0.0948]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.6665]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3790.0
Loss: 0.02670533023774624
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4074]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4101]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5220.0
Loss: 28164.8203125
KL Divergence: 34.28058624267578
Action 0 - predicted reward: tensor([[0.3979]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4193]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5250.0
Loss: 25996.576171875
KL Divergence: 34.2553596496582
Action 0 - predicted reward: tensor([[0.4352]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4302]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5165.0
Loss: 17701.21484375
KL Divergence: 34.338340759277344
Action 0 - predicted reward: tensor([[0.5458]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5432]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4945.0
Loss: 24950.4140625
KL Divergence: 34.254966735839844
2399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0854]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.1553]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3325.0
Loss: 0.03306500241160393
Action 0 - predicted reward: tensor([[-0.0518]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8472]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3895.0
Loss: 0.062284741550683975
Action 0 - predicted reward: tensor([[0.1616]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1797]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3845.0
Loss: 0.05430638790130615
Action 0 - predicted reward: tensor([[-0.1972]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5774]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3365.0
Loss: 0.03549913316965103
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5955]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3405.0
Loss: 0.04482831433415413
Action 0 - predicted reward: tensor([[0.2206]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.1558]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3065.0
Loss: 0.03363719582557678
Action 0 - predicted reward: tensor([[-0.0449]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.3267]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2715.0
Loss: 0.018996257334947586
Action 0 - predicted reward: tensor([[-0.2062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9191]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4565.0
Loss: 0.043397389352321625
Greedy
Action 0 - predicted reward: tensor([[0.0034]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9793]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2890.0
Loss: 0.01434576977044344
Action 0 - predicted reward: tensor([[0.1666]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.9359]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 0.012197653762996197
Action 0 - predicted reward: tensor([[0.1477]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6443]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3440.0
Loss: 0.06959622353315353
Action 0 - predicted reward: tensor([[-0.1149]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2189]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3900.0
Loss: 0.038920652121305466
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4401]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4550]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5330.0
Loss: 27760.451171875
KL Divergence: 32.83381652832031
Action 0 - predicted reward: tensor([[0.4889]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2041]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5285.0
Loss: 24790.55078125
KL Divergence: 32.78694152832031
Action 0 - predicted reward: tensor([[0.5266]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4745]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5240.0
Loss: 17790.736328125
KL Divergence: 32.8646240234375
Action 0 - predicted reward: tensor([[0.6103]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6149]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4985.0
Loss: 24455.66015625
KL Divergence: 32.79362487792969
2499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2158]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.2496]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3445.0
Loss: 0.028697842732071877
Action 0 - predicted reward: tensor([[-0.1878]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.5172]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4080.0
Loss: 0.05758880451321602
Action 0 - predicted reward: tensor([[-0.5261]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1464]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4040.0
Loss: 0.05679108202457428
Action 0 - predicted reward: tensor([[0.0960]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8243]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3440.0
Loss: 0.0329604372382164
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3448]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0446]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3445.0
Loss: 0.040788691490888596
Action 0 - predicted reward: tensor([[-0.0296]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.3450]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3070.0
Loss: 0.02756650000810623
Action 0 - predicted reward: tensor([[0.6143]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.8026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2760.0
Loss: 0.01761329174041748
Action 0 - predicted reward: tensor([[0.0999]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2374]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4600.0
Loss: 0.04564172402024269
Greedy
Action 0 - predicted reward: tensor([[0.0042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1461]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2960.0
Loss: 0.020620072260499
Action 0 - predicted reward: tensor([[-0.0836]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.7362]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6485.0
Loss: 0.009312890470027924
Action 0 - predicted reward: tensor([[0.1279]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.3548]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3440.0
Loss: 0.05621149390935898
Action 0 - predicted reward: tensor([[0.0640]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.9658]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3905.0
Loss: 0.028696691617369652
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4694]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3393]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5375.0
Loss: 27907.625
KL Divergence: 31.501224517822266
Action 0 - predicted reward: tensor([[0.5217]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5628]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5320.0
Loss: 24585.22265625
KL Divergence: 31.441679000854492
Action 0 - predicted reward: tensor([[0.5787]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5369]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5300.0
Loss: 17375.16796875
KL Divergence: 31.525922775268555
Action 0 - predicted reward: tensor([[0.6541]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6392]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5030.0
Loss: 24106.314453125
KL Divergence: 31.449542999267578
2599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0851]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.5265]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3535.0
Loss: 0.022040696814656258
Action 0 - predicted reward: tensor([[0.1343]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.9872]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4120.0
Loss: 0.049547336995601654
Action 0 - predicted reward: tensor([[-0.1489]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.1535]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4215.0
Loss: 0.05573255568742752
Action 0 - predicted reward: tensor([[0.1129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.1512]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3550.0
Loss: 0.03445514664053917
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.1712]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3485.0
Loss: 0.038845427334308624
Action 0 - predicted reward: tensor([[0.0656]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.5872]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3070.0
Loss: 0.019225964322686195
Action 0 - predicted reward: tensor([[0.0601]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.6805]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2840.0
Loss: 0.018829865381121635
Action 0 - predicted reward: tensor([[-0.2021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9911]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4640.0
Loss: 0.04855769872665405
Greedy
Action 0 - predicted reward: tensor([[-0.0228]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1757]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2965.0
Loss: 0.012722386978566647
Action 0 - predicted reward: tensor([[-0.0783]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.0086]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6735.0
Loss: 0.00658649206161499
Action 0 - predicted reward: tensor([[-0.3993]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-100.4295]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3545.0
Loss: 0.061443861573934555
Action 0 - predicted reward: tensor([[0.0894]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.8210]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3945.0
Loss: 0.02474460005760193
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.5906]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6226]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5420.0
Loss: 27602.263671875
KL Divergence: 30.258142471313477
Action 0 - predicted reward: tensor([[0.6011]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6373]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5360.0
Loss: 23777.615234375
KL Divergence: 30.180320739746094
Action 0 - predicted reward: tensor([[0.6576]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5104]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5315.0
Loss: 16988.966796875
KL Divergence: 30.310672760009766
Action 0 - predicted reward: tensor([[0.7370]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7427]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5045.0
Loss: 23826.529296875
KL Divergence: 30.21244239807129
2699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4956]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.9917]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3735.0
Loss: 0.03153499960899353
Action 0 - predicted reward: tensor([[-0.2024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5008]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4160.0
Loss: 0.04824952781200409
Action 0 - predicted reward: tensor([[-0.2616]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0065]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4255.0
Loss: 0.060982394963502884
Action 0 - predicted reward: tensor([[0.0193]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.0262]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3830.0
Loss: 0.05148022249341011
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0323]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3595.0
Loss: 0.04848799481987953
Action 0 - predicted reward: tensor([[-0.0725]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0187]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3110.0
Loss: 0.021450787782669067
Action 0 - predicted reward: tensor([[0.1228]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.2257]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2890.0
Loss: 0.023899216204881668
Action 0 - predicted reward: tensor([[-0.0960]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.8199]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4640.0
Loss: 0.053198207169771194
Greedy
Action 0 - predicted reward: tensor([[0.0616]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0129]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2970.0
Loss: 0.011581831611692905
Action 0 - predicted reward: tensor([[-0.0259]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.1463]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7020.0
Loss: 0.004332049749791622
Action 0 - predicted reward: tensor([[-0.6323]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.1299]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3615.0
Loss: 0.06683720648288727
Action 0 - predicted reward: tensor([[-0.0478]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.1214]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4085.0
Loss: 0.03303451091051102
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.5886]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3992]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5475.0
Loss: 26941.19921875
KL Divergence: 29.0964412689209
Action 0 - predicted reward: tensor([[0.6962]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7563]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5370.0
Loss: 23382.568359375
KL Divergence: 29.044939041137695
Action 0 - predicted reward: tensor([[0.7011]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4499]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5330.0
Loss: 16691.2109375
KL Divergence: 29.154672622680664
Action 0 - predicted reward: tensor([[0.7781]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7903]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5055.0
Loss: 23336.994140625
KL Divergence: 29.07001495361328
2799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3731]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.7146]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3880.0
Loss: 0.01987984962761402
Action 0 - predicted reward: tensor([[0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.1030]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4405.0
Loss: 0.0596730038523674
Action 0 - predicted reward: tensor([[0.2317]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2124]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4290.0
Loss: 0.040934085845947266
Action 0 - predicted reward: tensor([[0.2156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0702]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3910.0
Loss: 0.05948192626237869
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1884]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1420]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3630.0
Loss: 0.04483754560351372
Action 0 - predicted reward: tensor([[-0.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.3809]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3155.0
Loss: 0.015723230317234993
Action 0 - predicted reward: tensor([[0.5717]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4894]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2935.0
Loss: 0.01704249158501625
Action 0 - predicted reward: tensor([[0.0310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8212]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4800.0
Loss: 0.07211341708898544
Greedy
Action 0 - predicted reward: tensor([[0.0470]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.6835]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2980.0
Loss: 0.010784769430756569
Action 0 - predicted reward: tensor([[-0.0867]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.1706]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7305.0
Loss: 0.0027832272462546825
Action 0 - predicted reward: tensor([[0.0919]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.0439]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3650.0
Loss: 0.06307797878980637
Action 0 - predicted reward: tensor([[-0.1900]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7610]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4190.0
Loss: 0.035864606499671936
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6877]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5658]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5525.0
Loss: 26874.59375
KL Divergence: 28.022798538208008
Action 0 - predicted reward: tensor([[0.6961]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7644]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5450.0
Loss: 23198.322265625
KL Divergence: 27.9545955657959
Action 0 - predicted reward: tensor([[0.7750]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6247]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5410.0
Loss: 17187.712890625
KL Divergence: 28.08963394165039
Action 0 - predicted reward: tensor([[0.8494]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8796]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5130.0
Loss: 23587.34765625
KL Divergence: 27.991966247558594
2899.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3995.0
Loss: 0.028028130531311035
Action 0 - predicted reward: tensor([[-0.1971]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9144]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4555.0
Loss: 0.03523385524749756
Action 0 - predicted reward: tensor([[0.0740]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.3921]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4380.0
Loss: 0.04368392750620842
Action 0 - predicted reward: tensor([[-0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.1436]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4035.0
Loss: 0.05755949392914772
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0857]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.5783]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3630.0
Loss: 0.042791545391082764
Action 0 - predicted reward: tensor([[1.0146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.0892]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3195.0
Loss: 0.022167669609189034
Action 0 - predicted reward: tensor([[0.1545]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0057]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2955.0
Loss: 0.01586919277906418
Action 0 - predicted reward: tensor([[0.1935]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3384]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4835.0
Loss: 0.06646912544965744
Greedy
Action 0 - predicted reward: tensor([[0.0105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1781]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3055.0
Loss: 0.016978740692138672
Action 0 - predicted reward: tensor([[0.0708]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.2467]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7530.0
Loss: 0.0017545957816764712
Action 0 - predicted reward: tensor([[-0.2072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4455]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3720.0
Loss: 0.06761574745178223
Action 0 - predicted reward: tensor([[-0.1847]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-98.6224]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4225.0
Loss: 0.03221318870782852
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7602]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5367]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5550.0
Loss: 26321.177734375
KL Divergence: 27.022489547729492
Action 0 - predicted reward: tensor([[0.7522]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4072]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5465.0
Loss: 22678.28515625
KL Divergence: 26.99288558959961
Action 0 - predicted reward: tensor([[0.8336]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8754]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5430.0
Loss: 16899.505859375
KL Divergence: 27.07346534729004
Action 0 - predicted reward: tensor([[0.9261]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9582]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5145.0
Loss: 22811.91015625
KL Divergence: 27.006608963012695
2999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2529]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.0917]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4145.0
Loss: 0.026407696306705475
Action 0 - predicted reward: tensor([[-0.0573]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1498]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4730.0
Loss: 0.05469835549592972
Action 0 - predicted reward: tensor([[-0.0365]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7824]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4390.0
Loss: 0.035897932946681976
Action 0 - predicted reward: tensor([[0.0617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6170]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4230.0
Loss: 0.05802370607852936
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1596]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2357]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3640.0
Loss: 0.03845987468957901
Action 0 - predicted reward: tensor([[-0.2909]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0735]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3235.0
Loss: 0.013506370596587658
Action 0 - predicted reward: tensor([[-0.2076]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8293]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3015.0
Loss: 0.01820654422044754
Action 0 - predicted reward: tensor([[0.4953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1196]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4910.0
Loss: 0.06877179443836212
Greedy
Action 0 - predicted reward: tensor([[-0.0056]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.7452]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3125.0
Loss: 0.02008012682199478
Action 0 - predicted reward: tensor([[0.1514]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.0013]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7815.0
Loss: 0.0012059292057529092
Action 0 - predicted reward: tensor([[0.0606]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2298]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3720.0
Loss: 0.06030382961034775
Action 0 - predicted reward: tensor([[0.1843]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0318]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4295.0
Loss: 0.036700762808322906
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7905]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8403]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5560.0
Loss: 25673.0234375
KL Divergence: 26.08854103088379
Action 0 - predicted reward: tensor([[0.8358]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9048]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5510.0
Loss: 22555.287109375
KL Divergence: 26.070356369018555
Action 0 - predicted reward: tensor([[0.8639]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7470]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5465.0
Loss: 16997.658203125
KL Divergence: 26.14211654663086
Action 0 - predicted reward: tensor([[0.9703]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8473]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5170.0
Loss: 22545.830078125
KL Divergence: 26.100309371948242
3099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.4915]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1645]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4295.0
Loss: 0.017624691128730774
Action 0 - predicted reward: tensor([[-0.1937]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.3722]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4880.0
Loss: 0.059169258922338486
Action 0 - predicted reward: tensor([[-0.0566]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6760]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4565.0
Loss: 0.0473688580095768
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4420.0
Loss: 0.06599009037017822
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0221]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7308]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3680.0
Loss: 0.04170810431241989
Action 0 - predicted reward: tensor([[-0.3125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0639]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3275.0
Loss: 0.013153113424777985
Action 0 - predicted reward: tensor([[-0.0835]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.4751]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3065.0
Loss: 0.016407450661063194
Action 0 - predicted reward: tensor([[0.4903]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3236]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4945.0
Loss: 0.05910445749759674
Greedy
Action 0 - predicted reward: tensor([[-0.0332]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0597]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3265.0
Loss: 0.032155342400074005
Action 0 - predicted reward: tensor([[0.0170]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.4541]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8120.0
Loss: 0.0007887702668085694
Action 0 - predicted reward: tensor([[-0.0305]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2137]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3755.0
Loss: 0.05460047721862793
Action 0 - predicted reward: tensor([[-0.0212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.7474]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4405.0
Loss: 0.045653533190488815
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8094]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5379]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5565.0
Loss: 25119.09765625
KL Divergence: 25.2194881439209
Action 0 - predicted reward: tensor([[0.8576]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9294]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5650.0
Loss: 23538.76953125
KL Divergence: 25.203502655029297
Action 0 - predicted reward: tensor([[0.9135]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9719]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5475.0
Loss: 16703.46875
KL Divergence: 25.278322219848633
Action 0 - predicted reward: tensor([[1.0071]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8777]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5170.0
Loss: 21853.076171875
KL Divergence: 25.212413787841797
3199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-77.1753]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4385.0
Loss: 0.022979533299803734
Action 0 - predicted reward: tensor([[-1.1782]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4349]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4955.0
Loss: 0.04844784364104271
Action 0 - predicted reward: tensor([[0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9977]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4675.0
Loss: 0.03907659649848938
Action 0 - predicted reward: tensor([[-0.3109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1346]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4600.0
Loss: 0.051626287400722504
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0610]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6772]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3795.0
Loss: 0.04597174748778343
Action 0 - predicted reward: tensor([[0.0792]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.6369]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3310.0
Loss: 0.016110913828015327
Action 0 - predicted reward: tensor([[-0.3516]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0079]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3145.0
Loss: 0.014442406594753265
Action 0 - predicted reward: tensor([[0.2362]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.5779]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4950.0
Loss: 0.04929405078291893
Greedy
Action 0 - predicted reward: tensor([[0.1565]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7057]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3335.0
Loss: 0.036438897252082825
Action 0 - predicted reward: tensor([[0.1112]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.8307]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8410.0
Loss: 0.000564956630114466
Action 0 - predicted reward: tensor([[-0.2492]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6838]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3860.0
Loss: 0.06239115074276924
Action 0 - predicted reward: tensor([[-0.1010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-95.1806]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4485.0
Loss: 0.04231562092900276
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9242]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9489]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5565.0
Loss: 24598.6640625
KL Divergence: 24.3940486907959
Action 0 - predicted reward: tensor([[0.8688]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6430]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5660.0
Loss: 23057.26171875
KL Divergence: 24.400209426879883
Action 0 - predicted reward: tensor([[0.9988]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0727]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5515.0
Loss: 16838.689453125
KL Divergence: 24.466135025024414
Action 0 - predicted reward: tensor([[1.0351]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0890]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5170.0
Loss: 21617.087890625
KL Divergence: 24.39400863647461
3299.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4490.0
Loss: 0.02391980029642582
Action 0 - predicted reward: tensor([[0.2326]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9591]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5070.0
Loss: 0.04321841523051262
Action 0 - predicted reward: tensor([[-0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1681]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4680.0
Loss: 0.03863056004047394
Action 0 - predicted reward: tensor([[0.0856]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3486]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4710.0
Loss: 0.05468955263495445
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.4031]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3805.0
Loss: 0.04101855680346489
Action 0 - predicted reward: tensor([[-0.0280]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.5482]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3345.0
Loss: 0.014647065661847591
Action 0 - predicted reward: tensor([[0.3216]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0513]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3225.0
Loss: 0.013857669197022915
Action 0 - predicted reward: tensor([[-0.0068]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6073]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5140.0
Loss: 0.06225284934043884
Greedy
Action 0 - predicted reward: tensor([[0.0475]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.8341]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3370.0
Loss: 0.02802538126707077
Action 0 - predicted reward: tensor([[0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7770]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8645.0
Loss: 0.00040482633630745113
Action 0 - predicted reward: tensor([[0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.7713]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3930.0
Loss: 0.070844367146492
Action 0 - predicted reward: tensor([[0.0752]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.4055]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4520.0
Loss: 0.03881816938519478
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9020]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7973]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5565.0
Loss: 24061.451171875
KL Divergence: 23.63063621520996
Action 0 - predicted reward: tensor([[0.8997]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9781]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5680.0
Loss: 22418.458984375
KL Divergence: 23.64321517944336
Action 0 - predicted reward: tensor([[1.0186]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9025]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5540.0
Loss: 16519.83984375
KL Divergence: 23.691640853881836
Action 0 - predicted reward: tensor([[1.0604]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8827]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5180.0
Loss: 21140.9609375
KL Divergence: 23.632509231567383
3399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3793]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7506]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4575.0
Loss: 0.014237431809306145
Action 0 - predicted reward: tensor([[-0.3661]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7625]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5140.0
Loss: 0.05207565799355507
Action 0 - predicted reward: tensor([[0.0610]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5868]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4855.0
Loss: 0.0513782761991024
Action 0 - predicted reward: tensor([[0.1629]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6959]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4865.0
Loss: 0.06622782349586487
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2426]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8864]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3875.0
Loss: 0.04742402955889702
Action 0 - predicted reward: tensor([[0.1494]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.1501]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3460.0
Loss: 0.023832378908991814
Action 0 - predicted reward: tensor([[-0.4085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8682]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3345.0
Loss: 0.020937275141477585
Action 0 - predicted reward: tensor([[-0.3978]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7897]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5220.0
Loss: 0.0582573264837265
Greedy
Action 0 - predicted reward: tensor([[0.0511]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0271]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3510.0
Loss: 0.04458010196685791
Action 0 - predicted reward: tensor([[0.0013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.1389]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8920.0
Loss: 0.0002964772575069219
Action 0 - predicted reward: tensor([[0.2456]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.7659]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4005.0
Loss: 0.07990474998950958
Action 0 - predicted reward: tensor([[0.0748]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1950]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4660.0
Loss: 0.05573125556111336
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9632]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0354]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5565.0
Loss: 23584.916015625
KL Divergence: 22.922616958618164
Action 0 - predicted reward: tensor([[0.9615]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6536]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5695.0
Loss: 22116.3515625
KL Divergence: 22.915590286254883
Action 0 - predicted reward: tensor([[1.0989]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9133]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5575.0
Loss: 16609.58203125
KL Divergence: 22.964841842651367
Action 0 - predicted reward: tensor([[1.1250]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9924]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5190.0
Loss: 20749.3515625
KL Divergence: 22.913543701171875
3499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0247]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8132]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4685.0
Loss: 0.021240726113319397
Action 0 - predicted reward: tensor([[-0.1307]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.6978]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5220.0
Loss: 0.05375373736023903
Action 0 - predicted reward: tensor([[0.0476]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5675]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4930.0
Loss: 0.04420686885714531
Action 0 - predicted reward: tensor([[-0.0953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9598]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4940.0
Loss: 0.06532664597034454
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9177]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3950.0
Loss: 0.046166129410266876
Action 0 - predicted reward: tensor([[0.0241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0381]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3530.0
Loss: 0.019773056730628014
Action 0 - predicted reward: tensor([[-0.0311]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8428]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3490.0
Loss: 0.03189521282911301
Action 0 - predicted reward: tensor([[-0.1452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7369]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5220.0
Loss: 0.04237684980034828
Greedy
Action 0 - predicted reward: tensor([[0.4875]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9996]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3510.0
Loss: 0.03209489583969116
Action 0 - predicted reward: tensor([[0.2278]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.1436]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 9205.0
Loss: 0.00021928361093159765
Action 0 - predicted reward: tensor([[0.3882]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3538]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4055.0
Loss: 0.07135674357414246
Action 0 - predicted reward: tensor([[-0.0755]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1556]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4695.0
Loss: 0.0574684664607048
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0238]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0897]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5610.0
Loss: 23461.666015625
KL Divergence: 22.26373291015625
Action 0 - predicted reward: tensor([[1.0354]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1136]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5710.0
Loss: 21691.771484375
KL Divergence: 22.2561092376709
Action 0 - predicted reward: tensor([[1.0709]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1459]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5680.0
Loss: 17194.966796875
KL Divergence: 22.289600372314453
Action 0 - predicted reward: tensor([[1.1902]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2326]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5195.0
Loss: 20343.224609375
KL Divergence: 22.23799705505371
3599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0456]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0518]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4720.0
Loss: 0.012564466334879398
Action 0 - predicted reward: tensor([[0.4675]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5312]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5365.0
Loss: 0.054716356098651886
Action 0 - predicted reward: tensor([[-0.0484]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1690]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4965.0
Loss: 0.044013336300849915
Action 0 - predicted reward: tensor([[0.1117]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0203]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5160.0
Loss: 0.06850900501012802
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0975]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7634]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3950.0
Loss: 0.04154645651578903
Action 0 - predicted reward: tensor([[0.0354]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1087]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3640.0
Loss: 0.01613428257405758
Action 0 - predicted reward: tensor([[0.0065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.3297]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3535.0
Loss: 0.029065409675240517
Action 0 - predicted reward: tensor([[-0.0369]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.8483]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5230.0
Loss: 0.0343901589512825
Greedy
Action 0 - predicted reward: tensor([[-0.1295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.3717]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3545.0
Loss: 0.029080884531140327
Action 0 - predicted reward: tensor([[-0.0654]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.1577]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 9475.0
Loss: 0.00016051469719968736
Action 0 - predicted reward: tensor([[0.1240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.0702]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4130.0
Loss: 0.07181457430124283
Action 0 - predicted reward: tensor([[-0.2027]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7247]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4800.0
Loss: 0.05961564555764198
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0049]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0691]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5610.0
Loss: 22772.513671875
KL Divergence: 21.612802505493164
Action 0 - predicted reward: tensor([[1.0541]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8986]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5715.0
Loss: 21296.560546875
KL Divergence: 21.629776000976562
Action 0 - predicted reward: tensor([[1.0830]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1545]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5725.0
Loss: 17295.083984375
KL Divergence: 21.62892723083496
Action 0 - predicted reward: tensor([[1.2400]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3197]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5235.0
Loss: 20309.826171875
KL Divergence: 21.5999755859375
3699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0058]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.6097]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4805.0
Loss: 0.011339482851326466
Action 0 - predicted reward: tensor([[0.5219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.8925]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5550.0
Loss: 0.0695922002196312
Action 0 - predicted reward: tensor([[0.0256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8837]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5090.0
Loss: 0.05321130529046059
Action 0 - predicted reward: tensor([[0.1774]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.3638]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5240.0
Loss: 0.06483932584524155
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6331]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4055.0
Loss: 0.04745032638311386
Action 0 - predicted reward: tensor([[0.0250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.0757]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3710.0
Loss: 0.021137293428182602
Action 0 - predicted reward: tensor([[-0.0156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.6897]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3610.0
Loss: 0.028633398935198784
Action 0 - predicted reward: tensor([[-0.3806]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8018]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5340.0
Loss: 0.04742908477783203
Greedy
Action 0 - predicted reward: tensor([[0.0464]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8624]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3580.0
Loss: 0.0175174530595541
Action 0 - predicted reward: tensor([[0.0219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.6650]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 9760.0
Loss: 0.00011715956497937441
Action 0 - predicted reward: tensor([[-0.0434]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9997]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4200.0
Loss: 0.07111720740795135
Action 0 - predicted reward: tensor([[-0.0083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.4680]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4835.0
Loss: 0.05858168005943298
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0396]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8765]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5645.0
Loss: 22712.892578125
KL Divergence: 21.012157440185547
Action 0 - predicted reward: tensor([[1.0586]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0003]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5775.0
Loss: 21138.138671875
KL Divergence: 21.023469924926758
Action 0 - predicted reward: tensor([[1.1205]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1919]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5785.0
Loss: 17288.76171875
KL Divergence: 21.035663604736328
Action 0 - predicted reward: tensor([[1.2599]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3320]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5240.0
Loss: 19958.826171875
KL Divergence: 20.98792839050293
3799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8791]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4855.0
Loss: 0.011672121472656727
Action 0 - predicted reward: tensor([[-1.4041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2070]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5655.0
Loss: 0.06881297379732132
Action 0 - predicted reward: tensor([[0.3273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.4603]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5170.0
Loss: 0.05521661043167114
Action 0 - predicted reward: tensor([[0.0156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1352]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5395.0
Loss: 0.07200022786855698
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1200]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7047]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4125.0
Loss: 0.05227091163396835
Action 0 - predicted reward: tensor([[-0.0217]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.7560]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3710.0
Loss: 0.01772475056350231
Action 0 - predicted reward: tensor([[0.0915]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9334]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3625.0
Loss: 0.02681918442249298
Action 0 - predicted reward: tensor([[-0.8500]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8015]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5415.0
Loss: 0.04453860968351364
Greedy
Action 0 - predicted reward: tensor([[0.1789]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9387]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3655.0
Loss: 0.02037273533642292
Action 0 - predicted reward: tensor([[-0.0278]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7685]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10045.0
Loss: 8.862567483447492e-05
Action 0 - predicted reward: tensor([[-0.0294]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3627]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4340.0
Loss: 0.07725762575864792
Action 0 - predicted reward: tensor([[0.0588]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7621]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4945.0
Loss: 0.06750734150409698
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.2004]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2775]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5645.0
Loss: 22484.611328125
KL Divergence: 20.454957962036133
Action 0 - predicted reward: tensor([[1.0794]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1428]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5820.0
Loss: 21124.677734375
KL Divergence: 20.448911666870117
Action 0 - predicted reward: tensor([[1.1326]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2064]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5795.0
Loss: 16886.044921875
KL Divergence: 20.450223922729492
Action 0 - predicted reward: tensor([[1.2566]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9932]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5275.0
Loss: 19535.388671875
KL Divergence: 20.413619995117188
3899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3120]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7239]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4935.0
Loss: 0.016746100038290024
Action 0 - predicted reward: tensor([[-0.0165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4967]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5775.0
Loss: 0.0636337623000145
Action 0 - predicted reward: tensor([[0.0199]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.4659]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.04429401084780693
Action 0 - predicted reward: tensor([[-0.0209]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6620]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5575.0
Loss: 0.07902374863624573
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.9591]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4130.0
Loss: 0.044444359838962555
Action 0 - predicted reward: tensor([[-0.0842]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.3580]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3745.0
Loss: 0.01625215634703636
Action 0 - predicted reward: tensor([[0.3041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5930]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3770.0
Loss: 0.028602514415979385
Action 0 - predicted reward: tensor([[-0.0775]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8895]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5525.0
Loss: 0.052740298211574554
Greedy
Action 0 - predicted reward: tensor([[0.0132]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2519]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3705.0
Loss: 0.015781406313180923
Action 0 - predicted reward: tensor([[0.0250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.4039]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10270.0
Loss: 6.830778875155374e-05
Action 0 - predicted reward: tensor([[0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0364]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4340.0
Loss: 0.07113969326019287
Action 0 - predicted reward: tensor([[-0.1108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.8840]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5020.0
Loss: 0.07048901915550232
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.1952]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2802]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5680.0
Loss: 22383.59765625
KL Divergence: 19.929096221923828
Action 0 - predicted reward: tensor([[1.0875]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8972]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5895.0
Loss: 21314.30078125
KL Divergence: 19.91686248779297
Action 0 - predicted reward: tensor([[1.1689]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1689]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5835.0
Loss: 17062.94921875
KL Divergence: 19.908267974853516
Action 0 - predicted reward: tensor([[1.2907]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3580]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5280.0
Loss: 19571.89453125
KL Divergence: 19.86627197265625
3999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3674]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1867]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4970.0
Loss: 0.013972949236631393
Action 0 - predicted reward: tensor([[-0.0134]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2674]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5955.0
Loss: 0.07189621776342392
Action 0 - predicted reward: tensor([[-0.1530]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2089]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5250.0
Loss: 0.051092296838760376
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5760.0
Loss: 0.07085660099983215
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0637]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.7561]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4170.0
Loss: 0.04680238291621208
Action 0 - predicted reward: tensor([[0.0127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5907]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3785.0
Loss: 0.015292857773602009
Action 0 - predicted reward: tensor([[-0.2946]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.4942]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3850.0
Loss: 0.031640030443668365
Action 0 - predicted reward: tensor([[-0.0241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.3564]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5600.0
Loss: 0.0466444231569767
Greedy
Action 0 - predicted reward: tensor([[-0.1582]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1073]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3780.0
Loss: 0.014784262515604496
Action 0 - predicted reward: tensor([[-0.0137]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.9407]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 10530.0
Loss: 9.878190758172423e-05
Action 0 - predicted reward: tensor([[-0.0252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9648]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4375.0
Loss: 0.07440214604139328
Action 0 - predicted reward: tensor([[0.1994]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9555]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5125.0
Loss: 0.07017499208450317
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.2326]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2982]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5690.0
Loss: 22010.93359375
KL Divergence: 19.39802360534668
Action 0 - predicted reward: tensor([[1.1090]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8193]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5895.0
Loss: 20651.6796875
KL Divergence: 19.395357131958008
Action 0 - predicted reward: tensor([[1.1455]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2167]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5875.0
Loss: 17106.212890625
KL Divergence: 19.378742218017578
Action 0 - predicted reward: tensor([[1.2522]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3143]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5285.0
Loss: 19206.33984375
KL Divergence: 19.341930389404297
4099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0228]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5050.0
Loss: 0.02053161896765232
Action 0 - predicted reward: tensor([[-0.3781]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2554]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6070.0
Loss: 0.07163801044225693
Action 0 - predicted reward: tensor([[0.0965]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0808]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5355.0
Loss: 0.060455188155174255
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5945.0
Loss: 0.08790649473667145
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1920]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7077]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4205.0
Loss: 0.05143473297357559
Action 0 - predicted reward: tensor([[-0.0650]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9170]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3825.0
Loss: 0.014385603368282318
Action 0 - predicted reward: tensor([[0.0426]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.5692]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3955.0
Loss: 0.0321979746222496
Action 0 - predicted reward: tensor([[-0.2439]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.2343]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5725.0
Loss: 0.04655982181429863
Greedy
Action 0 - predicted reward: tensor([[-0.0486]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.6968]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3885.0
Loss: 0.021367628127336502
Action 0 - predicted reward: tensor([[-0.2407]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.1972]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10825.0
Loss: 0.0019621080718934536
Action 0 - predicted reward: tensor([[-0.0440]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1515]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4590.0
Loss: 0.08611324429512024
Action 0 - predicted reward: tensor([[-0.0897]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.9614]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5195.0
Loss: 0.06963272392749786
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.2343]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3102]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5725.0
Loss: 21685.33984375
KL Divergence: 18.917980194091797
Action 0 - predicted reward: tensor([[1.1461]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2035]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5910.0
Loss: 20615.083984375
KL Divergence: 18.9471435546875
Action 0 - predicted reward: tensor([[1.2484]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9186]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5890.0
Loss: 16351.1435546875
KL Divergence: 18.91656494140625
Action 0 - predicted reward: tensor([[1.3410]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9519]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5285.0
Loss: 18946.087890625
KL Divergence: 18.891328811645508
4199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2478]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4947]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5135.0
Loss: 0.01893709972500801
Action 0 - predicted reward: tensor([[-0.1023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5047]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6145.0
Loss: 0.05454196780920029
Action 0 - predicted reward: tensor([[0.0730]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9594]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5430.0
Loss: 0.0603497251868248
Action 0 - predicted reward: tensor([[0.0651]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.4744]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5990.0
Loss: 0.07625335454940796
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0187]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4240.0
Loss: 0.038479167968034744
Action 0 - predicted reward: tensor([[0.0489]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.2530]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3825.0
Loss: 0.014267407357692719
Action 0 - predicted reward: tensor([[0.2538]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0133]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3990.0
Loss: 0.030217744410037994
Action 0 - predicted reward: tensor([[-0.1828]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.0872]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5800.0
Loss: 0.04531221091747284
Greedy
Action 0 - predicted reward: tensor([[0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.5976]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3885.0
Loss: 0.020010096952319145
Action 0 - predicted reward: tensor([[0.2429]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.6720]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11080.0
Loss: 0.0011720681795850396
Action 0 - predicted reward: tensor([[0.2047]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8957]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4625.0
Loss: 0.06663786619901657
Action 0 - predicted reward: tensor([[0.0509]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9584]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5195.0
Loss: 0.054034847766160965
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.2764]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9565]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5765.0
Loss: 20650.646484375
KL Divergence: 18.91451644897461
Action 0 - predicted reward: tensor([[1.2379]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3052]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5915.0
Loss: 18177.099609375
KL Divergence: 18.93635368347168
Action 0 - predicted reward: tensor([[1.3073]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3789]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5965.0
Loss: 15840.2666015625
KL Divergence: 18.911775588989258
Action 0 - predicted reward: tensor([[1.4128]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0973]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5330.0
Loss: 17601.451171875
KL Divergence: 18.870920181274414
4299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.1277]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6948]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5220.0
Loss: 0.017439870163798332
Action 0 - predicted reward: tensor([[0.6438]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7378]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6150.0
Loss: 0.046229902654886246
Action 0 - predicted reward: tensor([[0.0089]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.0726]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5490.0
Loss: 0.05974632129073143
Action 0 - predicted reward: tensor([[0.1824]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2512]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6130.0
Loss: 0.08340601623058319
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1956]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.4157]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4280.0
Loss: 0.03938348963856697
Action 0 - predicted reward: tensor([[0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.4508]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3825.0
Loss: 0.014165224507451057
Action 0 - predicted reward: tensor([[-0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9838]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4025.0
Loss: 0.031563546508550644
Action 0 - predicted reward: tensor([[-0.0892]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7792]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5875.0
Loss: 0.043265700340270996
Greedy
Action 0 - predicted reward: tensor([[-0.1219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.1967]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3955.0
Loss: 0.023680876940488815
Action 0 - predicted reward: tensor([[-0.2020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7880]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11360.0
Loss: 0.0010084558743983507
Action 0 - predicted reward: tensor([[-0.0765]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9149]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4730.0
Loss: 0.06835383176803589
Action 0 - predicted reward: tensor([[0.0956]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.7510]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5230.0
Loss: 0.05681655928492546
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.3577]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9781]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5800.0
Loss: 20495.24609375
KL Divergence: 18.915678024291992
Action 0 - predicted reward: tensor([[1.2981]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3748]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5920.0
Loss: 17422.9140625
KL Divergence: 18.927318572998047
Action 0 - predicted reward: tensor([[1.3341]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4075]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5980.0
Loss: 15682.083984375
KL Divergence: 18.891117095947266
Action 0 - predicted reward: tensor([[1.4972]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2314]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5365.0
Loss: 17984.123046875
KL Divergence: 18.8615665435791
4399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3888]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2935]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5375.0
Loss: 0.019578712061047554
Action 0 - predicted reward: tensor([[-0.1049]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.9062]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6290.0
Loss: 0.05384182184934616
Action 0 - predicted reward: tensor([[0.0492]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1242]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5560.0
Loss: 0.05879019945859909
Action 0 - predicted reward: tensor([[-0.0714]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.1134]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6175.0
Loss: 0.08301792293787003
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2297]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1188]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4350.0
Loss: 0.04216098412871361
Action 0 - predicted reward: tensor([[0.0099]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.7027]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3860.0
Loss: 0.014138453640043736
Action 0 - predicted reward: tensor([[-0.0077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.4475]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4135.0
Loss: 0.03470337763428688
Action 0 - predicted reward: tensor([[-0.0127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.9720]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5950.0
Loss: 0.043235406279563904
Greedy
Action 0 - predicted reward: tensor([[-0.1598]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8100]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4035.0
Loss: 0.024465588852763176
Action 0 - predicted reward: tensor([[-0.0299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.8074]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 11610.0
Loss: 0.00033368857111781836
Action 0 - predicted reward: tensor([[-0.0477]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.2195]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4905.0
Loss: 0.07924436032772064
Action 0 - predicted reward: tensor([[-0.1396]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2025]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5305.0
Loss: 0.053193386644124985
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4043]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4717]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5835.0
Loss: 20860.853515625
KL Divergence: 18.900279998779297
Action 0 - predicted reward: tensor([[1.4453]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5345]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5925.0
Loss: 16384.837890625
KL Divergence: 18.92447280883789
Action 0 - predicted reward: tensor([[1.4191]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0426]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6020.0
Loss: 15776.7568359375
KL Divergence: 18.870813369750977
Action 0 - predicted reward: tensor([[1.5310]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4625]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5375.0
Loss: 17802.212890625
KL Divergence: 18.84677505493164
4499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2682]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7605]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5450.0
Loss: 0.0195608027279377
Action 0 - predicted reward: tensor([[-0.9148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.7651]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6335.0
Loss: 0.051471006125211716
Action 0 - predicted reward: tensor([[-0.0743]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0589]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5635.0
Loss: 0.06108821555972099
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6320.0
Loss: 0.08030042052268982
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.6010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7594]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4385.0
Loss: 0.03746521472930908
Action 0 - predicted reward: tensor([[0.0809]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9661]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3930.0
Loss: 0.017449161037802696
Action 0 - predicted reward: tensor([[0.0454]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.7248]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4180.0
Loss: 0.032963525503873825
Action 0 - predicted reward: tensor([[-0.0547]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7771]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6030.0
Loss: 0.04372837394475937
Greedy
Action 0 - predicted reward: tensor([[0.1084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.3502]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4140.0
Loss: 0.031401194632053375
Action 0 - predicted reward: tensor([[-0.1123]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.2614]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11830.0
Loss: 0.001037553884088993
Action 0 - predicted reward: tensor([[-0.0029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0588]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4940.0
Loss: 0.06514475494623184
Action 0 - predicted reward: tensor([[-0.1236]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.0004]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5340.0
Loss: 0.05347326770424843
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4515]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8713]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5940.0
Loss: 20959.369140625
KL Divergence: 18.887338638305664
Action 0 - predicted reward: tensor([[1.5411]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6176]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5970.0
Loss: 14204.0029296875
KL Divergence: 18.91617774963379
Action 0 - predicted reward: tensor([[1.4524]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1458]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6035.0
Loss: 15582.638671875
KL Divergence: 18.86773109436035
Action 0 - predicted reward: tensor([[1.6062]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6906]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5445.0
Loss: 18472.337890625
KL Divergence: 18.819564819335938
4599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1654]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4257]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5485.0
Loss: 0.018899597227573395
Action 0 - predicted reward: tensor([[0.1822]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.5047]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6455.0
Loss: 0.05596577003598213
Action 0 - predicted reward: tensor([[-0.1092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0595]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5715.0
Loss: 0.06780485063791275
Action 0 - predicted reward: tensor([[0.0119]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.3829]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6545.0
Loss: 0.08576774597167969
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2182]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1207]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4490.0
Loss: 0.03348257765173912
Action 0 - predicted reward: tensor([[-0.1880]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1063]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3970.0
Loss: 0.021494856104254723
Action 0 - predicted reward: tensor([[-0.0228]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.3319]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4220.0
Loss: 0.0323481410741806
Action 0 - predicted reward: tensor([[0.0624]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0561]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6075.0
Loss: 0.045825447887182236
Greedy
Action 0 - predicted reward: tensor([[0.0801]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.8007]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4140.0
Loss: 0.029203956946730614
Action 0 - predicted reward: tensor([[-0.0726]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.2098]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 12110.0
Loss: 7.033954898361117e-05
Action 0 - predicted reward: tensor([[0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0670]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4975.0
Loss: 0.06304705888032913
Action 0 - predicted reward: tensor([[-0.1455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6836]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5375.0
Loss: 0.053301721811294556
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4846]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1934]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6010.0
Loss: 21008.67578125
KL Divergence: 18.884634017944336
Action 0 - predicted reward: tensor([[1.6023]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5227]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6010.0
Loss: 13128.345703125
KL Divergence: 18.90060043334961
Action 0 - predicted reward: tensor([[1.5342]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1498]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6075.0
Loss: 14499.111328125
KL Divergence: 18.85883903503418
Action 0 - predicted reward: tensor([[1.6267]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2887]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5515.0
Loss: 19137.701171875
KL Divergence: 18.812023162841797
4699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0858]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.5458]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5625.0
Loss: 0.027516232803463936
Action 0 - predicted reward: tensor([[-0.2369]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.9599]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6670.0
Loss: 0.06320347636938095
Action 0 - predicted reward: tensor([[-0.0717]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.6294]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5750.0
Loss: 0.06964058429002762
Action 0 - predicted reward: tensor([[0.5008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2009]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6735.0
Loss: 0.07740478962659836
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0627]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.1178]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4500.0
Loss: 0.03136616200208664
Action 0 - predicted reward: tensor([[-0.0991]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4040.0
Loss: 0.03189955651760101
Action 0 - predicted reward: tensor([[-0.0449]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9064]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4230.0
Loss: 0.029273340478539467
Action 0 - predicted reward: tensor([[0.0459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2352]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6075.0
Loss: 0.04155263677239418
Greedy
Action 0 - predicted reward: tensor([[0.2773]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3058]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4210.0
Loss: 0.03194992616772652
Action 0 - predicted reward: tensor([[0.4312]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2592]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12335.0
Loss: 8.186806371668354e-05
Action 0 - predicted reward: tensor([[0.0990]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9359]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5010.0
Loss: 0.06156619265675545
Action 0 - predicted reward: tensor([[0.1964]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0995]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5375.0
Loss: 0.05214333534240723
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.5797]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1843]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6080.0
Loss: 20496.890625
KL Divergence: 18.88534927368164
Action 0 - predicted reward: tensor([[1.6934]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4422]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6050.0
Loss: 11773.1611328125
KL Divergence: 18.89606285095215
Action 0 - predicted reward: tensor([[1.5791]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9407]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6120.0
Loss: 13419.9599609375
KL Divergence: 18.846832275390625
Action 0 - predicted reward: tensor([[1.6880]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3639]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5515.0
Loss: 19229.642578125
KL Divergence: 18.80392837524414
4799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.6344]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9576]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5670.0
Loss: 0.02013203129172325
Action 0 - predicted reward: tensor([[0.3589]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5374]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6785.0
Loss: 0.06558719277381897
Action 0 - predicted reward: tensor([[0.0081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.5309]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5820.0
Loss: 0.06764953583478928
Action 0 - predicted reward: tensor([[-0.0572]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1091]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6810.0
Loss: 0.0720554068684578
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.5237]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4535.0
Loss: 0.03426786884665489
Action 0 - predicted reward: tensor([[0.0915]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.3732]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4080.0
Loss: 0.028878889977931976
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8706]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4265.0
Loss: 0.02537897601723671
Action 0 - predicted reward: tensor([[-0.0243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7314]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6110.0
Loss: 0.04060966894030571
Greedy
Action 0 - predicted reward: tensor([[0.1273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.0065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4315.0
Loss: 0.03732886537909508
Action 0 - predicted reward: tensor([[0.0147]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8327]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12575.0
Loss: 8.425299165537581e-05
Action 0 - predicted reward: tensor([[-0.1109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5980]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 5080.0
Loss: 0.06628232449293137
Action 0 - predicted reward: tensor([[-0.1032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0021]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5380.0
Loss: 0.045068200677633286
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.5934]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6621]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6080.0
Loss: 19684.2890625
KL Divergence: 18.871034622192383
Action 0 - predicted reward: tensor([[1.7871]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8626]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6050.0
Loss: 10682.009765625
KL Divergence: 18.887596130371094
Action 0 - predicted reward: tensor([[1.6427]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7115]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6120.0
Loss: 13196.580078125
KL Divergence: 18.832042694091797
Action 0 - predicted reward: tensor([[1.7487]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8084]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5550.0
Loss: 18423.962890625
KL Divergence: 18.800737380981445
4899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0887]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9830]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5715.0
Loss: 0.01940397173166275
Action 0 - predicted reward: tensor([[0.6620]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3865]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6865.0
Loss: 0.058744240552186966
Action 0 - predicted reward: tensor([[-0.0399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.8994]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5865.0
Loss: 0.06607980281114578
Action 0 - predicted reward: tensor([[-0.1655]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.0710]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6995.0
Loss: 0.08266223222017288
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0492]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7746]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4535.0
Loss: 0.028148848563432693
Action 0 - predicted reward: tensor([[-0.2741]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9425]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4085.0
Loss: 0.026866856962442398
Action 0 - predicted reward: tensor([[0.0630]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.0087]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4335.0
Loss: 0.02524215541779995
Action 0 - predicted reward: tensor([[-0.0174]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0007]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6110.0
Loss: 0.03908497840166092
Greedy
Action 0 - predicted reward: tensor([[-0.0075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.2259]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4350.0
Loss: 0.02910371869802475
Action 0 - predicted reward: tensor([[0.0705]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.0079]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 12855.0
Loss: 5.710541154257953e-05
Action 0 - predicted reward: tensor([[0.0181]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.0364]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5185.0
Loss: 0.0622933954000473
Action 0 - predicted reward: tensor([[0.2519]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9630]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5450.0
Loss: 0.04811517521739006
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.6344]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4676]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6120.0
Loss: 18032.029296875
KL Divergence: 18.85820198059082
Action 0 - predicted reward: tensor([[1.8346]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9162]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6055.0
Loss: 10741.1337890625
KL Divergence: 18.884258270263672
Action 0 - predicted reward: tensor([[1.6954]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7735]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6160.0
Loss: 12689.236328125
KL Divergence: 18.823108673095703
Action 0 - predicted reward: tensor([[1.7970]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2810]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5585.0
Loss: 18779.73828125
KL Divergence: 18.793668746948242
4999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4333]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1795]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5835.0
Loss: 0.0223538838326931
Action 0 - predicted reward: tensor([[0.3526]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.9033]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6975.0
Loss: 0.06271380931138992
Action 0 - predicted reward: tensor([[-0.1608]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.6858]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6020.0
Loss: 0.06980021297931671
Action 0 - predicted reward: tensor([[-0.0486]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.5700]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7105.0
Loss: 0.07934335619211197
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0234]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9099]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4570.0
Loss: 0.020475873723626137
Action 0 - predicted reward: tensor([[-0.0633]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6238]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4125.0
Loss: 0.02741249091923237
Action 0 - predicted reward: tensor([[0.1011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8912]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4475.0
Loss: 0.023746307939291
Action 0 - predicted reward: tensor([[0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2510]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 0.04848916083574295
Greedy
Action 0 - predicted reward: tensor([[0.1304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6886]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4425.0
Loss: 0.03435254096984863
Action 0 - predicted reward: tensor([[0.0310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.9340]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 13100.0
Loss: 2.8228792871232145e-05
Action 0 - predicted reward: tensor([[0.0330]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.5263]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5220.0
Loss: 0.06515367329120636
Action 0 - predicted reward: tensor([[0.1049]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7888]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5450.0
Loss: 0.03828419744968414
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7742]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8455]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6125.0
Loss: 16642.943359375
KL Divergence: 18.857666015625
Action 0 - predicted reward: tensor([[1.8696]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9406]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6070.0
Loss: 10500.287109375
KL Divergence: 18.870389938354492
Action 0 - predicted reward: tensor([[1.7512]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1793]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6175.0
Loss: 12728.7265625
KL Divergence: 18.808950424194336
Action 0 - predicted reward: tensor([[1.8532]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9181]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5655.0
Loss: 18237.595703125
KL Divergence: 18.78960609436035
5099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.7213]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5915.0
Loss: 0.02189599722623825
Action 0 - predicted reward: tensor([[-0.4193]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.9552]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7095.0
Loss: 0.06038900837302208
Action 0 - predicted reward: tensor([[-0.0441]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7304]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6095.0
Loss: 0.07305939495563507
Action 0 - predicted reward: tensor([[0.0770]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1099]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7225.0
Loss: 0.08157197386026382
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0431]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-75.5483]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4750.0
Loss: 0.028981419280171394
Action 0 - predicted reward: tensor([[-0.2203]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9558]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4140.0
Loss: 0.02529028058052063
Action 0 - predicted reward: tensor([[-0.5709]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7632]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4655.0
Loss: 0.03948439657688141
Action 0 - predicted reward: tensor([[-0.2371]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4024]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6300.0
Loss: 0.057130809873342514
Greedy
Action 0 - predicted reward: tensor([[-0.0358]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.5434]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4470.0
Loss: 0.033958133310079575
Action 0 - predicted reward: tensor([[0.0349]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.1732]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 13435.0
Loss: 0.0001425313821528107
Action 0 - predicted reward: tensor([[0.1040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.2826]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5325.0
Loss: 0.05918510630726814
Action 0 - predicted reward: tensor([[-0.0552]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7740]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5455.0
Loss: 0.03762218728661537
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.8287]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4996]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6125.0
Loss: 15544.7021484375
KL Divergence: 18.848207473754883
Action 0 - predicted reward: tensor([[1.8990]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6910]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6075.0
Loss: 10257.1279296875
KL Divergence: 18.856124877929688
Action 0 - predicted reward: tensor([[1.7724]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8524]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6180.0
Loss: 12482.072265625
KL Divergence: 18.798124313354492
Action 0 - predicted reward: tensor([[1.9186]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8821]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5695.0
Loss: 18005.70703125
KL Divergence: 18.782800674438477
5199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3575]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1268]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6030.0
Loss: 0.025048287585377693
Action 0 - predicted reward: tensor([[0.6783]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6659]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7135.0
Loss: 0.055090565234422684
Action 0 - predicted reward: tensor([[-0.0826]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5919]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6185.0
Loss: 0.07505658268928528
Action 0 - predicted reward: tensor([[0.0673]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0649]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7380.0
Loss: 0.08465226739645004
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2786]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0087]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4785.0
Loss: 0.023589735850691795
Action 0 - predicted reward: tensor([[-0.0240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.6549]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4140.0
Loss: 0.025830306112766266
Action 0 - predicted reward: tensor([[0.0991]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.6750]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4760.0
Loss: 0.03434527665376663
Action 0 - predicted reward: tensor([[-0.0474]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7585]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6350.0
Loss: 0.0475989505648613
Greedy
Action 0 - predicted reward: tensor([[-0.0064]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.6080]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4470.0
Loss: 0.031386468559503555
Action 0 - predicted reward: tensor([[0.3020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8672]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13670.0
Loss: 2.3964748834259808e-05
Action 0 - predicted reward: tensor([[-0.0795]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.2065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5430.0
Loss: 0.06357467919588089
Action 0 - predicted reward: tensor([[-0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.6460]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5460.0
Loss: 0.03758278489112854
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.8944]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9668]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6125.0
Loss: 15288.12890625
KL Divergence: 18.83596420288086
Action 0 - predicted reward: tensor([[1.9521]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3610]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6120.0
Loss: 10292.2021484375
KL Divergence: 18.858684539794922
Action 0 - predicted reward: tensor([[1.8377]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9066]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6190.0
Loss: 12246.7041015625
KL Divergence: 18.79050064086914
Action 0 - predicted reward: tensor([[2.0114]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0852]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5695.0
Loss: 17744.478515625
KL Divergence: 18.77284049987793
5299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3696]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0276]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6110.0
Loss: 0.01915869489312172
Action 0 - predicted reward: tensor([[0.2420]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6418]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7185.0
Loss: 0.05507616326212883
Action 0 - predicted reward: tensor([[0.0240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.9236]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6270.0
Loss: 0.06752230226993561
Action 0 - predicted reward: tensor([[-0.0241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0091]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7425.0
Loss: 0.07968132197856903
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4411]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0377]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4855.0
Loss: 0.019262701272964478
Action 0 - predicted reward: tensor([[0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4325]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4210.0
Loss: 0.02633407711982727
Action 0 - predicted reward: tensor([[-0.0494]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8135]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4830.0
Loss: 0.035414211452007294
Action 0 - predicted reward: tensor([[-0.0400]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1512]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.05119650810956955
Greedy
Action 0 - predicted reward: tensor([[0.1357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4927]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4470.0
Loss: 0.02731764130294323
Action 0 - predicted reward: tensor([[-0.0091]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.6918]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 13965.0
Loss: 3.7620062357746065e-05
Action 0 - predicted reward: tensor([[0.2989]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9758]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5500.0
Loss: 0.057998981326818466
Action 0 - predicted reward: tensor([[0.1767]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0075]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5535.0
Loss: 0.041913341730833054
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.9728]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6049]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6125.0
Loss: 14469.2109375
KL Divergence: 18.834083557128906
Action 0 - predicted reward: tensor([[1.9781]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6871]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6125.0
Loss: 10013.0283203125
KL Divergence: 18.844890594482422
Action 0 - predicted reward: tensor([[1.8800]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3538]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6200.0
Loss: 11991.62109375
KL Divergence: 18.790468215942383
Action 0 - predicted reward: tensor([[1.9762]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8489]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5695.0
Loss: 16617.25
KL Divergence: 18.771268844604492
5399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.0369]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6180.0
Loss: 0.0170601774007082
Action 0 - predicted reward: tensor([[-0.5663]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9826]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7280.0
Loss: 0.06573158502578735
Action 0 - predicted reward: tensor([[-0.2447]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9129]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.06475354731082916
Action 0 - predicted reward: tensor([[0.4413]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2760]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7545.0
Loss: 0.09421119838953018
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4205]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6430]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4960.0
Loss: 0.020787056535482407
Action 0 - predicted reward: tensor([[-0.1830]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0219]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4245.0
Loss: 0.022987984120845795
Action 0 - predicted reward: tensor([[0.6357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0595]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4865.0
Loss: 0.038364309817552567
Action 0 - predicted reward: tensor([[-0.6610]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9397]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.04651737958192825
Greedy
Action 0 - predicted reward: tensor([[0.0163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9140]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4510.0
Loss: 0.02518373355269432
Action 0 - predicted reward: tensor([[0.0193]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.3183]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14235.0
Loss: 2.5834524421952665e-05
Action 0 - predicted reward: tensor([[-0.1276]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.1030]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5500.0
Loss: 0.05640335753560066
Action 0 - predicted reward: tensor([[0.0827]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9512]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5580.0
Loss: 0.03565860912203789
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.0279]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4208]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6130.0
Loss: 13328.6435546875
KL Divergence: 18.822843551635742
Action 0 - predicted reward: tensor([[2.0011]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8863]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 10051.521484375
KL Divergence: 18.83209800720215
Action 0 - predicted reward: tensor([[1.9249]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9933]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6205.0
Loss: 11748.9482421875
KL Divergence: 18.778310775756836
Action 0 - predicted reward: tensor([[2.0817]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1471]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5730.0
Loss: 16046.7158203125
KL Divergence: 18.77103614807129
5499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0825]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6260.0
Loss: 0.02368999272584915
Action 0 - predicted reward: tensor([[-0.4445]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6282]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7355.0
Loss: 0.06700414419174194
Action 0 - predicted reward: tensor([[0.0408]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.8883]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6465.0
Loss: 0.05872095748782158
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7650.0
Loss: 0.09518473595380783
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0548]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.6977]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5035.0
Loss: 0.01967383548617363
Action 0 - predicted reward: tensor([[0.1687]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7505]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4260.0
Loss: 0.022105788812041283
Action 0 - predicted reward: tensor([[0.0855]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.4203]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4900.0
Loss: 0.03659186139702797
Action 0 - predicted reward: tensor([[0.0199]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.6416]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6455.0
Loss: 0.05019567161798477
Greedy
Action 0 - predicted reward: tensor([[0.5642]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9699]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4550.0
Loss: 0.028625667095184326
Action 0 - predicted reward: tensor([[0.0237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.4019]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 14485.0
Loss: 2.4303628379129805e-05
Action 0 - predicted reward: tensor([[-0.0521]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.6215]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5540.0
Loss: 0.057633716613054276
Action 0 - predicted reward: tensor([[0.1381]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8724]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5580.0
Loss: 0.03488248959183693
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.1012]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1728]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6135.0
Loss: 12778.986328125
KL Divergence: 18.82027244567871
Action 0 - predicted reward: tensor([[2.1014]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1766]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6150.0
Loss: 10079.3720703125
KL Divergence: 18.831554412841797
Action 0 - predicted reward: tensor([[1.9752]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0518]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6245.0
Loss: 12086.71484375
KL Divergence: 18.7781925201416
Action 0 - predicted reward: tensor([[2.1161]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5443]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5730.0
Loss: 15185.396484375
KL Divergence: 18.76303482055664
5599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2715]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0029]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6330.0
Loss: 0.02314263954758644
Action 0 - predicted reward: tensor([[0.0699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.6392]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7430.0
Loss: 0.06491195410490036
Action 0 - predicted reward: tensor([[-0.3493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4936]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6580.0
Loss: 0.0568917840719223
Action 0 - predicted reward: tensor([[0.5556]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7512]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7700.0
Loss: 0.09132096916437149
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2180]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0186]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5070.0
Loss: 0.018360646441578865
Action 0 - predicted reward: tensor([[-0.2905]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0469]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4260.0
Loss: 0.01844809018075466
Action 0 - predicted reward: tensor([[0.1067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.9721]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4940.0
Loss: 0.04027050733566284
Action 0 - predicted reward: tensor([[-0.0826]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.9975]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6595.0
Loss: 0.05024093762040138
Greedy
Action 0 - predicted reward: tensor([[0.0820]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7891]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4620.0
Loss: 0.03322850540280342
Action 0 - predicted reward: tensor([[0.0191]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.1015]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14745.0
Loss: 2.194489206885919e-05
Action 0 - predicted reward: tensor([[-0.0864]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7717]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5575.0
Loss: 0.056859128177165985
Action 0 - predicted reward: tensor([[0.0523]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.2174]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5580.0
Loss: 0.02843114361166954
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.1465]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2152]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6140.0
Loss: 12508.7607421875
KL Divergence: 18.818771362304688
Action 0 - predicted reward: tensor([[2.1234]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6389]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6175.0
Loss: 10104.6025390625
KL Divergence: 18.82352066040039
Action 0 - predicted reward: tensor([[1.9902]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0568]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6290.0
Loss: 11831.3974609375
KL Divergence: 18.76963233947754
Action 0 - predicted reward: tensor([[2.2208]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2983]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5730.0
Loss: 13732.7744140625
KL Divergence: 18.7542781829834
5699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.6142]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6370.0
Loss: 0.025549035519361496
Action 0 - predicted reward: tensor([[-0.9628]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0991]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 0.06533798575401306
Action 0 - predicted reward: tensor([[-0.1876]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8031]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6700.0
Loss: 0.05375109240412712
Action 0 - predicted reward: tensor([[0.1042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.2575]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7885.0
Loss: 0.08865144848823547
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1764]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5145.0
Loss: 0.015462376177310944
Action 0 - predicted reward: tensor([[-0.3395]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0507]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4270.0
Loss: 0.018757155165076256
Action 0 - predicted reward: tensor([[-0.0249]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.0713]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4975.0
Loss: 0.0426650270819664
Action 0 - predicted reward: tensor([[-0.2085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9015]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6735.0
Loss: 0.05792803317308426
Greedy
Action 0 - predicted reward: tensor([[0.0039]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.8696]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4690.0
Loss: 0.03034314326941967
Action 0 - predicted reward: tensor([[0.0109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.1393]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 14980.0
Loss: 1.6479112673550844e-05
Action 0 - predicted reward: tensor([[0.0693]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0528]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5575.0
Loss: 0.05609775707125664
Action 0 - predicted reward: tensor([[0.0721]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1239]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5580.0
Loss: 0.023720290511846542
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.1878]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2609]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6220.0
Loss: 12819.73828125
KL Divergence: 18.81512451171875
Action 0 - predicted reward: tensor([[2.1486]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2306]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6185.0
Loss: 10128.0712890625
KL Divergence: 18.823013305664062
Action 0 - predicted reward: tensor([[2.0637]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3900]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6305.0
Loss: 11868.740234375
KL Divergence: 18.76772117614746
Action 0 - predicted reward: tensor([[2.2435]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3147]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5730.0
Loss: 12854.08984375
KL Divergence: 18.744232177734375
5799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0928]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2043]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6450.0
Loss: 0.02633204311132431
Action 0 - predicted reward: tensor([[-0.0856]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.3720]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7575.0
Loss: 0.060279637575149536
Action 0 - predicted reward: tensor([[-0.2671]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.2735]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6820.0
Loss: 0.05416534096002579
Action 0 - predicted reward: tensor([[-0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.6668]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8060.0
Loss: 0.08336491882801056
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0248]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1154]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5180.0
Loss: 0.01898501068353653
Action 0 - predicted reward: tensor([[-0.0832]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.1793]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4305.0
Loss: 0.01559100579470396
Action 0 - predicted reward: tensor([[0.0264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.0402]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5045.0
Loss: 0.04318753629922867
Action 0 - predicted reward: tensor([[-0.1011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.6632]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6875.0
Loss: 0.07028938084840775
Greedy
Action 0 - predicted reward: tensor([[0.3282]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0160]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4690.0
Loss: 0.026560993865132332
Action 0 - predicted reward: tensor([[0.0436]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.9627]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 15265.0
Loss: 1.3466763448377606e-05
Action 0 - predicted reward: tensor([[-0.0130]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.5615]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5610.0
Loss: 0.05618724972009659
Action 0 - predicted reward: tensor([[-0.0011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1402]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5685.0
Loss: 0.02666955627501011
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.1758]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2473]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6255.0
Loss: 13131.939453125
KL Divergence: 18.795305252075195
Action 0 - predicted reward: tensor([[2.1632]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6462]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6185.0
Loss: 10142.2236328125
KL Divergence: 18.82317543029785
Action 0 - predicted reward: tensor([[2.1271]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7777]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6315.0
Loss: 11602.09375
KL Divergence: 18.763805389404297
Action 0 - predicted reward: tensor([[2.2891]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3560]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5730.0
Loss: 12275.23828125
KL Divergence: 18.733015060424805
5899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0589]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0695]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6595.0
Loss: 0.029960997402668
Action 0 - predicted reward: tensor([[0.5809]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8444]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7765.0
Loss: 0.06588871777057648
Action 0 - predicted reward: tensor([[0.1330]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2524]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6925.0
Loss: 0.05279941111803055
Action 0 - predicted reward: tensor([[0.0339]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1502]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8205.0
Loss: 0.07571935653686523
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.5582]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5180.0
Loss: 0.016626013442873955
Action 0 - predicted reward: tensor([[-0.0421]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.0511]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4345.0
Loss: 0.014662864618003368
Action 0 - predicted reward: tensor([[0.1175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.5017]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5115.0
Loss: 0.0390469953417778
Action 0 - predicted reward: tensor([[-0.0268]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0228]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7020.0
Loss: 0.07412955164909363
Greedy
Action 0 - predicted reward: tensor([[-0.0289]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.0807]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4690.0
Loss: 0.025871075689792633
Action 0 - predicted reward: tensor([[-0.0125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.7493]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 15500.0
Loss: 8.575371612096205e-05
Action 0 - predicted reward: tensor([[-0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.8527]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5610.0
Loss: 0.04670766368508339
Action 0 - predicted reward: tensor([[0.0530]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.4558]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5720.0
Loss: 0.02101563848555088
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2405]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3168]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6255.0
Loss: 13147.1953125
KL Divergence: 18.800626754760742
Action 0 - predicted reward: tensor([[2.2540]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0615]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6200.0
Loss: 9871.9755859375
KL Divergence: 18.81203842163086
Action 0 - predicted reward: tensor([[2.1864]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2505]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6315.0
Loss: 11619.5947265625
KL Divergence: 18.765243530273438
Action 0 - predicted reward: tensor([[2.3088]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7797]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5730.0
Loss: 12275.904296875
KL Divergence: 18.720232009887695
5999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1773]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.7622]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6705.0
Loss: 0.035534556955099106
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7810.0
Loss: 0.06426361203193665
Action 0 - predicted reward: tensor([[-0.0577]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1898]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7010.0
Loss: 0.05289948731660843
Action 0 - predicted reward: tensor([[0.0820]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.3189]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8295.0
Loss: 0.07394944876432419
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0317]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2894]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5215.0
Loss: 0.015754660591483116
Action 0 - predicted reward: tensor([[-0.2110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1735]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4420.0
Loss: 0.017815236002206802
Action 0 - predicted reward: tensor([[0.6772]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8431]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5120.0
Loss: 0.03718866780400276
Action 0 - predicted reward: tensor([[-0.0890]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.9952]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7090.0
Loss: 0.06298404932022095
Greedy
Action 0 - predicted reward: tensor([[0.2017]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.5742]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4760.0
Loss: 0.024838542565703392
Action 0 - predicted reward: tensor([[0.0138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6030]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15755.0
Loss: 2.8253605705685914e-05
Action 0 - predicted reward: tensor([[0.1242]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0514]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5645.0
Loss: 0.050355009734630585
Action 0 - predicted reward: tensor([[-0.0311]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.0916]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5755.0
Loss: 0.015471277758479118
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2769]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0366]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6290.0
Loss: 12863.1455078125
KL Divergence: 18.792600631713867
Action 0 - predicted reward: tensor([[2.2601]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9125]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6210.0
Loss: 9586.4638671875
KL Divergence: 18.806730270385742
Action 0 - predicted reward: tensor([[2.1898]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6687]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6315.0
Loss: 11645.671875
KL Divergence: 18.75000762939453
Action 0 - predicted reward: tensor([[2.3149]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7882]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5770.0
Loss: 11688.943359375
KL Divergence: 18.70779800415039
6099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1386]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.1958]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6775.0
Loss: 0.03241453692317009
Action 0 - predicted reward: tensor([[-1.2655]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9848]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7895.0
Loss: 0.0660974383354187
Action 0 - predicted reward: tensor([[-0.1538]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.0097]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7025.0
Loss: 0.04633496329188347
Action 0 - predicted reward: tensor([[0.3106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8958]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8365.0
Loss: 0.07670122385025024
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.7026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6434]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5325.0
Loss: 0.02751963771879673
Action 0 - predicted reward: tensor([[-0.1016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1088]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4455.0
Loss: 0.015564791858196259
Action 0 - predicted reward: tensor([[-0.2624]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9011]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5125.0
Loss: 0.03195922449231148
Action 0 - predicted reward: tensor([[-0.1170]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.9972]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7130.0
Loss: 0.056975413113832474
Greedy
Action 0 - predicted reward: tensor([[0.0371]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.7142]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4795.0
Loss: 0.017794743180274963
Action 0 - predicted reward: tensor([[-0.0160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.7861]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16030.0
Loss: 2.7666665118886158e-05
Action 0 - predicted reward: tensor([[0.0855]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9649]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5685.0
Loss: 0.04885758087038994
Action 0 - predicted reward: tensor([[0.2037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5392]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5760.0
Loss: 0.014707373455166817
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2606]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7742]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6290.0
Loss: 12869.5634765625
KL Divergence: 18.768186569213867
Action 0 - predicted reward: tensor([[2.3298]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4008]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6220.0
Loss: 9592.884765625
KL Divergence: 18.7948055267334
Action 0 - predicted reward: tensor([[2.2199]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2937]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6325.0
Loss: 11656.3671875
KL Divergence: 18.737810134887695
Action 0 - predicted reward: tensor([[2.3339]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8308]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5805.0
Loss: 11989.837890625
KL Divergence: 18.678878784179688
6199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.3637]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6815.0
Loss: 0.030488450080156326
Action 0 - predicted reward: tensor([[0.9104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1962]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8040.0
Loss: 0.07261069118976593
Action 0 - predicted reward: tensor([[-0.4059]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1745]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7105.0
Loss: 0.04684246703982353
Action 0 - predicted reward: tensor([[-0.1367]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.1469]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8380.0
Loss: 0.0692189484834671
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3506]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.4209]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5360.0
Loss: 0.024695780128240585
Action 0 - predicted reward: tensor([[-0.0034]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7012]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4495.0
Loss: 0.015047706663608551
Action 0 - predicted reward: tensor([[-1.6360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0228]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5160.0
Loss: 0.03210926428437233
Action 0 - predicted reward: tensor([[0.1415]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.8526]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7200.0
Loss: 0.0519796684384346
Greedy
Action 0 - predicted reward: tensor([[0.0377]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.7201]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4830.0
Loss: 0.018569761887192726
Action 0 - predicted reward: tensor([[0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.6016]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 16275.0
Loss: 2.5234663553419523e-05
Action 0 - predicted reward: tensor([[0.1864]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.6320]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5720.0
Loss: 0.05314061418175697
Action 0 - predicted reward: tensor([[0.0181]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3196]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5905.0
Loss: 0.01788186840713024
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3145]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3847]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6290.0
Loss: 12875.6171875
KL Divergence: 18.757524490356445
Action 0 - predicted reward: tensor([[2.3543]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4245]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6220.0
Loss: 9596.0966796875
KL Divergence: 18.785579681396484
Action 0 - predicted reward: tensor([[2.2657]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3425]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6360.0
Loss: 11966.150390625
KL Divergence: 18.741653442382812
Action 0 - predicted reward: tensor([[2.3596]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4276]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5810.0
Loss: 11692.7001953125
KL Divergence: 18.66864013671875
6299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0681]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.6994]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6890.0
Loss: 0.031207887455821037
Action 0 - predicted reward: tensor([[-2.4041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4740]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8045.0
Loss: 0.05893871933221817
Action 0 - predicted reward: tensor([[-0.0956]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2869]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7190.0
Loss: 0.043479155749082565
Action 0 - predicted reward: tensor([[-0.0380]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9634]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8505.0
Loss: 0.07095731049776077
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1335]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6032]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5465.0
Loss: 0.025041069835424423
Action 0 - predicted reward: tensor([[-0.3411]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8072]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4530.0
Loss: 0.01488169189542532
Action 0 - predicted reward: tensor([[-0.3875]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1834]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5195.0
Loss: 0.0359782800078392
Action 0 - predicted reward: tensor([[-0.6399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3910]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7305.0
Loss: 0.048474784940481186
Greedy
Action 0 - predicted reward: tensor([[-0.0966]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6769]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4835.0
Loss: 0.015106338076293468
Action 0 - predicted reward: tensor([[-0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.4905]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 16550.0
Loss: 2.712065543164499e-05
Action 0 - predicted reward: tensor([[-0.0321]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.5512]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5720.0
Loss: 0.052838850766420364
Action 0 - predicted reward: tensor([[0.4885]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0351]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5905.0
Loss: 0.015177734196186066
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3094]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3709]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6305.0
Loss: 12878.556640625
KL Divergence: 18.760530471801758
Action 0 - predicted reward: tensor([[2.3047]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7514]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6260.0
Loss: 9897.3115234375
KL Divergence: 18.77638816833496
Action 0 - predicted reward: tensor([[2.2798]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2487]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6360.0
Loss: 11974.53125
KL Divergence: 18.729127883911133
Action 0 - predicted reward: tensor([[2.3925]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8446]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5810.0
Loss: 11400.791015625
KL Divergence: 18.660375595092773
6399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.7267]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9918]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6970.0
Loss: 0.02927425317466259
Action 0 - predicted reward: tensor([[-0.0843]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0180]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8080.0
Loss: 0.04099194332957268
Action 0 - predicted reward: tensor([[0.2695]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7893]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7270.0
Loss: 0.04598737508058548
Action 0 - predicted reward: tensor([[-0.1505]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0778]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8510.0
Loss: 0.06220246106386185
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1751]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.3472]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5500.0
Loss: 0.02422742359340191
Action 0 - predicted reward: tensor([[-0.0136]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.7689]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4530.0
Loss: 0.014833914116024971
Action 0 - predicted reward: tensor([[0.0919]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1135]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5200.0
Loss: 0.030842624604701996
Action 0 - predicted reward: tensor([[-0.8639]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5488]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7400.0
Loss: 0.0426064059138298
Greedy
Action 0 - predicted reward: tensor([[0.4235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0792]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4905.0
Loss: 0.023205047473311424
Action 0 - predicted reward: tensor([[0.0447]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7267]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16865.0
Loss: 0.003037262475118041
Action 0 - predicted reward: tensor([[0.4742]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0183]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5720.0
Loss: 0.048226967453956604
Action 0 - predicted reward: tensor([[0.1424]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3603]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5975.0
Loss: 0.01867944933474064
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3314]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3979]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6305.0
Loss: 12879.396484375
KL Divergence: 18.743377685546875
Action 0 - predicted reward: tensor([[2.3368]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8985]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6260.0
Loss: 9899.923828125
KL Divergence: 18.76346778869629
Action 0 - predicted reward: tensor([[2.3145]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2603]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6360.0
Loss: 11977.8134765625
KL Divergence: 18.726552963256836
Action 0 - predicted reward: tensor([[2.4048]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4753]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5810.0
Loss: 11399.23046875
KL Divergence: 18.649396896362305
6499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0786]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.9367]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7005.0
Loss: 0.028326261788606644
Action 0 - predicted reward: tensor([[-0.5896]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.2694]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8170.0
Loss: 0.04485839605331421
Action 0 - predicted reward: tensor([[0.1707]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3785]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7275.0
Loss: 0.042330678552389145
Action 0 - predicted reward: tensor([[-0.0549]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.1967]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8620.0
Loss: 0.06488977372646332
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2890]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.7764]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5570.0
Loss: 0.024774258956313133
Action 0 - predicted reward: tensor([[0.0407]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.8693]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4530.0
Loss: 0.014676492661237717
Action 0 - predicted reward: tensor([[0.2384]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1327]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5305.0
Loss: 0.03221791237592697
Action 0 - predicted reward: tensor([[0.1801]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2915]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7515.0
Loss: 0.049778830260038376
Greedy
Action 0 - predicted reward: tensor([[-0.0297]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.1242]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4905.0
Loss: 0.01729174517095089
Action 0 - predicted reward: tensor([[-0.1410]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.3039]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 17120.0
Loss: 0.0006804703734815121
Action 0 - predicted reward: tensor([[-0.0251]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.8896]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5755.0
Loss: 0.045880500227212906
Action 0 - predicted reward: tensor([[-0.0514]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.0880]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6010.0
Loss: 0.01918189227581024
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3656]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4295]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6310.0
Loss: 12292.25
KL Divergence: 18.733869552612305
Action 0 - predicted reward: tensor([[2.3436]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4090]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6275.0
Loss: 9901.7548828125
KL Divergence: 18.762371063232422
Action 0 - predicted reward: tensor([[2.3217]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3911]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6360.0
Loss: 11686.615234375
KL Divergence: 18.715425491333008
Action 0 - predicted reward: tensor([[2.3945]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9797]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5810.0
Loss: 11100.2998046875
KL Divergence: 18.634401321411133
6599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0584]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.1399]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7155.0
Loss: 0.03785736858844757
Action 0 - predicted reward: tensor([[-1.4314]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7664]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8325.0
Loss: 0.05617094412446022
Action 0 - predicted reward: tensor([[0.0273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7708]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7400.0
Loss: 0.042431801557540894
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8770.0
Loss: 0.07212767750024796
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5681]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2598]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5605.0
Loss: 0.02023894339799881
Action 0 - predicted reward: tensor([[0.3842]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3350]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4605.0
Loss: 0.015706690028309822
Action 0 - predicted reward: tensor([[0.6583]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0919]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5345.0
Loss: 0.02701394446194172
Action 0 - predicted reward: tensor([[0.8728]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8268]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7625.0
Loss: 0.04469921812415123
Greedy
Action 0 - predicted reward: tensor([[0.0357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.4977]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4975.0
Loss: 0.021186990663409233
Action 0 - predicted reward: tensor([[-0.0661]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.8627]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 17370.0
Loss: 0.00022550870198756456
Action 0 - predicted reward: tensor([[-0.0448]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.1163]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5790.0
Loss: 0.04538770765066147
Action 0 - predicted reward: tensor([[0.1970]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2301]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6015.0
Loss: 0.01873169094324112
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3745]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9314]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6310.0
Loss: 11993.0888671875
KL Divergence: 18.725725173950195
Action 0 - predicted reward: tensor([[2.3393]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4116]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6275.0
Loss: 9901.9912109375
KL Divergence: 18.745288848876953
Action 0 - predicted reward: tensor([[2.3240]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3822]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6370.0
Loss: 11680.2109375
KL Divergence: 18.703582763671875
Action 0 - predicted reward: tensor([[2.3925]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2298]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5810.0
Loss: 10807.451171875
KL Divergence: 18.624849319458008
6699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1410]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.0551]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7300.0
Loss: 0.036285657435655594
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8470.0
Loss: 0.052920158952474594
Action 0 - predicted reward: tensor([[-0.0515]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7480.0
Loss: 0.045571763068437576
Action 0 - predicted reward: tensor([[0.1704]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5233]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8770.0
Loss: 0.06527784466743469
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.6692]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4441]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5675.0
Loss: 0.02680295705795288
Action 0 - predicted reward: tensor([[-0.2674]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2431]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4720.0
Loss: 0.018579155206680298
Action 0 - predicted reward: tensor([[-0.0276]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.5009]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5455.0
Loss: 0.0324113704264164
Action 0 - predicted reward: tensor([[-0.1623]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0555]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7700.0
Loss: 0.03631790354847908
Greedy
Action 0 - predicted reward: tensor([[0.3534]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9671]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5010.0
Loss: 0.021624919027090073
Action 0 - predicted reward: tensor([[-0.0841]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9119]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17570.0
Loss: 0.00012613939179573208
Action 0 - predicted reward: tensor([[0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.9767]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5825.0
Loss: 0.03758613392710686
Action 0 - predicted reward: tensor([[0.1538]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3837]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6020.0
Loss: 0.017893033102154732
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3993]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7507]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6310.0
Loss: 11699.490234375
KL Divergence: 18.714033126831055
Action 0 - predicted reward: tensor([[2.3669]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0212]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
Loss: 9903.6318359375
KL Divergence: 18.739849090576172
Action 0 - predicted reward: tensor([[2.3445]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4177]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6405.0
Loss: 11988.8779296875
KL Divergence: 18.687763214111328
Action 0 - predicted reward: tensor([[2.4214]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1158]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5815.0
Loss: 10803.45703125
KL Divergence: 18.614707946777344
6799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0988]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7865]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7390.0
Loss: 0.03406568616628647
Action 0 - predicted reward: tensor([[-0.7581]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.4468]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8580.0
Loss: 0.05624103546142578
Action 0 - predicted reward: tensor([[0.2262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.0597]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7480.0
Loss: 0.04184248670935631
Action 0 - predicted reward: tensor([[0.0625]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.8078]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8825.0
Loss: 0.06274048238992691
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0881]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.4848]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5780.0
Loss: 0.02365349978208542
Action 0 - predicted reward: tensor([[-0.2430]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8071]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4790.0
Loss: 0.02328893169760704
Action 0 - predicted reward: tensor([[0.0406]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.0369]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5600.0
Loss: 0.04239104688167572
Action 0 - predicted reward: tensor([[0.0007]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.0015]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7775.0
Loss: 0.03596426546573639
Greedy
Action 0 - predicted reward: tensor([[-0.0463]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.6534]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5010.0
Loss: 0.016603566706180573
Action 0 - predicted reward: tensor([[0.0796]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.7676]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17805.0
Loss: 0.00016361412417609245
Action 0 - predicted reward: tensor([[0.1630]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1036]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5860.0
Loss: 0.03616250678896904
Action 0 - predicted reward: tensor([[0.1935]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9672]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6020.0
Loss: 0.018290836364030838
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4103]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4697]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6310.0
Loss: 11403.4150390625
KL Divergence: 18.697114944458008
Action 0 - predicted reward: tensor([[2.3505]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2691]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6285.0
Loss: 9900.6240234375
KL Divergence: 18.72066307067871
Action 0 - predicted reward: tensor([[2.3425]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4180]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6405.0
Loss: 11988.5048828125
KL Divergence: 18.676225662231445
Action 0 - predicted reward: tensor([[2.4294]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6989]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5815.0
Loss: 10806.953125
KL Divergence: 18.596721649169922
6899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1712]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6239]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7465.0
Loss: 0.030296968296170235
Action 0 - predicted reward: tensor([[-0.0150]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7351]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8620.0
Loss: 0.05498605594038963
Action 0 - predicted reward: tensor([[0.0180]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.9189]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7480.0
Loss: 0.0411837100982666
Action 0 - predicted reward: tensor([[0.1150]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8217]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9050.0
Loss: 0.06773140281438828
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0776]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.0073]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5780.0
Loss: 0.02234479784965515
Action 0 - predicted reward: tensor([[-0.0652]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.6523]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4790.0
Loss: 0.01955900341272354
Action 0 - predicted reward: tensor([[-0.1022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7283]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5705.0
Loss: 0.03467216715216637
Action 0 - predicted reward: tensor([[-0.5460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8350]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7810.0
Loss: 0.03658321499824524
Greedy
Action 0 - predicted reward: tensor([[0.2314]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9484]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5050.0
Loss: 0.011980609968304634
Action 0 - predicted reward: tensor([[-0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.0697]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 18045.0
Loss: 5.7883305998984724e-05
Action 0 - predicted reward: tensor([[0.3263]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3149]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5860.0
Loss: 0.03543504327535629
Action 0 - predicted reward: tensor([[-0.0909]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.1772]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6020.0
Loss: 0.014912116341292858
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4259]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8290]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6310.0
Loss: 11104.767578125
KL Divergence: 18.698801040649414
Action 0 - predicted reward: tensor([[2.3469]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4163]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6290.0
Loss: 9604.564453125
KL Divergence: 18.703859329223633
Action 0 - predicted reward: tensor([[2.3548]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4265]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6405.0
Loss: 11396.5712890625
KL Divergence: 18.663768768310547
Action 0 - predicted reward: tensor([[2.4223]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0809]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5815.0
Loss: 10210.810546875
KL Divergence: 18.59157943725586
6999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.7401]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5713]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7570.0
Loss: 0.030299212783575058
Action 0 - predicted reward: tensor([[-0.5260]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.8055]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8660.0
Loss: 0.04969673976302147
Action 0 - predicted reward: tensor([[0.1888]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.3697]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7550.0
Loss: 0.04068223759531975
Action 0 - predicted reward: tensor([[0.2632]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1905]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9130.0
Loss: 0.061014771461486816
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.7813]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2759]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5780.0
Loss: 0.020597701892256737
Action 0 - predicted reward: tensor([[-0.1554]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0633]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4795.0
Loss: 0.018474994227290154
Action 0 - predicted reward: tensor([[0.2989]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.5652]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5810.0
Loss: 0.03114699013531208
Action 0 - predicted reward: tensor([[-0.0148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.3963]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7910.0
Loss: 0.03480038791894913
Greedy
Action 0 - predicted reward: tensor([[-0.0155]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.1652]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5125.0
Loss: 0.017795514315366745
Action 0 - predicted reward: tensor([[-0.0128]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.5359]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18280.0
Loss: 4.5402077375911176e-05
Action 0 - predicted reward: tensor([[0.3265]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9505]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5860.0
Loss: 0.03196267411112785
Action 0 - predicted reward: tensor([[0.0355]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.3343]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6020.0
Loss: 0.014600822702050209
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4153]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1072]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6310.0
Loss: 11101.7958984375
KL Divergence: 18.68268394470215
Action 0 - predicted reward: tensor([[2.3417]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3043]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6295.0
Loss: 9603.4833984375
KL Divergence: 18.68720054626465
Action 0 - predicted reward: tensor([[2.3605]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9392]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6405.0
Loss: 11395.576171875
KL Divergence: 18.647537231445312
Action 0 - predicted reward: tensor([[2.4070]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2135]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5850.0
Loss: 10508.4501953125
KL Divergence: 18.566770553588867
7099.
Epsilon Greedy 5%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7610.0
Loss: 0.028824051842093468
Action 0 - predicted reward: tensor([[-0.0243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-76.9224]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8915.0
Loss: 0.0560327023267746
Action 0 - predicted reward: tensor([[-0.0085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.1417]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7585.0
Loss: 0.03974318131804466
Action 0 - predicted reward: tensor([[0.0833]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.0159]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9275.0
Loss: 0.06386983394622803
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0332]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.2905]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5885.0
Loss: 0.027623586356639862
Action 0 - predicted reward: tensor([[0.0495]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1355]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4800.0
Loss: 0.018734244629740715
Action 0 - predicted reward: tensor([[0.0257]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.1933]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5880.0
Loss: 0.029322916641831398
Action 0 - predicted reward: tensor([[-0.2788]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1470]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7965.0
Loss: 0.03319384530186653
Greedy
Action 0 - predicted reward: tensor([[0.1197]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.6799]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5265.0
Loss: 0.02487063594162464
Action 0 - predicted reward: tensor([[0.0422]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.8422]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18450.0
Loss: 0.0059453221037983894
Action 0 - predicted reward: tensor([[-0.0306]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0577]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5965.0
Loss: 0.04079459607601166
Action 0 - predicted reward: tensor([[0.1022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.2781]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6090.0
Loss: 0.018263913691043854
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4049]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6236]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6350.0
Loss: 11399.5634765625
KL Divergence: 18.667177200317383
Action 0 - predicted reward: tensor([[2.3584]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9127]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6305.0
Loss: 9006.89453125
KL Divergence: 18.685392379760742
Action 0 - predicted reward: tensor([[2.3437]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7616]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6440.0
Loss: 11395.1943359375
KL Divergence: 18.62834358215332
Action 0 - predicted reward: tensor([[2.4017]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8909]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5850.0
Loss: 10509.544921875
KL Divergence: 18.55178451538086
7199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.0352]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7720.0
Loss: 0.037657711654901505
Action 0 - predicted reward: tensor([[-0.1660]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.7451]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9005.0
Loss: 0.057691577821969986
Action 0 - predicted reward: tensor([[0.0668]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.0610]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7670.0
Loss: 0.03782743588089943
Action 0 - predicted reward: tensor([[0.0251]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6959]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9280.0
Loss: 0.0652201771736145
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0254]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-82.1645]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5955.0
Loss: 0.031760651618242264
Action 0 - predicted reward: tensor([[-0.0763]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0520]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4840.0
Loss: 0.018599754199385643
Action 0 - predicted reward: tensor([[-0.6471]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8361]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6020.0
Loss: 0.030451348051428795
Action 0 - predicted reward: tensor([[0.0304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.6257]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8005.0
Loss: 0.03307289257645607
Greedy
Action 0 - predicted reward: tensor([[-0.0246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.6856]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5265.0
Loss: 0.01974755898118019
Action 0 - predicted reward: tensor([[0.0312]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0449]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18665.0
Loss: 0.00591562082991004
Action 0 - predicted reward: tensor([[0.2872]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9885]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5970.0
Loss: 0.03455347195267677
Action 0 - predicted reward: tensor([[-0.4555]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0095]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6090.0
Loss: 0.014657591469585896
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3951]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4555]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6390.0
Loss: 11701.203125
KL Divergence: 18.648422241210938
Action 0 - predicted reward: tensor([[2.3724]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4422]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6305.0
Loss: 8114.59033203125
KL Divergence: 18.67141342163086
Action 0 - predicted reward: tensor([[2.3084]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0017]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6545.0
Loss: 12280.1953125
KL Divergence: 18.604822158813477
Action 0 - predicted reward: tensor([[2.4191]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9899]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5850.0
Loss: 10510.029296875
KL Divergence: 18.52893829345703
7299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2497]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5271]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7935.0
Loss: 0.043642185628414154
Action 0 - predicted reward: tensor([[-0.4900]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0576]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9115.0
Loss: 0.052018314599990845
Action 0 - predicted reward: tensor([[0.0260]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1557]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7710.0
Loss: 0.034313131123781204
Action 0 - predicted reward: tensor([[0.2262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9413]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9425.0
Loss: 0.06363758444786072
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.9379]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7476]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5990.0
Loss: 0.031852561980485916
Action 0 - predicted reward: tensor([[0.0573]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.0879]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4845.0
Loss: 0.018526971340179443
Action 0 - predicted reward: tensor([[-0.2131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.3952]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6090.0
Loss: 0.029620543122291565
Action 0 - predicted reward: tensor([[-0.1967]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.9578]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8080.0
Loss: 0.03710734844207764
Greedy
Action 0 - predicted reward: tensor([[0.1784]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8044]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5300.0
Loss: 0.020601963624358177
Action 0 - predicted reward: tensor([[0.0312]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8962]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18840.0
Loss: 0.002176064532250166
Action 0 - predicted reward: tensor([[-0.0782]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4632]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6040.0
Loss: 0.033263131976127625
Action 0 - predicted reward: tensor([[-0.1345]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0820]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6125.0
Loss: 0.014236681163311005
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4534]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5045]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6395.0
Loss: 11702.7822265625
KL Divergence: 18.640005111694336
Action 0 - predicted reward: tensor([[2.4019]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4682]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6315.0
Loss: 8115.52783203125
KL Divergence: 18.658672332763672
Action 0 - predicted reward: tensor([[2.2920]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3563]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6550.0
Loss: 11983.8544921875
KL Divergence: 18.587350845336914
Action 0 - predicted reward: tensor([[2.4034]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4569]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5850.0
Loss: 10509.9599609375
KL Divergence: 18.50876808166504
7399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1366]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.5271]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7940.0
Loss: 0.03821723163127899
Action 0 - predicted reward: tensor([[0.8425]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1612]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9255.0
Loss: 0.05501217395067215
Action 0 - predicted reward: tensor([[-0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.5111]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7825.0
Loss: 0.030605655163526535
Action 0 - predicted reward: tensor([[-0.0327]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0924]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9465.0
Loss: 0.0605771541595459
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0938]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.4611]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5995.0
Loss: 0.02545890398323536
Action 0 - predicted reward: tensor([[0.0220]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.2022]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4860.0
Loss: 0.01775429956614971
Action 0 - predicted reward: tensor([[0.4019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5393]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6230.0
Loss: 0.03811141848564148
Action 0 - predicted reward: tensor([[-0.7628]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9628]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8200.0
Loss: 0.04092444106936455
Greedy
Action 0 - predicted reward: tensor([[0.5314]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9677]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5300.0
Loss: 0.017497137188911438
Action 0 - predicted reward: tensor([[0.0427]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0777]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18950.0
Loss: 0.0006108063389547169
Action 0 - predicted reward: tensor([[-0.0110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3647]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6040.0
Loss: 0.027365494519472122
Action 0 - predicted reward: tensor([[0.1042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9840]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6125.0
Loss: 0.013633240014314651
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4255]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4879]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6395.0
Loss: 11702.8076171875
KL Divergence: 18.612619400024414
Action 0 - predicted reward: tensor([[2.3726]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1346]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6320.0
Loss: 8110.6591796875
KL Divergence: 18.64899444580078
Action 0 - predicted reward: tensor([[2.3274]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4207]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6550.0
Loss: 11987.25
KL Divergence: 18.582847595214844
Action 0 - predicted reward: tensor([[2.4390]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4912]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5850.0
Loss: 10508.72265625
KL Divergence: 18.498559951782227
7499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-80.3767]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8025.0
Loss: 0.03298775106668472
Action 0 - predicted reward: tensor([[0.2442]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-83.6698]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9370.0
Loss: 0.0544055737555027
Action 0 - predicted reward: tensor([[0.1011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.3584]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7870.0
Loss: 0.03143835812807083
Action 0 - predicted reward: tensor([[-0.1933]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9996]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9540.0
Loss: 0.05158494785428047
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.6408]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8496]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6065.0
Loss: 0.02676725946366787
Action 0 - predicted reward: tensor([[0.1375]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.9610]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4895.0
Loss: 0.018953245133161545
Action 0 - predicted reward: tensor([[0.1316]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.3456]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6305.0
Loss: 0.03667737543582916
Action 0 - predicted reward: tensor([[-0.6201]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.5847]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8255.0
Loss: 0.033723630011081696
Greedy
Action 0 - predicted reward: tensor([[-0.1911]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.0374]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5335.0
Loss: 0.020426567643880844
Action 0 - predicted reward: tensor([[-0.1148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.7242]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 19030.0
Loss: 0.00022063447977416217
Action 0 - predicted reward: tensor([[0.1549]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9588]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6075.0
Loss: 0.02327708899974823
Action 0 - predicted reward: tensor([[-0.2113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0745]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6160.0
Loss: 0.01291296910494566
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3753]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9014]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6395.0
Loss: 11697.345703125
KL Divergence: 18.598472595214844
Action 0 - predicted reward: tensor([[2.3773]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9231]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6335.0
Loss: 8115.62744140625
KL Divergence: 18.629484176635742
Action 0 - predicted reward: tensor([[2.3149]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6215]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6555.0
Loss: 11686.115234375
KL Divergence: 18.564666748046875
Action 0 - predicted reward: tensor([[2.3925]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7335]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5885.0
Loss: 10808.603515625
KL Divergence: 18.48518180847168
7599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1735]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.3669]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8170.0
Loss: 0.03939899057149887
Action 0 - predicted reward: tensor([[0.1896]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.2691]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9450.0
Loss: 0.05669380724430084
Action 0 - predicted reward: tensor([[0.0899]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1809]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7980.0
Loss: 0.039200905710458755
Action 0 - predicted reward: tensor([[0.0749]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.0454]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9620.0
Loss: 0.04339981451630592
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0073]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.9185]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6170.0
Loss: 0.025071054697036743
Action 0 - predicted reward: tensor([[-0.3114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0302]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4900.0
Loss: 0.01827278919517994
Action 0 - predicted reward: tensor([[0.4127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2898]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6340.0
Loss: 0.03142619505524635
Action 0 - predicted reward: tensor([[-1.9903]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8884]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8330.0
Loss: 0.032919201999902725
Greedy
Action 0 - predicted reward: tensor([[0.3009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9658]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5370.0
Loss: 0.019784295931458473
Action 0 - predicted reward: tensor([[-0.0328]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.0886]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19150.0
Loss: 0.000309539056615904
Action 0 - predicted reward: tensor([[0.0061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3762]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6075.0
Loss: 0.022817518562078476
Action 0 - predicted reward: tensor([[0.0305]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7720]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6230.0
Loss: 0.01140441931784153
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3565]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4744]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6395.0
Loss: 11403.2705078125
KL Divergence: 18.587360382080078
Action 0 - predicted reward: tensor([[2.3474]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1090]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6345.0
Loss: 8114.916015625
KL Divergence: 18.6131534576416
Action 0 - predicted reward: tensor([[2.3583]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5981]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6630.0
Loss: 11395.6845703125
KL Divergence: 18.561588287353516
Action 0 - predicted reward: tensor([[2.4288]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4809]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5920.0
Loss: 11106.8876953125
KL Divergence: 18.46547508239746
7699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8730]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8285.0
Loss: 0.03660918399691582
Action 0 - predicted reward: tensor([[0.7192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.3959]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9535.0
Loss: 0.05486932396888733
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7985.0
Loss: 0.03372478485107422
Action 0 - predicted reward: tensor([[-0.0509]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0650]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9695.0
Loss: 0.04330621287226677
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0303]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-76.4211]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6175.0
Loss: 0.024641212075948715
Action 0 - predicted reward: tensor([[-0.0225]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8354]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4910.0
Loss: 0.017520945519208908
Action 0 - predicted reward: tensor([[-0.0770]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6030]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6445.0
Loss: 0.032649558037519455
Action 0 - predicted reward: tensor([[0.0382]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.9270]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8345.0
Loss: 0.026986151933670044
Greedy
Action 0 - predicted reward: tensor([[0.2057]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9829]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5370.0
Loss: 0.019760215654969215
Action 0 - predicted reward: tensor([[-0.0173]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.9354]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 19305.0
Loss: 0.005525956396013498
Action 0 - predicted reward: tensor([[-0.0240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.5970]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6110.0
Loss: 0.021601026877760887
Action 0 - predicted reward: tensor([[0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.9132]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6230.0
Loss: 0.00714693171903491
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3596]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7965]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6395.0
Loss: 11105.81640625
KL Divergence: 18.57898712158203
Action 0 - predicted reward: tensor([[2.3842]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4507]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6350.0
Loss: 8106.7646484375
KL Divergence: 18.60639762878418
Action 0 - predicted reward: tensor([[2.3943]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4607]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6665.0
Loss: 11397.9296875
KL Divergence: 18.550737380981445
Action 0 - predicted reward: tensor([[2.4341]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4936]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5920.0
Loss: 10808.5654296875
KL Divergence: 18.450223922729492
7799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0427]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.9372]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8335.0
Loss: 0.04022352024912834
Action 0 - predicted reward: tensor([[1.2045]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2307]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9625.0
Loss: 0.05795668438076973
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8090.0
Loss: 0.05535610765218735
Action 0 - predicted reward: tensor([[-0.0953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.1775]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9705.0
Loss: 0.037975359708070755
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0511]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.2590]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6210.0
Loss: 0.02095732092857361
Action 0 - predicted reward: tensor([[-0.1729]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8895]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4910.0
Loss: 0.019253263249993324
Action 0 - predicted reward: tensor([[-0.1737]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4567]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6515.0
Loss: 0.030949663370847702
Action 0 - predicted reward: tensor([[0.0090]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4619]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8380.0
Loss: 0.03172477334737778
Greedy
Action 0 - predicted reward: tensor([[-0.0295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.6370]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5405.0
Loss: 0.01959848217666149
Action 0 - predicted reward: tensor([[-0.0109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.9411]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19445.0
Loss: 0.0041376687586307526
Action 0 - predicted reward: tensor([[-0.2418]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1036]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6110.0
Loss: 0.01363097969442606
Action 0 - predicted reward: tensor([[-0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6226]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6235.0
Loss: 0.006435769144445658
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3863]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4565]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6395.0
Loss: 11104.640625
KL Divergence: 18.57094383239746
Action 0 - predicted reward: tensor([[2.4067]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2062]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6360.0
Loss: 7812.55810546875
KL Divergence: 18.605623245239258
Action 0 - predicted reward: tensor([[2.3953]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4515]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6665.0
Loss: 11102.2880859375
KL Divergence: 18.547374725341797
Action 0 - predicted reward: tensor([[2.4363]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4965]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5920.0
Loss: 10798.15625
KL Divergence: 18.449676513671875
7899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0344]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1320]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8340.0
Loss: 0.03554956614971161
Action 0 - predicted reward: tensor([[1.2821]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.0246]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9695.0
Loss: 0.05635684356093407
Action 0 - predicted reward: tensor([[0.1109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0117]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8110.0
Loss: 0.030415285378694534
Action 0 - predicted reward: tensor([[0.2181]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3269]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9750.0
Loss: 0.03809115290641785
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0521]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6245.0
Loss: 0.020668767392635345
Action 0 - predicted reward: tensor([[-0.1788]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.5329]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4945.0
Loss: 0.022307543084025383
Action 0 - predicted reward: tensor([[-0.3007]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6620.0
Loss: 0.03833017870783806
Action 0 - predicted reward: tensor([[-0.0058]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.5407]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8420.0
Loss: 0.025709200650453568
Greedy
Action 0 - predicted reward: tensor([[0.0468]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-70.7507]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5405.0
Loss: 0.01914186216890812
Action 0 - predicted reward: tensor([[-0.0081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2931]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19495.0
Loss: 0.003569719847291708
Action 0 - predicted reward: tensor([[0.0358]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5010]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6110.0
Loss: 0.009985293261706829
Action 0 - predicted reward: tensor([[-0.0335]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9428]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
Loss: 0.006853719707578421
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3730]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8255]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6400.0
Loss: 11098.2490234375
KL Divergence: 18.553356170654297
Action 0 - predicted reward: tensor([[2.3642]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2262]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6360.0
Loss: 7518.232421875
KL Divergence: 18.58181381225586
Action 0 - predicted reward: tensor([[2.3701]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4178]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6670.0
Loss: 11101.4443359375
KL Divergence: 18.53118324279785
Action 0 - predicted reward: tensor([[2.4211]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3876]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5925.0
Loss: 10504.8232421875
KL Divergence: 18.42453384399414
7999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0824]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9772]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8380.0
Loss: 0.032760024070739746
Action 0 - predicted reward: tensor([[-0.4172]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.2905]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9745.0
Loss: 0.05435512587428093
Action 0 - predicted reward: tensor([[0.0149]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0767]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8225.0
Loss: 0.02801542356610298
Action 0 - predicted reward: tensor([[-0.1585]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.4343]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9800.0
Loss: 0.038005974143743515
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0619]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0521]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6250.0
Loss: 0.017145652323961258
Action 0 - predicted reward: tensor([[-0.2104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0646]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4950.0
Loss: 0.017451738938689232
Action 0 - predicted reward: tensor([[-0.9266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6342]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6690.0
Loss: 0.04255985841155052
Action 0 - predicted reward: tensor([[-0.0132]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9884]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8530.0
Loss: 0.033363666385412216
Greedy
Action 0 - predicted reward: tensor([[0.3497]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6884]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5440.0
Loss: 0.0191221721470356
Action 0 - predicted reward: tensor([[-0.1078]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1568]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 19575.0
Loss: 0.007373180240392685
Action 0 - predicted reward: tensor([[0.0054]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.0678]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6110.0
Loss: 0.01004337053745985
Action 0 - predicted reward: tensor([[0.2186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9897]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6280.0
Loss: 0.006727679166942835
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3760]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0284]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6400.0
Loss: 10805.0849609375
KL Divergence: 18.54326057434082
Action 0 - predicted reward: tensor([[2.4141]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4778]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6380.0
Loss: 6924.5625
KL Divergence: 18.58332061767578
Action 0 - predicted reward: tensor([[2.4332]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4891]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6675.0
Loss: 10800.638671875
KL Divergence: 18.528762817382812
Action 0 - predicted reward: tensor([[2.4292]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4733]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5960.0
Loss: 10809.314453125
KL Divergence: 18.421131134033203
8099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.8914]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9205]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8420.0
Loss: 0.031949035823345184
Action 0 - predicted reward: tensor([[0.3133]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.5779]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9825.0
Loss: 0.05094665661454201
Action 0 - predicted reward: tensor([[0.0498]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.3377]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8315.0
Loss: 0.03190046176314354
Action 0 - predicted reward: tensor([[-0.0329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2194]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9915.0
Loss: 0.04150687903165817
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0440]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.5890]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6250.0
Loss: 0.013536217622458935
Action 0 - predicted reward: tensor([[0.0423]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.0811]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4950.0
Loss: 0.015715867280960083
Action 0 - predicted reward: tensor([[0.1802]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8350]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6795.0
Loss: 0.0442010834813118
Action 0 - predicted reward: tensor([[-2.1259]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9376]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8670.0
Loss: 0.027317095547914505
Greedy
Action 0 - predicted reward: tensor([[0.0292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.5856]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5475.0
Loss: 0.019237378612160683
Action 0 - predicted reward: tensor([[0.0160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2314]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19670.0
Loss: 0.009145339950919151
Action 0 - predicted reward: tensor([[0.0060]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.3974]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6110.0
Loss: 0.003685073694214225
Action 0 - predicted reward: tensor([[0.0074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.9691]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6280.0
Loss: 0.00677714915946126
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3907]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8159]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6400.0
Loss: 10799.986328125
KL Divergence: 18.530916213989258
Action 0 - predicted reward: tensor([[2.4385]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8958]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6390.0
Loss: 6924.96337890625
KL Divergence: 18.573896408081055
Action 0 - predicted reward: tensor([[2.3643]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3576]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6675.0
Loss: 10502.7509765625
KL Divergence: 18.492944717407227
Action 0 - predicted reward: tensor([[2.4117]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1456]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5960.0
Loss: 10810.3173828125
KL Divergence: 18.405054092407227
8199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3924]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1294]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8535.0
Loss: 0.03706056997179985
Action 0 - predicted reward: tensor([[0.2846]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.5006]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9940.0
Loss: 0.05219943821430206
Action 0 - predicted reward: tensor([[-0.0301]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9889]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8420.0
Loss: 0.028261683881282806
Action 0 - predicted reward: tensor([[-0.0207]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.0576]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9930.0
Loss: 0.029627161100506783
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2642]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8711]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6285.0
Loss: 0.016332359984517097
Action 0 - predicted reward: tensor([[0.1614]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0161]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4950.0
Loss: 0.010702348314225674
Action 0 - predicted reward: tensor([[-0.3891]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1148]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6795.0
Loss: 0.036181285977363586
Action 0 - predicted reward: tensor([[0.0028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0383]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8680.0
Loss: 0.023601463064551353
Greedy
Action 0 - predicted reward: tensor([[-0.0543]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.7054]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5475.0
Loss: 0.015719247981905937
Action 0 - predicted reward: tensor([[-0.0401]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1312]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19735.0
Loss: 0.008517649956047535
Action 0 - predicted reward: tensor([[-0.0166]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.1898]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6110.0
Loss: 0.003375875996425748
Action 0 - predicted reward: tensor([[0.0095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.5428]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6315.0
Loss: 0.011640780605375767
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3915]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6219]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6405.0
Loss: 10502.4716796875
KL Divergence: 18.52177619934082
Action 0 - predicted reward: tensor([[2.4121]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0231]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6400.0
Loss: 6924.60595703125
KL Divergence: 18.551374435424805
Action 0 - predicted reward: tensor([[2.3813]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6785]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6715.0
Loss: 10804.7333984375
KL Divergence: 18.48317527770996
Action 0 - predicted reward: tensor([[2.4876]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5359]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5995.0
Loss: 11108.6806640625
KL Divergence: 18.394948959350586
8299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2198]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.1040]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8680.0
Loss: 0.036589860916137695
Action 0 - predicted reward: tensor([[-0.1467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7479]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10055.0
Loss: 0.05290358141064644
Action 0 - predicted reward: tensor([[-0.0678]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.9589]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8425.0
Loss: 0.02492150478065014
Action 0 - predicted reward: tensor([[0.1471]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.9369]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9975.0
Loss: 0.028723424300551414
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2912]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0299]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6355.0
Loss: 0.019645536318421364
Action 0 - predicted reward: tensor([[-0.0573]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.3600]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4985.0
Loss: 0.01369449868798256
Action 0 - predicted reward: tensor([[0.2302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2348]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6830.0
Loss: 0.03536646068096161
Action 0 - predicted reward: tensor([[-0.1268]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.1303]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8790.0
Loss: 0.024574555456638336
Greedy
Action 0 - predicted reward: tensor([[-0.0059]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.9299]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5475.0
Loss: 0.01565597392618656
Action 0 - predicted reward: tensor([[0.0062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.1227]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19770.0
Loss: 0.00831787008792162
Action 0 - predicted reward: tensor([[0.0030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.3026]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 0.00331143569201231
Action 0 - predicted reward: tensor([[0.2774]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0545]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6320.0
Loss: 0.010972353629767895
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3847]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4402]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6405.0
Loss: 10206.5966796875
KL Divergence: 18.498214721679688
Action 0 - predicted reward: tensor([[2.4162]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4743]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6400.0
Loss: 6923.49755859375
KL Divergence: 18.535308837890625
Action 0 - predicted reward: tensor([[2.3887]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7396]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6750.0
Loss: 10502.8447265625
KL Divergence: 18.472442626953125
Action 0 - predicted reward: tensor([[2.4140]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7363]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6035.0
Loss: 11108.1728515625
KL Divergence: 18.37303352355957
8399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.7537]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8766]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8890.0
Loss: 0.0446101650595665
Action 0 - predicted reward: tensor([[-0.7851]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.0754]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10335.0
Loss: 0.06478307396173477
Action 0 - predicted reward: tensor([[0.1630]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.8806]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8470.0
Loss: 0.02520042657852173
Action 0 - predicted reward: tensor([[0.1791]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0489]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10020.0
Loss: 0.02885582111775875
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0046]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6390.0
Loss: 0.01900259591639042
Action 0 - predicted reward: tensor([[0.0061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.2114]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4990.0
Loss: 0.013741598464548588
Action 0 - predicted reward: tensor([[-1.3726]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8980]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6935.0
Loss: 0.045820362865924835
Action 0 - predicted reward: tensor([[-0.0519]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.5849]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8900.0
Loss: 0.030755288898944855
Greedy
Action 0 - predicted reward: tensor([[0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.8972]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5475.0
Loss: 0.015460574068129063
Action 0 - predicted reward: tensor([[-0.0495]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.5434]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19825.0
Loss: 0.00816996581852436
Action 0 - predicted reward: tensor([[-0.0514]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.1792]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 0.0003653274616226554
Action 0 - predicted reward: tensor([[0.1005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9198]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6320.0
Loss: 0.011254384182393551
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3996]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6834]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6405.0
Loss: 9904.529296875
KL Divergence: 18.498125076293945
Action 0 - predicted reward: tensor([[2.4281]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8795]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6415.0
Loss: 6924.07373046875
KL Divergence: 18.53232765197754
Action 0 - predicted reward: tensor([[2.4259]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4879]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6750.0
Loss: 10210.42578125
KL Divergence: 18.44925880432129
Action 0 - predicted reward: tensor([[2.4581]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5141]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6070.0
Loss: 11103.7626953125
KL Divergence: 18.36699676513672
8499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0308]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.3305]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9080.0
Loss: 0.042398612946271896
Action 0 - predicted reward: tensor([[-4.2958]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3944]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10445.0
Loss: 0.06435975432395935
Action 0 - predicted reward: tensor([[0.1259]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1090]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8515.0
Loss: 0.02536310814321041
Action 0 - predicted reward: tensor([[0.3575]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8864]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10165.0
Loss: 0.029414726421236992
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0263]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.2561]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6425.0
Loss: 0.02141115441918373
Action 0 - predicted reward: tensor([[0.1318]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1305]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4995.0
Loss: 0.013721360825002193
Action 0 - predicted reward: tensor([[0.4576]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4365]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7005.0
Loss: 0.04702797904610634
Action 0 - predicted reward: tensor([[-7.5382]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0605]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8940.0
Loss: 0.02063833922147751
Greedy
Action 0 - predicted reward: tensor([[0.1940]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0602]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5475.0
Loss: 0.015388112515211105
Action 0 - predicted reward: tensor([[-0.0167]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7733]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 19865.0
Loss: 0.00817432627081871
Action 0 - predicted reward: tensor([[0.0357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6377]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6180.0
Loss: 0.00013227798626758158
Action 0 - predicted reward: tensor([[-0.0262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9768]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6320.0
Loss: 0.011248782277107239
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4148]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3801]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6405.0
Loss: 9611.2109375
KL Divergence: 18.48999786376953
Action 0 - predicted reward: tensor([[2.4243]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4986]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6415.0
Loss: 6924.31640625
KL Divergence: 18.51835060119629
Action 0 - predicted reward: tensor([[2.4026]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4684]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6750.0
Loss: 10210.5283203125
KL Divergence: 18.436019897460938
Action 0 - predicted reward: tensor([[2.4376]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4346]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6070.0
Loss: 11107.998046875
KL Divergence: 18.348392486572266
8599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-89.9338]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9230.0
Loss: 0.04020901396870613
Action 0 - predicted reward: tensor([[0.1741]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8128]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10585.0
Loss: 0.07289914786815643
Action 0 - predicted reward: tensor([[0.1274]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.4209]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8695.0
Loss: 0.02938135899603367
Action 0 - predicted reward: tensor([[0.2541]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0191]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10270.0
Loss: 0.032352790236473083
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0503]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.8886]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6425.0
Loss: 0.020546572282910347
Action 0 - predicted reward: tensor([[0.1774]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9991]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5030.0
Loss: 0.015429416671395302
Action 0 - predicted reward: tensor([[0.0424]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.0749]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7110.0
Loss: 0.0573979951441288
Action 0 - predicted reward: tensor([[-0.0627]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6426]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8945.0
Loss: 0.017542291432619095
Greedy
Action 0 - predicted reward: tensor([[0.0238]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-80.1538]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5510.0
Loss: 0.017462758347392082
Action 0 - predicted reward: tensor([[-0.0264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8761]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19985.0
Loss: 0.017212213948369026
Action 0 - predicted reward: tensor([[-0.0225]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.6241]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6180.0
Loss: 5.522786523215473e-05
Action 0 - predicted reward: tensor([[-0.0393]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0371]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6320.0
Loss: 0.010741416364908218
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4359]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4897]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6440.0
Loss: 9016.708984375
KL Divergence: 18.475452423095703
Action 0 - predicted reward: tensor([[2.4086]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1912]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6435.0
Loss: 6626.375
KL Divergence: 18.502992630004883
Action 0 - predicted reward: tensor([[2.3832]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2597]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6755.0
Loss: 10205.6552734375
KL Divergence: 18.424535751342773
Action 0 - predicted reward: tensor([[2.5165]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5720]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6105.0
Loss: 10805.8115234375
KL Divergence: 18.347026824951172
8699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2988]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.4731]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9375.0
Loss: 0.03648106008768082
Action 0 - predicted reward: tensor([[0.1327]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0861]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 10740.0
Loss: 0.07669966667890549
Action 0 - predicted reward: tensor([[0.3959]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3861]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8765.0
Loss: 0.029400907456874847
Action 0 - predicted reward: tensor([[0.1198]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10350.0
Loss: 0.03406822308897972
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0049]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0178]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6425.0
Loss: 0.02026754803955555
Action 0 - predicted reward: tensor([[0.0743]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9086]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5070.0
Loss: 0.014148877002298832
Action 0 - predicted reward: tensor([[0.7864]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1271]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7145.0
Loss: 0.05075914040207863
Action 0 - predicted reward: tensor([[-0.0279]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.1808]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8980.0
Loss: 0.017173508182168007
Greedy
Action 0 - predicted reward: tensor([[-0.0351]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.9112]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5510.0
Loss: 0.016385996714234352
Action 0 - predicted reward: tensor([[-0.1093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9048]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20040.0
Loss: 0.01551173347979784
Action 0 - predicted reward: tensor([[-0.0844]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9924]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6180.0
Loss: 3.665267286123708e-05
Action 0 - predicted reward: tensor([[0.1195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.9185]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6360.0
Loss: 0.011179805733263493
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4806]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0632]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6440.0
Loss: 8421.248046875
KL Divergence: 18.475486755371094
Action 0 - predicted reward: tensor([[2.4177]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4807]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6445.0
Loss: 6328.82080078125
KL Divergence: 18.49646759033203
Action 0 - predicted reward: tensor([[2.4165]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4766]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6755.0
Loss: 9912.904296875
KL Divergence: 18.411823272705078
Action 0 - predicted reward: tensor([[2.4608]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4368]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6140.0
Loss: 10511.12109375
KL Divergence: 18.343212127685547
8799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-3.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1231]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9380.0
Loss: 0.03089357353746891
Action 0 - predicted reward: tensor([[0.2299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7920]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10775.0
Loss: 0.06659059971570969
Action 0 - predicted reward: tensor([[-0.0273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7594]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8880.0
Loss: 0.0285284835845232
Action 0 - predicted reward: tensor([[0.1022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2922]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10530.0
Loss: 0.05051777511835098
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0191]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.6558]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6500.0
Loss: 0.02440955676138401
Action 0 - predicted reward: tensor([[-0.0503]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9960]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5140.0
Loss: 0.017678143456578255
Action 0 - predicted reward: tensor([[-1.6754]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2712]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7220.0
Loss: 0.05040803551673889
Action 0 - predicted reward: tensor([[-0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.9014]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9020.0
Loss: 0.021107610315084457
Greedy
Action 0 - predicted reward: tensor([[-0.0013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.8162]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5545.0
Loss: 0.01726960390806198
Action 0 - predicted reward: tensor([[-0.1594]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0462]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20100.0
Loss: 0.014783350750803947
Action 0 - predicted reward: tensor([[0.0151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1029]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6215.0
Loss: 0.0013146370183676481
Action 0 - predicted reward: tensor([[0.0198]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9292]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6360.0
Loss: 0.007611742243170738
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4688]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5310]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6480.0
Loss: 8121.8251953125
KL Divergence: 18.46361541748047
Action 0 - predicted reward: tensor([[2.3985]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4616]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6460.0
Loss: 6029.9609375
KL Divergence: 18.473400115966797
Action 0 - predicted reward: tensor([[2.4723]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5302]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6755.0
Loss: 9610.625
KL Divergence: 18.412832260131836
Action 0 - predicted reward: tensor([[2.4318]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7322]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6140.0
Loss: 10501.6923828125
KL Divergence: 18.32662582397461
8899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0308]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.2364]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9390.0
Loss: 0.030675169080495834
Action 0 - predicted reward: tensor([[0.1510]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9284]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10880.0
Loss: 0.06528287380933762
Action 0 - predicted reward: tensor([[0.0755]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.1185]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9020.0
Loss: 0.025971077382564545
Action 0 - predicted reward: tensor([[0.1644]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.8038]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10575.0
Loss: 0.04022079333662987
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.7376]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1659]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6570.0
Loss: 0.029081840068101883
Action 0 - predicted reward: tensor([[-0.3643]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8050]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5140.0
Loss: 0.013918129727244377
Action 0 - predicted reward: tensor([[-0.1516]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.9736]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7290.0
Loss: 0.04814981296658516
Action 0 - predicted reward: tensor([[-0.0117]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[9.5674]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 9055.0
Loss: 0.03069687820971012
Greedy
Action 0 - predicted reward: tensor([[0.3173]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5545.0
Loss: 0.012206834740936756
Action 0 - predicted reward: tensor([[-0.1005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0956]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20115.0
Loss: 0.013583647087216377
Action 0 - predicted reward: tensor([[0.0982]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2058]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6250.0
Loss: 0.0001359822490485385
Action 0 - predicted reward: tensor([[0.0231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1105]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6430.0
Loss: 0.014482748694717884
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4330]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0406]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6480.0
Loss: 8121.15625
KL Divergence: 18.44740104675293
Action 0 - predicted reward: tensor([[2.4345]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5010]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6465.0
Loss: 6029.3896484375
KL Divergence: 18.471179962158203
Action 0 - predicted reward: tensor([[2.4310]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4098]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6755.0
Loss: 9615.3017578125
KL Divergence: 18.390653610229492
Action 0 - predicted reward: tensor([[2.4569]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6169]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 10207.7373046875
KL Divergence: 18.31180191040039
8999.
Epsilon Greedy 5%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9465.0
Loss: 0.03181379660964012
Action 0 - predicted reward: tensor([[-0.1729]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.2074]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11000.0
Loss: 0.06368038058280945
Action 0 - predicted reward: tensor([[0.0472]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9756]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9060.0
Loss: 0.02282995730638504
Action 0 - predicted reward: tensor([[-0.0005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0843]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10615.0
Loss: 0.03945187106728554
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0848]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.7191]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6570.0
Loss: 0.024675529450178146
Action 0 - predicted reward: tensor([[-0.0228]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.7208]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5140.0
Loss: 0.014080452732741833
Action 0 - predicted reward: tensor([[-0.9612]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1577]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7360.0
Loss: 0.04349668696522713
Action 0 - predicted reward: tensor([[0.2221]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-68.4251]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9075.0
Loss: 0.02159523032605648
Greedy
Action 0 - predicted reward: tensor([[-0.0349]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-73.0574]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5545.0
Loss: 0.012185702100396156
Action 0 - predicted reward: tensor([[-0.0262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7582]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20160.0
Loss: 0.012088874354958534
Action 0 - predicted reward: tensor([[-0.6202]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9948]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6285.0
Loss: 0.0009432480437681079
Action 0 - predicted reward: tensor([[-0.2816]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9928]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6430.0
Loss: 0.014394070953130722
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4468]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7723]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6495.0
Loss: 7822.95361328125
KL Divergence: 18.430461883544922
Action 0 - predicted reward: tensor([[2.4417]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2070]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6465.0
Loss: 6030.5712890625
KL Divergence: 18.46306800842285
Action 0 - predicted reward: tensor([[2.4360]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8076]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6790.0
Loss: 9614.56640625
KL Divergence: 18.373628616333008
Action 0 - predicted reward: tensor([[2.4491]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5643]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 9914.0947265625
KL Divergence: 18.292404174804688
9099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.5076]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0844]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9465.0
Loss: 0.0312152411788702
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11115.0
Loss: 0.06393541395664215
Action 0 - predicted reward: tensor([[0.2935]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.4978]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9060.0
Loss: 0.018603798002004623
Action 0 - predicted reward: tensor([[0.0023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.1486]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10655.0
Loss: 0.038202349096536636
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1446]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6640.0
Loss: 0.026634274050593376
Action 0 - predicted reward: tensor([[0.0057]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9588]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5150.0
Loss: 0.013749458827078342
Action 0 - predicted reward: tensor([[0.0212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.8961]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7435.0
Loss: 0.044683102518320084
Action 0 - predicted reward: tensor([[0.7403]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2036]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9115.0
Loss: 0.020567506551742554
Greedy
Action 0 - predicted reward: tensor([[0.2859]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9536]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5580.0
Loss: 0.012633200734853745
Action 0 - predicted reward: tensor([[0.9319]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1873]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20180.0
Loss: 0.01142914965748787
Action 0 - predicted reward: tensor([[0.0065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.8822]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6320.0
Loss: 5.199640872888267e-05
Action 0 - predicted reward: tensor([[0.0493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8342]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6430.0
Loss: 0.014280580915510654
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4779]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5337]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6500.0
Loss: 7822.71875
KL Divergence: 18.412633895874023
Action 0 - predicted reward: tensor([[2.4317]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4897]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6475.0
Loss: 6024.4267578125
KL Divergence: 18.444639205932617
Action 0 - predicted reward: tensor([[2.4628]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5325]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6795.0
Loss: 9615.748046875
KL Divergence: 18.36813735961914
Action 0 - predicted reward: tensor([[2.4851]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5992]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 9316.66796875
KL Divergence: 18.280651092529297
9199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0458]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1278]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9465.0
Loss: 0.027449676766991615
Action 0 - predicted reward: tensor([[0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.7721]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11230.0
Loss: 0.060476209968328476
Action 0 - predicted reward: tensor([[0.2892]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7975]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9165.0
Loss: 0.028272686526179314
Action 0 - predicted reward: tensor([[0.0330]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1455]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10695.0
Loss: 0.035653844475746155
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.4718]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6640.0
Loss: 0.024243919178843498
Action 0 - predicted reward: tensor([[0.6506]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0208]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5150.0
Loss: 0.01353220734745264
Action 0 - predicted reward: tensor([[-1.1225]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5480]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7545.0
Loss: 0.048844028264284134
Action 0 - predicted reward: tensor([[0.0051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.2674]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9120.0
Loss: 0.020553074777126312
Greedy
Action 0 - predicted reward: tensor([[0.0444]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2993]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5580.0
Loss: 0.012484508566558361
Action 0 - predicted reward: tensor([[0.1459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1104]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20240.0
Loss: 0.010669494979083538
Action 0 - predicted reward: tensor([[0.0201]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.8044]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6355.0
Loss: 0.004927909933030605
Action 0 - predicted reward: tensor([[-0.1360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1112]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6470.0
Loss: 0.015015736222267151
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4511]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5213]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6540.0
Loss: 8121.45166015625
KL Divergence: 18.392139434814453
Action 0 - predicted reward: tensor([[2.4147]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9858]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6475.0
Loss: 6029.39794921875
KL Divergence: 18.435062408447266
Action 0 - predicted reward: tensor([[2.4927]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2784]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6795.0
Loss: 9611.4091796875
KL Divergence: 18.349376678466797
Action 0 - predicted reward: tensor([[2.5123]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2259]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 9008.103515625
KL Divergence: 18.271608352661133
9299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1157]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0188]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9510.0
Loss: 0.021716464310884476
Action 0 - predicted reward: tensor([[-0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[8.9415]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 11320.0
Loss: 0.06693769246339798
Action 0 - predicted reward: tensor([[-0.3274]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8752]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9275.0
Loss: 0.024550987407565117
Action 0 - predicted reward: tensor([[0.0073]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.1940]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10730.0
Loss: 0.03210536018013954
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1808]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7167]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6710.0
Loss: 0.026256605982780457
Action 0 - predicted reward: tensor([[-0.0137]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.5913]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5195.0
Loss: 0.01876254566013813
Action 0 - predicted reward: tensor([[0.0858]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.7930]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7580.0
Loss: 0.04311852529644966
Action 0 - predicted reward: tensor([[-1.0301]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9150]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9125.0
Loss: 0.020119559019804
Greedy
Action 0 - predicted reward: tensor([[0.0482]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9534]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5615.0
Loss: 0.012434487231075764
Action 0 - predicted reward: tensor([[0.1418]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1569]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20260.0
Loss: 0.007821070961654186
Action 0 - predicted reward: tensor([[-0.0644]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.1905]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6355.0
Loss: 0.0042011188343167305
Action 0 - predicted reward: tensor([[0.0075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.0667]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6470.0
Loss: 0.014373923651874065
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4472]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5109]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6545.0
Loss: 8120.74560546875
KL Divergence: 18.373714447021484
Action 0 - predicted reward: tensor([[2.4365]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1407]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6480.0
Loss: 5730.78125
KL Divergence: 18.42473602294922
Action 0 - predicted reward: tensor([[2.4640]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8520]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6795.0
Loss: 9615.8671875
KL Divergence: 18.33624267578125
Action 0 - predicted reward: tensor([[2.4777]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2208]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 9017.658203125
KL Divergence: 18.249460220336914
9399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9372]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9620.0
Loss: 0.02934829518198967
Action 0 - predicted reward: tensor([[-0.0027]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.9547]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11360.0
Loss: 0.05454394966363907
Action 0 - predicted reward: tensor([[-0.1971]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8854]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9380.0
Loss: 0.025518260896205902
Action 0 - predicted reward: tensor([[0.1041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0811]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10810.0
Loss: 0.03051915019750595
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2331]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9096]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6710.0
Loss: 0.024682408198714256
Action 0 - predicted reward: tensor([[-0.0094]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.1993]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5270.0
Loss: 0.021854279562830925
Action 0 - predicted reward: tensor([[0.3866]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1196]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7615.0
Loss: 0.04370615631341934
Action 0 - predicted reward: tensor([[0.0554]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3208]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9240.0
Loss: 0.028801511973142624
Greedy
Action 0 - predicted reward: tensor([[-0.0290]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.2763]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5650.0
Loss: 0.011986942030489445
Action 0 - predicted reward: tensor([[-0.0384]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0405]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20280.0
Loss: 0.006779288407415152
Action 0 - predicted reward: tensor([[0.0396]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.2078]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6355.0
Loss: 0.0038939807564020157
Action 0 - predicted reward: tensor([[-0.0096]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1601]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6540.0
Loss: 0.01897851750254631
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4648]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8537]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6545.0
Loss: 8120.37646484375
KL Divergence: 18.373464584350586
Action 0 - predicted reward: tensor([[2.4553]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5144]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6480.0
Loss: 5731.51611328125
KL Divergence: 18.412504196166992
Action 0 - predicted reward: tensor([[2.5112]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1272]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6795.0
Loss: 9615.068359375
KL Divergence: 18.328388214111328
Action 0 - predicted reward: tensor([[2.4578]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6226]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 9017.8681640625
KL Divergence: 18.237071990966797
9499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3015]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9625.0
Loss: 0.017838822677731514
Action 0 - predicted reward: tensor([[-0.9505]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.2333]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11535.0
Loss: 0.07180400937795639
Action 0 - predicted reward: tensor([[0.1397]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6307]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9520.0
Loss: 0.027519620954990387
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10895.0
Loss: 0.029372725635766983
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0649]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4303]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6750.0
Loss: 0.021472129970788956
Action 0 - predicted reward: tensor([[-0.1001]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.9698]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5280.0
Loss: 0.020728183910250664
Action 0 - predicted reward: tensor([[-0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.8850]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7720.0
Loss: 0.05040797218680382
Action 0 - predicted reward: tensor([[0.3331]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9225]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9320.0
Loss: 0.029333194717764854
Greedy
Action 0 - predicted reward: tensor([[0.1980]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9992]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5650.0
Loss: 0.012134827673435211
Action 0 - predicted reward: tensor([[0.0645]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0511]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20320.0
Loss: 0.006168465130031109
Action 0 - predicted reward: tensor([[0.0644]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.8586]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6390.0
Loss: 0.007482453715056181
Action 0 - predicted reward: tensor([[0.0368]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.2430]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6555.0
Loss: 0.015068444423377514
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4345]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1057]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6545.0
Loss: 8121.3974609375
KL Divergence: 18.364212036132812
Action 0 - predicted reward: tensor([[2.4462]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3657]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6490.0
Loss: 5731.14990234375
KL Divergence: 18.40454864501953
Action 0 - predicted reward: tensor([[2.5018]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1637]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6800.0
Loss: 9615.16796875
KL Divergence: 18.30733299255371
Action 0 - predicted reward: tensor([[2.4795]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5273]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6150.0
Loss: 8714.4970703125
KL Divergence: 18.21855354309082
9599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1703]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9665.0
Loss: 0.01793716289103031
Action 0 - predicted reward: tensor([[0.8610]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6811]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11610.0
Loss: 0.062242452055215836
Action 0 - predicted reward: tensor([[-0.0299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.7057]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9560.0
Loss: 0.028241638094186783
Action 0 - predicted reward: tensor([[0.0818]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1895]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10970.0
Loss: 0.028368189930915833
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0222]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6750.0
Loss: 0.021081754937767982
Action 0 - predicted reward: tensor([[-0.0846]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.1579]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5360.0
Loss: 0.02101861499249935
Action 0 - predicted reward: tensor([[0.0385]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.8215]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7755.0
Loss: 0.049976397305727005
Action 0 - predicted reward: tensor([[-0.9950]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8926]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9390.0
Loss: 0.023737091571092606
Greedy
Action 0 - predicted reward: tensor([[-0.0118]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0617]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5685.0
Loss: 0.018024787306785583
Action 0 - predicted reward: tensor([[0.2698]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9957]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20365.0
Loss: 0.005055720917880535
Action 0 - predicted reward: tensor([[0.0452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0820]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6390.0
Loss: 0.007111023645848036
Action 0 - predicted reward: tensor([[0.1998]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0565]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6555.0
Loss: 0.014340179972350597
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4185]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3286]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6580.0
Loss: 8417.5078125
KL Divergence: 18.34404182434082
Action 0 - predicted reward: tensor([[2.4071]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9783]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6505.0
Loss: 5730.5205078125
KL Divergence: 18.3883113861084
Action 0 - predicted reward: tensor([[2.4935]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2277]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6835.0
Loss: 9615.2724609375
KL Divergence: 18.305316925048828
Action 0 - predicted reward: tensor([[2.4965]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5425]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6150.0
Loss: 8719.3701171875
KL Divergence: 18.187999725341797
9699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4501]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9384]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9775.0
Loss: 0.025717873126268387
Action 0 - predicted reward: tensor([[0.5063]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.5316]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11725.0
Loss: 0.062369558960199356
Action 0 - predicted reward: tensor([[-0.1155]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1551]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9635.0
Loss: 0.02949335239827633
Action 0 - predicted reward: tensor([[0.6422]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8472]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11080.0
Loss: 0.03370972350239754
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3895]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2253]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6820.0
Loss: 0.023816034197807312
Action 0 - predicted reward: tensor([[0.0344]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.9873]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5410.0
Loss: 0.017813891172409058
Action 0 - predicted reward: tensor([[-0.0153]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.6782]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7935.0
Loss: 0.0604449138045311
Action 0 - predicted reward: tensor([[0.0092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.3179]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9500.0
Loss: 0.02484538033604622
Greedy
Action 0 - predicted reward: tensor([[0.3908]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0622]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5720.0
Loss: 0.010828979313373566
Action 0 - predicted reward: tensor([[-0.0319]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8951]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20415.0
Loss: 0.004137700889259577
Action 0 - predicted reward: tensor([[0.0001]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.5494]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6390.0
Loss: 0.0071113682352006435
Action 0 - predicted reward: tensor([[0.0268]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.2597]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6555.0
Loss: 0.014574105851352215
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4450]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4963]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6580.0
Loss: 8418.1865234375
KL Divergence: 18.328022003173828
Action 0 - predicted reward: tensor([[2.4356]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6145]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6515.0
Loss: 5731.2275390625
KL Divergence: 18.377649307250977
Action 0 - predicted reward: tensor([[2.4659]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2966]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6835.0
Loss: 9316.6357421875
KL Divergence: 18.280765533447266
Action 0 - predicted reward: tensor([[2.4985]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5549]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6155.0
Loss: 8714.3173828125
KL Divergence: 18.18988609313965
9799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0768]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7282]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9860.0
Loss: 0.02816171571612358
Action 0 - predicted reward: tensor([[0.0870]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.4824]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11770.0
Loss: 0.05397622287273407
Action 0 - predicted reward: tensor([[-0.0954]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.3222]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9710.0
Loss: 0.03126455470919609
Action 0 - predicted reward: tensor([[-0.1606]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1991]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11085.0
Loss: 0.028867632150650024
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0155]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.8611]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6890.0
Loss: 0.020115336403250694
Action 0 - predicted reward: tensor([[-0.0100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0886]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5450.0
Loss: 0.014650428667664528
Action 0 - predicted reward: tensor([[-0.1219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.5567]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7970.0
Loss: 0.05046340078115463
Action 0 - predicted reward: tensor([[-0.0183]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.4064]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9590.0
Loss: 0.03230134770274162
Greedy
Action 0 - predicted reward: tensor([[-0.0703]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.6355]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5790.0
Loss: 0.013836498372256756
Action 0 - predicted reward: tensor([[-0.0576]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.5725]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20475.0
Loss: 0.009655351750552654
Action 0 - predicted reward: tensor([[0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.2389]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6425.0
Loss: 0.011495115235447884
Action 0 - predicted reward: tensor([[-0.1810]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0205]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6595.0
Loss: 0.012084487825632095
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4207]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4800]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6585.0
Loss: 7822.2587890625
KL Divergence: 18.30728530883789
Action 0 - predicted reward: tensor([[2.4658]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5191]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6520.0
Loss: 5731.5302734375
KL Divergence: 18.36420440673828
Action 0 - predicted reward: tensor([[2.4727]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0865]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6835.0
Loss: 9316.3740234375
KL Divergence: 18.263381958007812
Action 0 - predicted reward: tensor([[2.5035]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5479]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6155.0
Loss: 8719.26953125
KL Divergence: 18.176170349121094
9899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1696]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.8538]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10010.0
Loss: 0.03362065181136131
Action 0 - predicted reward: tensor([[0.3621]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.2283]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11910.0
Loss: 0.054664865136146545
Action 0 - predicted reward: tensor([[-0.2394]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.2396]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9825.0
Loss: 0.03985489159822464
Action 0 - predicted reward: tensor([[0.0048]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0815]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11155.0
Loss: 0.030650056898593903
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.2928]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6965.0
Loss: 0.018171142786741257
Action 0 - predicted reward: tensor([[0.0797]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.9810]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5490.0
Loss: 0.015596079640090466
Action 0 - predicted reward: tensor([[-0.1583]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.3129]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8005.0
Loss: 0.052005551755428314
Action 0 - predicted reward: tensor([[-6.8380]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1073]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9625.0
Loss: 0.026217970997095108
Greedy
Action 0 - predicted reward: tensor([[0.0718]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0257]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5790.0
Loss: 0.01089932769536972
Action 0 - predicted reward: tensor([[0.0288]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0792]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20510.0
Loss: 0.007771678734570742
Action 0 - predicted reward: tensor([[0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.8269]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6495.0
Loss: 0.01821037009358406
Action 0 - predicted reward: tensor([[0.2079]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9915]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6595.0
Loss: 0.011115417815744877
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4621]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7784]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6600.0
Loss: 7523.76513671875
KL Divergence: 18.305391311645508
Action 0 - predicted reward: tensor([[2.4472]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5117]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6530.0
Loss: 5731.55224609375
KL Divergence: 18.35221290588379
Action 0 - predicted reward: tensor([[2.4790]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1338]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6835.0
Loss: 9316.78125
KL Divergence: 18.24500846862793
Action 0 - predicted reward: tensor([[2.5209]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5666]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6155.0
Loss: 8719.4501953125
KL Divergence: 18.167207717895508
9999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1211]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.9279]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10010.0
Loss: 0.032815121114254
Action 0 - predicted reward: tensor([[0.2128]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1009]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11990.0
Loss: 0.06349298357963562
Action 0 - predicted reward: tensor([[-0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.6980]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9905.0
Loss: 0.035245850682258606
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11260.0
Loss: 0.023111358284950256
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.2751]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6965.0
Loss: 0.01771845482289791
Action 0 - predicted reward: tensor([[0.3086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0944]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5530.0
Loss: 0.019473206251859665
Action 0 - predicted reward: tensor([[-0.0548]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.8668]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8040.0
Loss: 0.051023781299591064
Action 0 - predicted reward: tensor([[-3.4422]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9129]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9660.0
Loss: 0.032522670924663544
Greedy
Action 0 - predicted reward: tensor([[0.0706]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0893]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5825.0
Loss: 0.010837791487574577
Action 0 - predicted reward: tensor([[0.0553]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.2734]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20540.0
Loss: 0.007311433553695679
Action 0 - predicted reward: tensor([[0.0011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.1911]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6495.0
Loss: 0.014053115621209145
Action 0 - predicted reward: tensor([[0.0044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0322]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6595.0
Loss: 0.010897955857217312
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4439]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9211]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6605.0
Loss: 7524.18798828125
KL Divergence: 18.299625396728516
Action 0 - predicted reward: tensor([[2.4398]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6961]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6535.0
Loss: 5731.119140625
KL Divergence: 18.34573745727539
Action 0 - predicted reward: tensor([[2.4934]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5625]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6835.0
Loss: 9316.810546875
KL Divergence: 18.23092269897461
Action 0 - predicted reward: tensor([[2.5017]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5424]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6155.0
Loss: 8713.150390625
KL Divergence: 18.153287887573242
10099.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 10195.0
Loss: 0.04008237645030022
Action 0 - predicted reward: tensor([[0.0351]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.2602]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12045.0
Loss: 0.05695244297385216
Action 0 - predicted reward: tensor([[-0.0493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.4158]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9915.0
Loss: 0.03439434990286827
Action 0 - predicted reward: tensor([[0.0810]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.0397]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11330.0
Loss: 0.02817564085125923
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0654]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.5959]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7035.0
Loss: 0.0152833741158247
Action 0 - predicted reward: tensor([[-0.1350]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3183]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5635.0
Loss: 0.023745065554976463
Action 0 - predicted reward: tensor([[0.0705]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.5461]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8110.0
Loss: 0.04862532019615173
Action 0 - predicted reward: tensor([[1.2403]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0485]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9695.0
Loss: 0.031539712101221085
Greedy
Action 0 - predicted reward: tensor([[-0.0486]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1080]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5825.0
Loss: 0.010773921385407448
Action 0 - predicted reward: tensor([[0.0763]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.6060]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20590.0
Loss: 0.007243906147778034
Action 0 - predicted reward: tensor([[0.0565]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9633]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6495.0
Loss: 0.013745685108006
Action 0 - predicted reward: tensor([[0.0706]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0090]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6595.0
Loss: 0.009793048724532127
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4465]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5342]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6610.0
Loss: 7223.34912109375
KL Divergence: 18.291624069213867
Action 0 - predicted reward: tensor([[2.4479]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5020]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6535.0
Loss: 5729.53466796875
KL Divergence: 18.338611602783203
Action 0 - predicted reward: tensor([[2.4593]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0423]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6835.0
Loss: 9312.90625
KL Divergence: 18.2188720703125
Action 0 - predicted reward: tensor([[2.5105]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5588]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6165.0
Loss: 8417.87890625
KL Divergence: 18.149616241455078
10199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0389]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5408]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10315.0
Loss: 0.03450573980808258
Action 0 - predicted reward: tensor([[-3.3655]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0981]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12080.0
Loss: 0.060352668166160583
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10045.0
Loss: 0.037352174520492554
Action 0 - predicted reward: tensor([[0.0277]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9661]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11365.0
Loss: 0.027309516444802284
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.8179]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7075.0
Loss: 0.01434994488954544
Action 0 - predicted reward: tensor([[-0.0672]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.4467]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5745.0
Loss: 0.027133408933877945
Action 0 - predicted reward: tensor([[-0.5476]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0776]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8185.0
Loss: 0.051976997405290604
Action 0 - predicted reward: tensor([[0.0169]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.3168]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9730.0
Loss: 0.03445590287446976
Greedy
Action 0 - predicted reward: tensor([[0.1722]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0166]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5825.0
Loss: 0.010725402273237705
Action 0 - predicted reward: tensor([[-0.0328]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.4577]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20615.0
Loss: 0.007024046499282122
Action 0 - predicted reward: tensor([[0.0009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6664]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6495.0
Loss: 0.013623643666505814
Action 0 - predicted reward: tensor([[-0.0457]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-83.5090]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6605.0
Loss: 0.009746839292347431
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4538]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9149]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6615.0
Loss: 7224.00048828125
KL Divergence: 18.289478302001953
Action 0 - predicted reward: tensor([[2.4204]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0790]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6545.0
Loss: 5729.12255859375
KL Divergence: 18.33584976196289
Action 0 - predicted reward: tensor([[2.4524]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1879]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6835.0
Loss: 9314.0791015625
KL Divergence: 18.217451095581055
Action 0 - predicted reward: tensor([[2.4838]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1083]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6170.0
Loss: 8119.7783203125
KL Divergence: 18.14847755432129
10299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4651]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9242]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10360.0
Loss: 0.03187203034758568
Action 0 - predicted reward: tensor([[0.6974]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.3496]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12195.0
Loss: 0.060515500605106354
Action 0 - predicted reward: tensor([[-0.0028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5915]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10045.0
Loss: 0.0340925008058548
Action 0 - predicted reward: tensor([[0.0328]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0514]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11365.0
Loss: 0.026651298627257347
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1107]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6866]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7145.0
Loss: 0.01773947663605213
Action 0 - predicted reward: tensor([[-0.0964]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3330]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5785.0
Loss: 0.02205471508204937
Action 0 - predicted reward: tensor([[-0.0381]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0900]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8190.0
Loss: 0.04617321491241455
Action 0 - predicted reward: tensor([[-0.7014]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4866]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9770.0
Loss: 0.02820536307990551
Greedy
Action 0 - predicted reward: tensor([[0.1358]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8021]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5825.0
Loss: 0.009727544151246548
Action 0 - predicted reward: tensor([[-0.0271]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.8143]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20625.0
Loss: 0.006904876325279474
Action 0 - predicted reward: tensor([[0.1102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9532]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 0.016857830807566643
Action 0 - predicted reward: tensor([[-0.0749]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9904]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6645.0
Loss: 0.013057658448815346
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4728]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5199]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6615.0
Loss: 7221.9580078125
KL Divergence: 18.295166015625
Action 0 - predicted reward: tensor([[2.4275]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5019]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6545.0
Loss: 5729.69580078125
KL Divergence: 18.327180862426758
Action 0 - predicted reward: tensor([[2.4733]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5293]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6835.0
Loss: 9015.77734375
KL Divergence: 18.211118698120117
Action 0 - predicted reward: tensor([[2.4999]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5523]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6170.0
Loss: 8118.59765625
KL Divergence: 18.146398544311523
10399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.7454]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10370.0
Loss: 0.03149406239390373
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 12315.0
Loss: 0.06345389038324356
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10045.0
Loss: 0.04195063188672066
Action 0 - predicted reward: tensor([[0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.2236]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11475.0
Loss: 0.03347687050700188
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0209]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3045]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7260.0
Loss: 0.020542621612548828
Action 0 - predicted reward: tensor([[-0.0587]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9875]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5800.0
Loss: 0.02042890340089798
Action 0 - predicted reward: tensor([[-0.4089]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.3526]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8230.0
Loss: 0.040928952395915985
Action 0 - predicted reward: tensor([[0.8065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3452]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9805.0
Loss: 0.02698606066405773
Greedy
Action 0 - predicted reward: tensor([[0.3356]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8945]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5825.0
Loss: 0.009623880498111248
Action 0 - predicted reward: tensor([[0.1025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2085]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20695.0
Loss: 0.011714457534253597
Action 0 - predicted reward: tensor([[0.0291]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.4111]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6600.0
Loss: 0.013508805073797703
Action 0 - predicted reward: tensor([[0.0244]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.8037]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6685.0
Loss: 0.010474879294633865
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4410]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5079]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6615.0
Loss: 7223.7568359375
KL Divergence: 18.282712936401367
Action 0 - predicted reward: tensor([[2.4303]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3246]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6550.0
Loss: 5431.095703125
KL Divergence: 18.339094161987305
Action 0 - predicted reward: tensor([[2.4867]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5344]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6835.0
Loss: 9010.3974609375
KL Divergence: 18.21479034423828
Action 0 - predicted reward: tensor([[2.4865]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8219]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6170.0
Loss: 8119.9287109375
KL Divergence: 18.147314071655273
10499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.1540]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10405.0
Loss: 0.026279710233211517
Action 0 - predicted reward: tensor([[-6.0412]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9737]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12465.0
Loss: 0.06578690558671951
Action 0 - predicted reward: tensor([[0.0884]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0107]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10050.0
Loss: 0.03730415925383568
Action 0 - predicted reward: tensor([[0.0912]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.6096]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11595.0
Loss: 0.029401108622550964
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0421]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.2640]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7295.0
Loss: 0.0143770482391119
Action 0 - predicted reward: tensor([[-0.2209]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0674]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5805.0
Loss: 0.01794658601284027
Action 0 - predicted reward: tensor([[-0.1125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.0771]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8340.0
Loss: 0.03541535884141922
Action 0 - predicted reward: tensor([[-8.3138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9821]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9915.0
Loss: 0.028492463752627373
Greedy
Action 0 - predicted reward: tensor([[-0.0284]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.2064]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5825.0
Loss: 0.009603611193597317
Action 0 - predicted reward: tensor([[0.0638]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0555]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20730.0
Loss: 0.010686763562262058
Action 0 - predicted reward: tensor([[-0.0282]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.0201]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6600.0
Loss: 0.013404041528701782
Action 0 - predicted reward: tensor([[0.0907]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9715]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6685.0
Loss: 0.006742121186107397
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4250]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6113]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 7222.75244140625
KL Divergence: 18.28382682800293
Action 0 - predicted reward: tensor([[2.4227]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0411]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6560.0
Loss: 5429.92919921875
KL Divergence: 18.329538345336914
Action 0 - predicted reward: tensor([[2.4479]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5179]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6870.0
Loss: 9313.857421875
KL Divergence: 18.20696258544922
Action 0 - predicted reward: tensor([[2.4702]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5788]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6170.0
Loss: 8119.271484375
KL Divergence: 18.140966415405273
10599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9653]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10550.0
Loss: 0.03166532889008522
Action 0 - predicted reward: tensor([[4.5944]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7513]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12590.0
Loss: 0.06738253682851791
Action 0 - predicted reward: tensor([[0.1614]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1394]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10095.0
Loss: 0.03760974481701851
Action 0 - predicted reward: tensor([[-0.0074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0015]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11710.0
Loss: 0.03536158427596092
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.1978]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7330.0
Loss: 0.014095312915742397
Action 0 - predicted reward: tensor([[0.2284]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.3394]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5875.0
Loss: 0.020708182826638222
Action 0 - predicted reward: tensor([[-0.0258]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.6951]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8375.0
Loss: 0.03793070837855339
Action 0 - predicted reward: tensor([[-8.6304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9401]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9915.0
Loss: 0.02534577250480652
Greedy
Action 0 - predicted reward: tensor([[0.0075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.2136]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5860.0
Loss: 0.011564858257770538
Action 0 - predicted reward: tensor([[0.0068]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8549]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 20755.0
Loss: 0.010488899424672127
Action 0 - predicted reward: tensor([[0.0028]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.6458]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6600.0
Loss: 0.013274628669023514
Action 0 - predicted reward: tensor([[0.0235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9603]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6685.0
Loss: 0.00668461574241519
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4347]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7963]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6620.0
Loss: 7223.1962890625
KL Divergence: 18.28496551513672
Action 0 - predicted reward: tensor([[2.4520]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5121]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6565.0
Loss: 5430.26416015625
KL Divergence: 18.33207893371582
Action 0 - predicted reward: tensor([[2.4263]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1175]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6870.0
Loss: 9313.146484375
KL Divergence: 18.199542999267578
Action 0 - predicted reward: tensor([[2.4998]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5521]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6170.0
Loss: 8120.00341796875
KL Divergence: 18.141870498657227
10699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9174]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10670.0
Loss: 0.03342515602707863
Action 0 - predicted reward: tensor([[-0.1494]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.7994]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12735.0
Loss: 0.07089565694332123
Action 0 - predicted reward: tensor([[0.0260]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.7106]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10095.0
Loss: 0.033386483788490295
Action 0 - predicted reward: tensor([[0.0261]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.8120]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11720.0
Loss: 0.032911404967308044
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2322]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2985]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7330.0
Loss: 0.016333764418959618
Action 0 - predicted reward: tensor([[0.1727]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9835]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5875.0
Loss: 0.015026248060166836
Action 0 - predicted reward: tensor([[-0.0695]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.6381]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8375.0
Loss: 0.03724726662039757
Action 0 - predicted reward: tensor([[-11.5754]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0319]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9990.0
Loss: 0.022107014432549477
Greedy
Action 0 - predicted reward: tensor([[0.0085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.7495]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5860.0
Loss: 0.009654002264142036
Action 0 - predicted reward: tensor([[0.0152]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.0141]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20805.0
Loss: 0.013539411127567291
Action 0 - predicted reward: tensor([[0.0190]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.4388]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6600.0
Loss: 0.013166134245693684
Action 0 - predicted reward: tensor([[-0.0635]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-76.0584]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6685.0
Loss: 0.00655199121683836
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4392]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8971]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6625.0
Loss: 7223.5625
KL Divergence: 18.281295776367188
Action 0 - predicted reward: tensor([[2.4321]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8224]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6575.0
Loss: 5430.880859375
KL Divergence: 18.32643699645996
Action 0 - predicted reward: tensor([[2.4307]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8983]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6870.0
Loss: 9311.939453125
KL Divergence: 18.20512580871582
Action 0 - predicted reward: tensor([[2.4914]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5406]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6170.0
Loss: 8115.11865234375
KL Divergence: 18.14131736755371
10799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0483]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.0065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10755.0
Loss: 0.03280138969421387
Action 0 - predicted reward: tensor([[-0.3572]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.0958]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12780.0
Loss: 0.0690227821469307
Action 0 - predicted reward: tensor([[-0.0734]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8613]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10145.0
Loss: 0.03639102354645729
Action 0 - predicted reward: tensor([[0.0396]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.7575]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11725.0
Loss: 0.03425968438386917
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3917]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0769]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7370.0
Loss: 0.01829623617231846
Action 0 - predicted reward: tensor([[0.0893]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8267]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5915.0
Loss: 0.015527038834989071
Action 0 - predicted reward: tensor([[-0.0428]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0004]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8415.0
Loss: 0.03950286656618118
Action 0 - predicted reward: tensor([[-13.8181]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9278]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10100.0
Loss: 0.030661029741168022
Greedy
Action 0 - predicted reward: tensor([[0.0507]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9949]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5860.0
Loss: 0.009640651755034924
Action 0 - predicted reward: tensor([[-0.1257]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9580]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20810.0
Loss: 0.013665261678397655
Action 0 - predicted reward: tensor([[0.0416]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.4028]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6635.0
Loss: 0.010138439014554024
Action 0 - predicted reward: tensor([[-0.4399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9617]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6720.0
Loss: 0.009198985062539577
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4354]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4767]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6625.0
Loss: 7223.14453125
KL Divergence: 18.282814025878906
Action 0 - predicted reward: tensor([[2.4307]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1770]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6585.0
Loss: 5431.44384765625
KL Divergence: 18.326881408691406
Action 0 - predicted reward: tensor([[2.4413]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5122]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6870.0
Loss: 9010.1083984375
KL Divergence: 18.20102310180664
Action 0 - predicted reward: tensor([[2.4915]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5544]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6170.0
Loss: 8119.3671875
KL Divergence: 18.137758255004883
10899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0436]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.3542]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10905.0
Loss: 0.03286378085613251
Action 0 - predicted reward: tensor([[-2.4229]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7551]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12905.0
Loss: 0.0643533393740654
Action 0 - predicted reward: tensor([[0.2676]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8568]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10225.0
Loss: 0.03986451402306557
Action 0 - predicted reward: tensor([[0.0271]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-82.1376]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11795.0
Loss: 0.031080830842256546
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5347]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1487]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7375.0
Loss: 0.017986411228775978
Action 0 - predicted reward: tensor([[-0.0432]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0243]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5950.0
Loss: 0.018129806965589523
Action 0 - predicted reward: tensor([[0.0289]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.6284]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 8450.0
Loss: 0.04199805110692978
Action 0 - predicted reward: tensor([[0.0355]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.7240]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10105.0
Loss: 0.022369487211108208
Greedy
Action 0 - predicted reward: tensor([[0.2966]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.7526]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5895.0
Loss: 0.011963066644966602
Action 0 - predicted reward: tensor([[-0.0673]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9987]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20810.0
Loss: 0.013632159680128098
Action 0 - predicted reward: tensor([[-0.1306]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9866]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 0.009649144485592842
Action 0 - predicted reward: tensor([[0.0262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.5513]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6755.0
Loss: 0.011352840811014175
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4545]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5205]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6625.0
Loss: 7223.4228515625
KL Divergence: 18.280092239379883
Action 0 - predicted reward: tensor([[2.4274]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7967]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6640.0
Loss: 5729.458984375
KL Divergence: 18.324974060058594
Action 0 - predicted reward: tensor([[2.4281]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9605]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6870.0
Loss: 9013.7099609375
KL Divergence: 18.19629669189453
Action 0 - predicted reward: tensor([[2.4929]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5506]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6170.0
Loss: 8119.63623046875
KL Divergence: 18.1368408203125
10999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1524]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.0904]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10905.0
Loss: 0.02880876138806343
Action 0 - predicted reward: tensor([[-0.5838]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[8.1021]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 13110.0
Loss: 0.07196525484323502
Action 0 - predicted reward: tensor([[0.0417]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9808]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10305.0
Loss: 0.039507973939180374
Action 0 - predicted reward: tensor([[0.2639]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8960]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11875.0
Loss: 0.026615895330905914
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1786]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[8.5162]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7485.0
Loss: 0.021968232467770576
Action 0 - predicted reward: tensor([[0.0986]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.1689]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6020.0
Loss: 0.014027523808181286
Action 0 - predicted reward: tensor([[-0.0283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.6139]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8520.0
Loss: 0.0338965468108654
Action 0 - predicted reward: tensor([[-0.0699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.1777]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10105.0
Loss: 0.021529505029320717
Greedy
Action 0 - predicted reward: tensor([[0.2012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7097]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5895.0
Loss: 0.010818051174283028
Action 0 - predicted reward: tensor([[-0.0615]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.7405]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20825.0
Loss: 0.013455934822559357
Action 0 - predicted reward: tensor([[0.0189]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.6601]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6635.0
Loss: 0.009799374267458916
Action 0 - predicted reward: tensor([[0.0963]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0192]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6825.0
Loss: 0.006794645916670561
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4664]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5249]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6625.0
Loss: 7223.65234375
KL Divergence: 18.279359817504883
Action 0 - predicted reward: tensor([[2.4237]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4854]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6675.0
Loss: 6026.443359375
KL Divergence: 18.31882667541504
Action 0 - predicted reward: tensor([[2.4285]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4846]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6905.0
Loss: 9306.705078125
KL Divergence: 18.187585830688477
Action 0 - predicted reward: tensor([[2.4866]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5415]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6175.0
Loss: 8119.669921875
KL Divergence: 18.127422332763672
11099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0323]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6881]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11020.0
Loss: 0.03514783829450607
Action 0 - predicted reward: tensor([[0.1073]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.0619]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13160.0
Loss: 0.058461934328079224
Action 0 - predicted reward: tensor([[-0.0508]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.2962]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10485.0
Loss: 0.051199302077293396
Action 0 - predicted reward: tensor([[0.0410]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.4662]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11985.0
Loss: 0.026575280353426933
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0373]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.7234]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7485.0
Loss: 0.01088983379304409
Action 0 - predicted reward: tensor([[0.0897]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9782]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6130.0
Loss: 0.023979730904102325
Action 0 - predicted reward: tensor([[-0.0324]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.2447]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8635.0
Loss: 0.038019221276044846
Action 0 - predicted reward: tensor([[-3.1130]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0518]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10145.0
Loss: 0.024876942858099937
Greedy
Action 0 - predicted reward: tensor([[-0.0304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.1075]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5895.0
Loss: 0.010409553535282612
Action 0 - predicted reward: tensor([[0.1269]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9831]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20860.0
Loss: 0.01756419986486435
Action 0 - predicted reward: tensor([[-0.2531]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9721]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 0.009700572118163109
Action 0 - predicted reward: tensor([[0.0216]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-76.7008]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6825.0
Loss: 0.006516349036246538
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4429]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6569]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6630.0
Loss: 7222.88330078125
KL Divergence: 18.279062271118164
Action 0 - predicted reward: tensor([[2.4293]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4810]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6680.0
Loss: 6027.12841796875
KL Divergence: 18.318557739257812
Action 0 - predicted reward: tensor([[2.4234]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4882]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6905.0
Loss: 9312.06640625
KL Divergence: 18.18896484375
Action 0 - predicted reward: tensor([[2.4572]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6017]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6210.0
Loss: 8119.08349609375
KL Divergence: 18.135480880737305
11199.
Epsilon Greedy 5%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 11100.0
Loss: 0.03439130261540413
Action 0 - predicted reward: tensor([[0.0756]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.8036]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13265.0
Loss: 0.05741932615637779
Action 0 - predicted reward: tensor([[-0.0284]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0859]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10625.0
Loss: 0.05427192151546478
Action 0 - predicted reward: tensor([[-0.0372]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.1928]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12165.0
Loss: 0.029447132721543312
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.3352]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7560.0
Loss: 0.017590362578630447
Action 0 - predicted reward: tensor([[0.0615]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.8017]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6170.0
Loss: 0.022707689553499222
Action 0 - predicted reward: tensor([[-0.0540]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.0595]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8740.0
Loss: 0.03800247237086296
Action 0 - predicted reward: tensor([[0.0018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.0020]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10220.0
Loss: 0.026748549193143845
Greedy
Action 0 - predicted reward: tensor([[0.2348]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0692]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5930.0
Loss: 0.014683489687740803
Action 0 - predicted reward: tensor([[-0.0620]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.1162]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20870.0
Loss: 0.012755891308188438
Action 0 - predicted reward: tensor([[0.0135]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8679]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6670.0
Loss: 0.009734081104397774
Action 0 - predicted reward: tensor([[-0.0291]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.6103]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6860.0
Loss: 0.003394311759620905
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4603]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8463]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6630.0
Loss: 6925.984375
KL Divergence: 18.28175926208496
Action 0 - predicted reward: tensor([[2.4041]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1311]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6685.0
Loss: 6025.98876953125
KL Divergence: 18.31719398498535
Action 0 - predicted reward: tensor([[2.4353]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5041]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6905.0
Loss: 8717.0966796875
KL Divergence: 18.19441032409668
Action 0 - predicted reward: tensor([[2.4842]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5374]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6210.0
Loss: 8115.150390625
KL Divergence: 18.131446838378906
11299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2568]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0277]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11105.0
Loss: 0.0340430922806263
Action 0 - predicted reward: tensor([[-0.2363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.1256]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13405.0
Loss: 0.05979377031326294
Action 0 - predicted reward: tensor([[-0.0848]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2489]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10805.0
Loss: 0.051718272268772125
Action 0 - predicted reward: tensor([[0.2468]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0144]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12270.0
Loss: 0.025512460619211197
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.6666]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0741]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7670.0
Loss: 0.021243417635560036
Action 0 - predicted reward: tensor([[-0.0092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-85.6244]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6245.0
Loss: 0.026662034913897514
Action 0 - predicted reward: tensor([[-0.0236]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6028]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8775.0
Loss: 0.03897308185696602
Action 0 - predicted reward: tensor([[0.9636]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0797]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10295.0
Loss: 0.02534429170191288
Greedy
Action 0 - predicted reward: tensor([[0.0033]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.1753]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5930.0
Loss: 0.013651739805936813
Action 0 - predicted reward: tensor([[0.0671]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.8057]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20885.0
Loss: 0.01077340543270111
Action 0 - predicted reward: tensor([[-0.0271]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.3449]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6705.0
Loss: 0.009689025580883026
Action 0 - predicted reward: tensor([[0.0533]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.5751]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6860.0
Loss: 0.003307412378489971
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4892]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5394]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6630.0
Loss: 6627.232421875
KL Divergence: 18.282495498657227
Action 0 - predicted reward: tensor([[2.4047]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2169]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6690.0
Loss: 6026.9873046875
KL Divergence: 18.32173728942871
Action 0 - predicted reward: tensor([[2.4628]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0191]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6905.0
Loss: 8113.94482421875
KL Divergence: 18.1987361907959
Action 0 - predicted reward: tensor([[2.4895]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5386]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6215.0
Loss: 8118.529296875
KL Divergence: 18.130361557006836
11399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3486]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8893]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11185.0
Loss: 0.03458785265684128
Action 0 - predicted reward: tensor([[-2.8179]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9835]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13550.0
Loss: 0.05566522851586342
Action 0 - predicted reward: tensor([[-0.0226]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.7871]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10925.0
Loss: 0.05182533711194992
Action 0 - predicted reward: tensor([[0.0658]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4000]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12380.0
Loss: 0.03056824579834938
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0504]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.7096]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7705.0
Loss: 0.018260572105646133
Action 0 - predicted reward: tensor([[-0.0802]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-82.0032]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6250.0
Loss: 0.021488461643457413
Action 0 - predicted reward: tensor([[-0.9840]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1243]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8820.0
Loss: 0.03785010054707527
Action 0 - predicted reward: tensor([[0.7178]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0639]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10370.0
Loss: 0.04032975062727928
Greedy
Action 0 - predicted reward: tensor([[0.2092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9688]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5930.0
Loss: 0.010264181531965733
Action 0 - predicted reward: tensor([[-0.0313]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7270]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 20935.0
Loss: 0.014007105492055416
Action 0 - predicted reward: tensor([[-0.1261]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9924]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6705.0
Loss: 0.009641473181545734
Action 0 - predicted reward: tensor([[0.0393]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9352]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6860.0
Loss: 0.0033866011071950197
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4720]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5277]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6630.0
Loss: 6626.52880859375
KL Divergence: 18.274778366088867
Action 0 - predicted reward: tensor([[2.3878]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0818]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6695.0
Loss: 6023.859375
KL Divergence: 18.315324783325195
Action 0 - predicted reward: tensor([[2.4613]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5308]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6905.0
Loss: 8119.4609375
KL Divergence: 18.194721221923828
Action 0 - predicted reward: tensor([[2.4969]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5477]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6215.0
Loss: 8119.08154296875
KL Divergence: 18.134885787963867
11499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.9551]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11230.0
Loss: 0.031224658712744713
Action 0 - predicted reward: tensor([[0.5318]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9660]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13675.0
Loss: 0.057218100875616074
Action 0 - predicted reward: tensor([[0.0358]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.8435]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11085.0
Loss: 0.058306898921728134
Action 0 - predicted reward: tensor([[0.1790]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4007]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12455.0
Loss: 0.028324944898486137
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0410]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9861]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7845.0
Loss: 0.031514912843704224
Action 0 - predicted reward: tensor([[0.0302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.7694]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6255.0
Loss: 0.021632835268974304
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8895.0
Loss: 0.034329257905483246
Action 0 - predicted reward: tensor([[2.3153]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9679]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10385.0
Loss: 0.03714349493384361
Greedy
Action 0 - predicted reward: tensor([[0.2667]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8997]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5930.0
Loss: 0.009972551837563515
Action 0 - predicted reward: tensor([[0.0156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1521]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 20945.0
Loss: 0.012575355358421803
Action 0 - predicted reward: tensor([[-0.0076]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6571]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6705.0
Loss: 0.009635214693844318
Action 0 - predicted reward: tensor([[0.0298]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6823]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6865.0
Loss: 0.0033296106848865747
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4573]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7777]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6630.0
Loss: 6626.7646484375
KL Divergence: 18.280609130859375
Action 0 - predicted reward: tensor([[2.3912]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9641]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6695.0
Loss: 6025.22998046875
KL Divergence: 18.31238555908203
Action 0 - predicted reward: tensor([[2.4701]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5295]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6905.0
Loss: 8118.96337890625
KL Divergence: 18.189931869506836
Action 0 - predicted reward: tensor([[2.4709]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9056]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6215.0
Loss: 8119.7646484375
KL Divergence: 18.126773834228516
11599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1716]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9833]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11300.0
Loss: 0.03347497060894966
Action 0 - predicted reward: tensor([[-2.6344]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7234]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13720.0
Loss: 0.04998692870140076
Action 0 - predicted reward: tensor([[-0.2824]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4081]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11130.0
Loss: 0.053145915269851685
Action 0 - predicted reward: tensor([[0.1543]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0297]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12605.0
Loss: 0.029234914109110832
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0714]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.8685]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7915.0
Loss: 0.02822030335664749
Action 0 - predicted reward: tensor([[0.1731]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0438]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6290.0
Loss: 0.021462013944983482
Action 0 - predicted reward: tensor([[0.0216]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-85.1017]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8895.0
Loss: 0.032600391656160355
Action 0 - predicted reward: tensor([[0.0618]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.6740]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10490.0
Loss: 0.039766788482666016
Greedy
Action 0 - predicted reward: tensor([[0.2988]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9512]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5930.0
Loss: 0.0071090469136834145
Action 0 - predicted reward: tensor([[0.2042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.1019]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21025.0
Loss: 0.015920177102088928
Action 0 - predicted reward: tensor([[0.0246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.0342]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6740.0
Loss: 0.013272972777485847
Action 0 - predicted reward: tensor([[-0.0572]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.1343]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6865.0
Loss: 0.0032737439032644033
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4502]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3382]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6630.0
Loss: 6625.96533203125
KL Divergence: 18.272523880004883
Action 0 - predicted reward: tensor([[2.3985]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2679]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6705.0
Loss: 6025.091796875
KL Divergence: 18.312145233154297
Action 0 - predicted reward: tensor([[2.4461]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8934]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6905.0
Loss: 8119.0927734375
KL Divergence: 18.186376571655273
Action 0 - predicted reward: tensor([[2.4730]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7759]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6220.0
Loss: 7820.38818359375
KL Divergence: 18.134855270385742
11699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.2975]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11305.0
Loss: 0.02998662367463112
Action 0 - predicted reward: tensor([[-4.8389]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6455]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13755.0
Loss: 0.0370219461619854
Action 0 - predicted reward: tensor([[0.0246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.0518]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11180.0
Loss: 0.05191593989729881
Action 0 - predicted reward: tensor([[-0.0473]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2476]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12645.0
Loss: 0.024577658623456955
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0657]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.6656]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7950.0
Loss: 0.02623111382126808
Action 0 - predicted reward: tensor([[0.0077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9413]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6290.0
Loss: 0.020033087581396103
Action 0 - predicted reward: tensor([[-1.1285]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0880]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9000.0
Loss: 0.03615318238735199
Action 0 - predicted reward: tensor([[-5.6353]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0606]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10530.0
Loss: 0.04452439025044441
Greedy
Action 0 - predicted reward: tensor([[-0.0189]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.8439]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5965.0
Loss: 0.01065574586391449
Action 0 - predicted reward: tensor([[0.0715]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9077]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21065.0
Loss: 0.01679946668446064
Action 0 - predicted reward: tensor([[-0.0016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.2317]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6775.0
Loss: 0.013195029459893703
Action 0 - predicted reward: tensor([[-0.2121]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9589]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6865.0
Loss: 0.003250790061429143
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4735]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5383]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 6626.609375
KL Divergence: 18.28080940246582
Action 0 - predicted reward: tensor([[2.4010]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9515]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6715.0
Loss: 6025.8544921875
KL Divergence: 18.317794799804688
Action 0 - predicted reward: tensor([[2.4648]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5293]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6940.0
Loss: 7815.58642578125
KL Divergence: 18.18496322631836
Action 0 - predicted reward: tensor([[2.5054]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5508]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 7521.9248046875
KL Divergence: 18.125484466552734
11799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5461]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5804]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11315.0
Loss: 0.029854053631424904
Action 0 - predicted reward: tensor([[1.8309]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2006]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13795.0
Loss: 0.038853004574775696
Action 0 - predicted reward: tensor([[-0.0930]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5576]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11230.0
Loss: 0.04801149666309357
Action 0 - predicted reward: tensor([[-0.0668]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0896]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12720.0
Loss: 0.02330392226576805
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2345]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0381]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7985.0
Loss: 0.026348615065217018
Action 0 - predicted reward: tensor([[0.0572]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.1441]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6395.0
Loss: 0.02445286512374878
Action 0 - predicted reward: tensor([[-0.0593]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.0507]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9140.0
Loss: 0.034663498401641846
Action 0 - predicted reward: tensor([[0.0072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.7629]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10535.0
Loss: 0.035796359181404114
Greedy
Action 0 - predicted reward: tensor([[-0.1365]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0876]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6000.0
Loss: 0.011517132632434368
Action 0 - predicted reward: tensor([[-0.1475]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9861]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21155.0
Loss: 0.01681382581591606
Action 0 - predicted reward: tensor([[0.2238]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0783]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6845.0
Loss: 0.011322247795760632
Action 0 - predicted reward: tensor([[0.0977]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0236]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6870.0
Loss: 0.003226132597774267
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4943]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5456]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 6626.45068359375
KL Divergence: 18.27528953552246
Action 0 - predicted reward: tensor([[2.4309]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4975]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6725.0
Loss: 6027.87158203125
KL Divergence: 18.31562614440918
Action 0 - predicted reward: tensor([[2.4520]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1309]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6975.0
Loss: 7821.23583984375
KL Divergence: 18.182476043701172
Action 0 - predicted reward: tensor([[2.5121]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5578]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 7521.9208984375
KL Divergence: 18.12958335876465
11899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0458]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.8503]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11395.0
Loss: 0.029588237404823303
Action 0 - predicted reward: tensor([[-9.7094]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8606]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13875.0
Loss: 0.038598865270614624
Action 0 - predicted reward: tensor([[0.1246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0871]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11300.0
Loss: 0.048643045127391815
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12800.0
Loss: 0.023416759446263313
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1772]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.6101]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8025.0
Loss: 0.026088058948516846
Action 0 - predicted reward: tensor([[0.0164]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.4595]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6470.0
Loss: 0.026628103107213974
Action 0 - predicted reward: tensor([[-0.7496]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6917]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9210.0
Loss: 0.028606710955500603
Action 0 - predicted reward: tensor([[-0.0388]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8092]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10575.0
Loss: 0.03654695302248001
Greedy
Action 0 - predicted reward: tensor([[-0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.7251]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6000.0
Loss: 0.009307089261710644
Action 0 - predicted reward: tensor([[-0.2338]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8005]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21195.0
Loss: 0.01264313142746687
Action 0 - predicted reward: tensor([[0.0138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.2894]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6880.0
Loss: 0.010210911743342876
Action 0 - predicted reward: tensor([[0.0893]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3644]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6905.0
Loss: 0.00544141698628664
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4922]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5487]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 6626.95166015625
KL Divergence: 18.27301788330078
Action 0 - predicted reward: tensor([[2.4256]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1051]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6740.0
Loss: 6028.271484375
KL Divergence: 18.317611694335938
Action 0 - predicted reward: tensor([[2.4667]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5242]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6980.0
Loss: 7820.8046875
KL Divergence: 18.189966201782227
Action 0 - predicted reward: tensor([[2.4817]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7623]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 7521.73974609375
KL Divergence: 18.128246307373047
11999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7840]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11475.0
Loss: 0.032829735428094864
Action 0 - predicted reward: tensor([[-0.1315]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7895]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 13955.0
Loss: 0.041021350771188736
Action 0 - predicted reward: tensor([[-0.0323]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.0470]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11410.0
Loss: 0.053984664380550385
Action 0 - predicted reward: tensor([[-0.1273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0237]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12905.0
Loss: 0.028857054188847542
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.9952]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1308]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8060.0
Loss: 0.02256670780479908
Action 0 - predicted reward: tensor([[-0.0399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-86.9120]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6575.0
Loss: 0.027303894981741905
Action 0 - predicted reward: tensor([[0.2157]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1415]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9210.0
Loss: 0.02801208756864071
Action 0 - predicted reward: tensor([[-0.0935]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9209]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10645.0
Loss: 0.03522057086229324
Greedy
Action 0 - predicted reward: tensor([[0.0139]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.3390]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6000.0
Loss: 0.006184528581798077
Action 0 - predicted reward: tensor([[0.1648]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2173]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21200.0
Loss: 0.011679112911224365
Action 0 - predicted reward: tensor([[0.0252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.7405]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6880.0
Loss: 0.01017665583640337
Action 0 - predicted reward: tensor([[0.0152]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.6925]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6905.0
Loss: 0.0032681250013411045
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4846]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5375]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 6625.58984375
KL Divergence: 18.271060943603516
Action 0 - predicted reward: tensor([[2.4492]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5168]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6755.0
Loss: 6028.41064453125
KL Divergence: 18.31599998474121
Action 0 - predicted reward: tensor([[2.4411]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0429]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6980.0
Loss: 7820.65869140625
KL Divergence: 18.19097137451172
Action 0 - predicted reward: tensor([[2.4942]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5526]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 7516.03759765625
KL Divergence: 18.123916625976562
12099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0345]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.1776]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11550.0
Loss: 0.027895938605070114
Action 0 - predicted reward: tensor([[0.3536]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7365]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14030.0
Loss: 0.030870597809553146
Action 0 - predicted reward: tensor([[0.2549]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6255]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11645.0
Loss: 0.057888686656951904
Action 0 - predicted reward: tensor([[0.0108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0096]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12910.0
Loss: 0.02827867679297924
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0033]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9232]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8060.0
Loss: 0.01547160279005766
Action 0 - predicted reward: tensor([[0.2303]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0692]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6580.0
Loss: 0.025302141904830933
Action 0 - predicted reward: tensor([[0.2869]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1038]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9315.0
Loss: 0.026894833892583847
Action 0 - predicted reward: tensor([[0.0373]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.9557]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10720.0
Loss: 0.033461134880781174
Greedy
Action 0 - predicted reward: tensor([[0.0290]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6853]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6000.0
Loss: 0.005081404000520706
Action 0 - predicted reward: tensor([[-0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1319]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21200.0
Loss: 0.011430026032030582
Action 0 - predicted reward: tensor([[0.0443]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0054]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6880.0
Loss: 0.010107285343110561
Action 0 - predicted reward: tensor([[0.1012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0233]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6910.0
Loss: 0.0032414470333606005
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4858]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5461]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 6626.3330078125
KL Divergence: 18.27475929260254
Action 0 - predicted reward: tensor([[2.4368]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9097]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6755.0
Loss: 6023.98583984375
KL Divergence: 18.320110321044922
Action 0 - predicted reward: tensor([[2.4519]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5114]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6980.0
Loss: 7819.20263671875
KL Divergence: 18.197751998901367
Action 0 - predicted reward: tensor([[2.5081]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5595]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 7223.091796875
KL Divergence: 18.132020950317383
12199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8870]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11585.0
Loss: 0.027156468480825424
Action 0 - predicted reward: tensor([[0.0389]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2593]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14105.0
Loss: 0.03272150456905365
Action 0 - predicted reward: tensor([[0.1458]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.0297]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11785.0
Loss: 0.061150796711444855
Action 0 - predicted reward: tensor([[-0.1155]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1597]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12910.0
Loss: 0.027781803160905838
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[1.0125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0189]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8095.0
Loss: 0.013774240389466286
Action 0 - predicted reward: tensor([[0.0467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0701]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6590.0
Loss: 0.025244075804948807
Action 0 - predicted reward: tensor([[-3.8191]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5354]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 9460.0
Loss: 0.03860194981098175
Action 0 - predicted reward: tensor([[0.8146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1399]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10800.0
Loss: 0.03226509317755699
Greedy
Action 0 - predicted reward: tensor([[0.0516]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2770]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6000.0
Loss: 0.004106834996491671
Action 0 - predicted reward: tensor([[0.2332]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5017]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21285.0
Loss: 0.015641113743185997
Action 0 - predicted reward: tensor([[-0.0329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8990]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6880.0
Loss: 0.010047459974884987
Action 0 - predicted reward: tensor([[-0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.3499]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6915.0
Loss: 0.0014055719366297126
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4832]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5474]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 6626.61474609375
KL Divergence: 18.273963928222656
Action 0 - predicted reward: tensor([[2.4591]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5242]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6760.0
Loss: 6029.21533203125
KL Divergence: 18.319765090942383
Action 0 - predicted reward: tensor([[2.4569]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5045]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6980.0
Loss: 7820.36279296875
KL Divergence: 18.192718505859375
Action 0 - predicted reward: tensor([[2.4881]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8485]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 7223.08349609375
KL Divergence: 18.127527236938477
12299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1421]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1511]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11660.0
Loss: 0.03753320500254631
Action 0 - predicted reward: tensor([[0.1431]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.5826]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14220.0
Loss: 0.033183738589286804
Action 0 - predicted reward: tensor([[0.0727]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2137]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11900.0
Loss: 0.057192351669073105
Action 0 - predicted reward: tensor([[0.1341]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2481]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12985.0
Loss: 0.0285270344465971
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3890]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8664]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8130.0
Loss: 0.01413934025913477
Action 0 - predicted reward: tensor([[0.0332]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.5947]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6660.0
Loss: 0.034223124384880066
Action 0 - predicted reward: tensor([[-0.1333]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.5690]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9460.0
Loss: 0.032330773770809174
Action 0 - predicted reward: tensor([[-0.1430]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2252]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10840.0
Loss: 0.03251717984676361
Greedy
Action 0 - predicted reward: tensor([[0.0453]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.1842]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6000.0
Loss: 0.003521837992593646
Action 0 - predicted reward: tensor([[-0.5604]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9412]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21290.0
Loss: 0.011735865846276283
Action 0 - predicted reward: tensor([[-0.0253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9774]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6880.0
Loss: 0.009969796054065228
Action 0 - predicted reward: tensor([[-0.2796]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9969]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6915.0
Loss: 2.2450143660535105e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4807]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5379]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 6626.46484375
KL Divergence: 18.269174575805664
Action 0 - predicted reward: tensor([[2.4638]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5320]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6770.0
Loss: 6029.0634765625
KL Divergence: 18.321359634399414
Action 0 - predicted reward: tensor([[2.4325]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9757]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 7521.6474609375
KL Divergence: 18.195201873779297
Action 0 - predicted reward: tensor([[2.4892]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8111]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 6924.861328125
KL Divergence: 18.1256103515625
12399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0427]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2100]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11735.0
Loss: 0.034845560789108276
Action 0 - predicted reward: tensor([[-0.2949]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.0101]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14265.0
Loss: 0.02515731006860733
Action 0 - predicted reward: tensor([[-0.0382]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8031]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12010.0
Loss: 0.06487350910902023
Action 0 - predicted reward: tensor([[-0.0310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0245]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13090.0
Loss: 0.03041786141693592
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.2501]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8165.0
Loss: 0.013544167391955853
Action 0 - predicted reward: tensor([[0.0270]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.3148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6660.0
Loss: 0.03296529874205589
Action 0 - predicted reward: tensor([[-0.2816]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.1081]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9495.0
Loss: 0.02854839153587818
Action 0 - predicted reward: tensor([[-0.0639]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5161]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10915.0
Loss: 0.03883267194032669
Greedy
Action 0 - predicted reward: tensor([[0.0105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8789]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6035.0
Loss: 0.005285690538585186
Action 0 - predicted reward: tensor([[0.0482]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0894]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21300.0
Loss: 0.01109846867620945
Action 0 - predicted reward: tensor([[-0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.4493]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6880.0
Loss: 0.009906809777021408
Action 0 - predicted reward: tensor([[-0.0550]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9743]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6955.0
Loss: 0.00484909163787961
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4735]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5371]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6635.0
Loss: 6626.2763671875
KL Divergence: 18.268104553222656
Action 0 - predicted reward: tensor([[2.4922]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5467]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6770.0
Loss: 6028.72216796875
KL Divergence: 18.322050094604492
Action 0 - predicted reward: tensor([[2.4615]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5232]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 7223.1953125
KL Divergence: 18.197294235229492
Action 0 - predicted reward: tensor([[2.5036]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6510]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 6620.26220703125
KL Divergence: 18.124391555786133
12499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2798]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0629]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11740.0
Loss: 0.03219614550471306
Action 0 - predicted reward: tensor([[-0.0784]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6568]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14410.0
Loss: 0.02696075476706028
Action 0 - predicted reward: tensor([[-0.0777]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-70.9779]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12055.0
Loss: 0.0654434859752655
Action 0 - predicted reward: tensor([[-0.0094]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9792]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13165.0
Loss: 0.024067102000117302
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.1370]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8200.0
Loss: 0.014069914817810059
Action 0 - predicted reward: tensor([[0.1231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1417]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6665.0
Loss: 0.02906554378569126
Action 0 - predicted reward: tensor([[0.1332]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1097]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9535.0
Loss: 0.024845410138368607
Action 0 - predicted reward: tensor([[0.1247]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.1692]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10950.0
Loss: 0.034578170627355576
Greedy
Action 0 - predicted reward: tensor([[-0.0372]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.4656]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6035.0
Loss: 0.00020100612891837955
Action 0 - predicted reward: tensor([[-0.0450]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.0583]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21320.0
Loss: 0.0108343455940485
Action 0 - predicted reward: tensor([[-0.2051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0791]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6880.0
Loss: 0.009898179210722446
Action 0 - predicted reward: tensor([[-0.0231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-68.3294]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6965.0
Loss: 0.004012807738035917
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4640]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7875]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6635.0
Loss: 6622.0556640625
KL Divergence: 18.26604461669922
Action 0 - predicted reward: tensor([[2.4916]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5478]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6775.0
Loss: 6029.37255859375
KL Divergence: 18.32166290283203
Action 0 - predicted reward: tensor([[2.4516]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8823]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 7223.27587890625
KL Divergence: 18.19472885131836
Action 0 - predicted reward: tensor([[2.5065]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6843]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 6325.89404296875
KL Divergence: 18.126928329467773
12599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3466]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3000]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11815.0
Loss: 0.0355103500187397
Action 0 - predicted reward: tensor([[0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.7201]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14480.0
Loss: 0.029950978234410286
Action 0 - predicted reward: tensor([[-0.0508]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.5654]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12170.0
Loss: 0.06232040375471115
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13235.0
Loss: 0.027912629768252373
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2103]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9337]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8200.0
Loss: 0.012953020632266998
Action 0 - predicted reward: tensor([[0.0279]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0354]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6745.0
Loss: 0.03743216022849083
Action 0 - predicted reward: tensor([[-0.4117]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1992]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9535.0
Loss: 0.02149825170636177
Action 0 - predicted reward: tensor([[-0.0289]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.0864]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11000.0
Loss: 0.030993575230240822
Greedy
Action 0 - predicted reward: tensor([[0.1750]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0282]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6035.0
Loss: 8.071593038039282e-05
Action 0 - predicted reward: tensor([[-0.5841]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9825]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21335.0
Loss: 0.007618891075253487
Action 0 - predicted reward: tensor([[-0.0052]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.4645]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6920.0
Loss: 0.009831065312027931
Action 0 - predicted reward: tensor([[-0.0290]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.2269]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6965.0
Loss: 0.004064332693815231
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5001]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5488]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6640.0
Loss: 6626.2783203125
KL Divergence: 18.2669677734375
Action 0 - predicted reward: tensor([[2.4634]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9127]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6780.0
Loss: 6029.66357421875
KL Divergence: 18.324562072753906
Action 0 - predicted reward: tensor([[2.4521]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0513]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 7224.51904296875
KL Divergence: 18.189001083374023
Action 0 - predicted reward: tensor([[2.5150]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2300]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 6324.86083984375
KL Divergence: 18.12525749206543
12699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1915]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8281]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11885.0
Loss: 0.04189349710941315
Action 0 - predicted reward: tensor([[0.2029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3005]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14590.0
Loss: 0.033377375453710556
Action 0 - predicted reward: tensor([[-0.2092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.7057]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12250.0
Loss: 0.06121907755732536
Action 0 - predicted reward: tensor([[0.1334]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.4235]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13305.0
Loss: 0.03230123594403267
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2850]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9270]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8275.0
Loss: 0.01446959562599659
Action 0 - predicted reward: tensor([[-0.0856]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9379]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6795.0
Loss: 0.029197292402386665
Action 0 - predicted reward: tensor([[0.0724]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.7138]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9535.0
Loss: 0.01742461696267128
Action 0 - predicted reward: tensor([[0.0009]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.8266]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11075.0
Loss: 0.03195099160075188
Greedy
Action 0 - predicted reward: tensor([[0.2011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9989]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6070.0
Loss: 8.09568737167865e-05
Action 0 - predicted reward: tensor([[0.8877]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9725]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21350.0
Loss: 0.007358888164162636
Action 0 - predicted reward: tensor([[-0.0434]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0414]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6955.0
Loss: 0.006407720968127251
Action 0 - predicted reward: tensor([[-0.0077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.7579]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6965.0
Loss: 0.003949253354221582
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4871]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5447]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6645.0
Loss: 6327.49072265625
KL Divergence: 18.269092559814453
Action 0 - predicted reward: tensor([[2.4618]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1784]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6785.0
Loss: 6024.68310546875
KL Divergence: 18.31775665283203
Action 0 - predicted reward: tensor([[2.4915]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5447]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 7223.58544921875
KL Divergence: 18.190359115600586
Action 0 - predicted reward: tensor([[2.5239]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8850]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 6025.3017578125
KL Divergence: 18.13237762451172
12799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3835]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7113]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11995.0
Loss: 0.03248799219727516
Action 0 - predicted reward: tensor([[-0.1419]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.8182]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14745.0
Loss: 0.03640985116362572
Action 0 - predicted reward: tensor([[-0.1926]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7425]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12395.0
Loss: 0.06018490344285965
Action 0 - predicted reward: tensor([[-0.0072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.0288]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13350.0
Loss: 0.029217572882771492
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1463]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1379]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8275.0
Loss: 0.010166605934500694
Action 0 - predicted reward: tensor([[0.0120]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-70.3927]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6900.0
Loss: 0.03014444373548031
Action 0 - predicted reward: tensor([[0.0565]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.8342]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9535.0
Loss: 0.016472619026899338
Action 0 - predicted reward: tensor([[-0.0069]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.2723]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11215.0
Loss: 0.03513752296566963
Greedy
Action 0 - predicted reward: tensor([[0.1810]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9390]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6070.0
Loss: 4.3230513256276026e-05
Action 0 - predicted reward: tensor([[0.0287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.7489]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21395.0
Loss: 0.008822702802717686
Action 0 - predicted reward: tensor([[0.0136]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-85.9346]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6955.0
Loss: 0.006364455912262201
Action 0 - predicted reward: tensor([[0.0236]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0034]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7000.0
Loss: 0.003846715670078993
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4667]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7507]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6645.0
Loss: 6327.27490234375
KL Divergence: 18.267969131469727
Action 0 - predicted reward: tensor([[2.4838]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5428]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6785.0
Loss: 6029.75390625
KL Divergence: 18.315217971801758
Action 0 - predicted reward: tensor([[2.4724]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9628]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 7222.67529296875
KL Divergence: 18.1822566986084
Action 0 - predicted reward: tensor([[2.5231]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8016]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 5726.8818359375
KL Divergence: 18.12730598449707
12899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4144]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.3812]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12035.0
Loss: 0.03573311120271683
Action 0 - predicted reward: tensor([[-0.0111]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.4309]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14790.0
Loss: 0.03635222092270851
Action 0 - predicted reward: tensor([[0.1104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9728]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12510.0
Loss: 0.0627160593867302
Action 0 - predicted reward: tensor([[-0.1424]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8160]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13395.0
Loss: 0.02372661605477333
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0513]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8275.0
Loss: 0.009855863638222218
Action 0 - predicted reward: tensor([[0.0305]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.1645]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6905.0
Loss: 0.02833736687898636
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9605.0
Loss: 0.018203066661953926
Action 0 - predicted reward: tensor([[0.0001]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.3477]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11400.0
Loss: 0.034053925424814224
Greedy
Action 0 - predicted reward: tensor([[-0.0222]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.0527]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6070.0
Loss: 2.9068291041767225e-05
Action 0 - predicted reward: tensor([[-0.1266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0374]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21430.0
Loss: 0.01273452676832676
Action 0 - predicted reward: tensor([[0.1303]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9442]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6955.0
Loss: 0.006359172519296408
Action 0 - predicted reward: tensor([[-0.2306]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0029]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7000.0
Loss: 0.0038535764906555414
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4863]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8532]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6645.0
Loss: 6028.828125
KL Divergence: 18.267118453979492
Action 0 - predicted reward: tensor([[2.4907]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5467]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6795.0
Loss: 6029.3134765625
KL Divergence: 18.317745208740234
Action 0 - predicted reward: tensor([[2.4899]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5572]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 7223.4775390625
KL Divergence: 18.1824951171875
Action 0 - predicted reward: tensor([[2.5190]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8608]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 5727.94873046875
KL Divergence: 18.121509552001953
12999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0656]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.9143]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12085.0
Loss: 0.035730957984924316
Action 0 - predicted reward: tensor([[0.3347]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.3457]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14845.0
Loss: 0.03540217876434326
Action 0 - predicted reward: tensor([[-0.1953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8630]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12590.0
Loss: 0.05149722099304199
Action 0 - predicted reward: tensor([[0.1329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3091]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13470.0
Loss: 0.023625938221812248
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0912]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6411]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8310.0
Loss: 0.010013873688876629
Action 0 - predicted reward: tensor([[-0.0363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0852]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6925.0
Loss: 0.027719244360923767
Action 0 - predicted reward: tensor([[-0.0040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.3359]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9680.0
Loss: 0.02314300835132599
Action 0 - predicted reward: tensor([[-3.1625]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9706]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11480.0
Loss: 0.02706003747880459
Greedy
Action 0 - predicted reward: tensor([[-0.0160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0483]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6070.0
Loss: 0.00011884580453624949
Action 0 - predicted reward: tensor([[-0.0882]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.4151]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21435.0
Loss: 0.008601531386375427
Action 0 - predicted reward: tensor([[-0.0226]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9890]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6955.0
Loss: 0.0032081990502774715
Action 0 - predicted reward: tensor([[-0.2801]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0198]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7000.0
Loss: 0.003699034685268998
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5142]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5768]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6645.0
Loss: 6027.68994140625
KL Divergence: 18.268598556518555
Action 0 - predicted reward: tensor([[2.4757]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1046]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6795.0
Loss: 6029.3876953125
KL Divergence: 18.309326171875
Action 0 - predicted reward: tensor([[2.4752]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3760]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 7223.69921875
KL Divergence: 18.183406829833984
Action 0 - predicted reward: tensor([[2.5434]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5999]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 5726.5302734375
KL Divergence: 18.128530502319336
13099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2382]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.4620]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12085.0
Loss: 0.0346301794052124
Action 0 - predicted reward: tensor([[0.5010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9847]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14885.0
Loss: 0.03951101750135422
Action 0 - predicted reward: tensor([[0.0922]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9454]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12660.0
Loss: 0.04957525059580803
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 13590.0
Loss: 0.027313586324453354
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1944]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9883]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8345.0
Loss: 0.007994849234819412
Action 0 - predicted reward: tensor([[0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.7536]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 0.03276907652616501
Action 0 - predicted reward: tensor([[-0.3266]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1656]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9685.0
Loss: 0.014808024279773235
Action 0 - predicted reward: tensor([[-5.3585e-05]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.1849]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11555.0
Loss: 0.016244467347860336
Greedy
Action 0 - predicted reward: tensor([[0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.9704]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6070.0
Loss: 3.866922270390205e-05
Action 0 - predicted reward: tensor([[-0.0044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.3965]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21445.0
Loss: 0.007847700268030167
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.9145]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6955.0
Loss: 0.003214219119399786
Action 0 - predicted reward: tensor([[-0.4152]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0435]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7005.0
Loss: 0.0037716059014201164
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5053]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5736]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6650.0
Loss: 6028.8349609375
KL Divergence: 18.269989013671875
Action 0 - predicted reward: tensor([[2.4745]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5403]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6795.0
Loss: 6028.54931640625
KL Divergence: 18.321130752563477
Action 0 - predicted reward: tensor([[2.4823]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5469]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 6923.9384765625
KL Divergence: 18.17683219909668
Action 0 - predicted reward: tensor([[2.5441]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6033]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 5726.48095703125
KL Divergence: 18.113933563232422
13199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.1335]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12125.0
Loss: 0.031179405748844147
Action 0 - predicted reward: tensor([[-0.4010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.0922]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14970.0
Loss: 0.04035855457186699
Action 0 - predicted reward: tensor([[0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9962]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12735.0
Loss: 0.0513988733291626
Action 0 - predicted reward: tensor([[0.1214]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9934]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13650.0
Loss: 0.023501621559262276
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0465]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.3932]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8350.0
Loss: 0.006734561175107956
Action 0 - predicted reward: tensor([[-0.0136]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.8725]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7095.0
Loss: 0.028523530811071396
Action 0 - predicted reward: tensor([[-0.0041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.6129]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9720.0
Loss: 0.013775985687971115
Action 0 - predicted reward: tensor([[-4.8378]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9484]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11665.0
Loss: 0.017204245552420616
Greedy
Action 0 - predicted reward: tensor([[-0.1101]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4629]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6070.0
Loss: 2.8287797249504365e-05
Action 0 - predicted reward: tensor([[0.0974]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.3304]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21455.0
Loss: 0.0072498139925301075
Action 0 - predicted reward: tensor([[-0.0146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.8233]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6960.0
Loss: 0.0032004546374082565
Action 0 - predicted reward: tensor([[0.0451]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.0974]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7005.0
Loss: 0.0035955675411969423
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4936]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7413]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6655.0
Loss: 6027.9111328125
KL Divergence: 18.267841339111328
Action 0 - predicted reward: tensor([[2.4730]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5416]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6805.0
Loss: 6029.783203125
KL Divergence: 18.31201934814453
Action 0 - predicted reward: tensor([[2.4612]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1528]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 6924.787109375
KL Divergence: 18.167919158935547
Action 0 - predicted reward: tensor([[2.5284]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2857]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 5726.90576171875
KL Divergence: 18.120689392089844
13299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0779]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8821]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12165.0
Loss: 0.03088034689426422
Action 0 - predicted reward: tensor([[-0.0831]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4199]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15005.0
Loss: 0.036702122539281845
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 12820.0
Loss: 0.04472973570227623
Action 0 - predicted reward: tensor([[-0.0579]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1846]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13695.0
Loss: 0.027438592165708542
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0901]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0127]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8350.0
Loss: 0.006642685737460852
Action 0 - predicted reward: tensor([[0.1643]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9926]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7100.0
Loss: 0.028638888150453568
Action 0 - predicted reward: tensor([[-0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.4685]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9755.0
Loss: 0.014058228582143784
Action 0 - predicted reward: tensor([[-1.6340]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8495]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11750.0
Loss: 0.019756799563765526
Greedy
Action 0 - predicted reward: tensor([[0.2227]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9464]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6105.0
Loss: 0.0006216289475560188
Action 0 - predicted reward: tensor([[-1.5958]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9739]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21460.0
Loss: 0.0066916486248373985
Action 0 - predicted reward: tensor([[0.0041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.0380]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6960.0
Loss: 0.0031893118284642696
Action 0 - predicted reward: tensor([[0.0379]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9801]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7005.0
Loss: 0.0036892408970743418
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5140]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5601]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6660.0
Loss: 5730.623046875
KL Divergence: 18.269203186035156
Action 0 - predicted reward: tensor([[2.4925]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5362]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6815.0
Loss: 6029.67431640625
KL Divergence: 18.31073570251465
Action 0 - predicted reward: tensor([[2.4713]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5260]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 6924.94482421875
KL Divergence: 18.170696258544922
Action 0 - predicted reward: tensor([[2.5389]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5921]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 5726.55126953125
KL Divergence: 18.12330436706543
13399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0476]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.0559]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12240.0
Loss: 0.03212927281856537
Action 0 - predicted reward: tensor([[-0.0479]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0873]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15110.0
Loss: 0.04114779084920883
Action 0 - predicted reward: tensor([[0.0304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9202]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12900.0
Loss: 0.043733999133110046
Action 0 - predicted reward: tensor([[0.0534]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.6294]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13740.0
Loss: 0.02679675817489624
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0174]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6793]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8420.0
Loss: 0.006637868471443653
Action 0 - predicted reward: tensor([[-0.1755]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9204]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7175.0
Loss: 0.030231492593884468
Action 0 - predicted reward: tensor([[-0.3223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8992]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9790.0
Loss: 0.018229030072689056
Action 0 - predicted reward: tensor([[-0.2547]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8749]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11830.0
Loss: 0.018798910081386566
Greedy
Action 0 - predicted reward: tensor([[0.2563]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9606]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6105.0
Loss: 3.2858933991519734e-05
Action 0 - predicted reward: tensor([[-0.0125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9616]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21465.0
Loss: 0.006015089340507984
Action 0 - predicted reward: tensor([[-0.1608]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9056]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6960.0
Loss: 0.0031879348680377007
Action 0 - predicted reward: tensor([[-0.4642]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0263]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7005.0
Loss: 0.0036137953866273165
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4907]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5350]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6675.0
Loss: 5728.8466796875
KL Divergence: 18.255897521972656
Action 0 - predicted reward: tensor([[2.4556]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5198]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6820.0
Loss: 6029.0810546875
KL Divergence: 18.303335189819336
Action 0 - predicted reward: tensor([[2.4779]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5271]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 6925.52587890625
KL Divergence: 18.16661834716797
Action 0 - predicted reward: tensor([[2.5172]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9611]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 5728.0244140625
KL Divergence: 18.116655349731445
13499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1536]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9775]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12275.0
Loss: 0.024647457525134087
Action 0 - predicted reward: tensor([[0.0297]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[12.5006]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15215.0
Loss: 0.051756732165813446
Action 0 - predicted reward: tensor([[-0.1162]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8042]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12985.0
Loss: 0.04718916118144989
Action 0 - predicted reward: tensor([[0.0302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8274]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13815.0
Loss: 0.028130006045103073
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0315]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.6269]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8460.0
Loss: 0.006609181873500347
Action 0 - predicted reward: tensor([[0.1321]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1375]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7175.0
Loss: 0.02855575457215309
Action 0 - predicted reward: tensor([[0.5693]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6944]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9790.0
Loss: 0.013884559273719788
Action 0 - predicted reward: tensor([[-10.1422]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9948]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11970.0
Loss: 0.029398510232567787
Greedy
Action 0 - predicted reward: tensor([[-0.0093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.3199]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6105.0
Loss: 2.2989092030911706e-05
Action 0 - predicted reward: tensor([[0.4700]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8484]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21470.0
Loss: 0.005774566903710365
Action 0 - predicted reward: tensor([[-0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.1396]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6960.0
Loss: 0.003185362322255969
Action 0 - predicted reward: tensor([[0.0342]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0544]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7010.0
Loss: 0.0034621357917785645
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5089]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5666]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6675.0
Loss: 5730.10546875
KL Divergence: 18.264171600341797
Action 0 - predicted reward: tensor([[2.4534]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9400]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6825.0
Loss: 6028.3798828125
KL Divergence: 18.30296516418457
Action 0 - predicted reward: tensor([[2.4669]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5220]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 6926.013671875
KL Divergence: 18.159269332885742
Action 0 - predicted reward: tensor([[2.5228]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8656]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 5726.9873046875
KL Divergence: 18.115188598632812
13599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.5316]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12285.0
Loss: 0.02397570200264454
Action 0 - predicted reward: tensor([[-0.4687]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2562]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15260.0
Loss: 0.0497422069311142
Action 0 - predicted reward: tensor([[-0.2591]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.1372]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13020.0
Loss: 0.04265785217285156
Action 0 - predicted reward: tensor([[-0.0309]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1560]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13895.0
Loss: 0.023696528747677803
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0616]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.7375]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8500.0
Loss: 0.006628005299717188
Action 0 - predicted reward: tensor([[1.2585]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2855]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7190.0
Loss: 0.025901297107338905
Action 0 - predicted reward: tensor([[0.0127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5271]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9790.0
Loss: 0.009944041259586811
Action 0 - predicted reward: tensor([[-2.2653]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9974]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12010.0
Loss: 0.024657851085066795
Greedy
Action 0 - predicted reward: tensor([[-0.0142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.4398]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6105.0
Loss: 2.0898560251225717e-05
Action 0 - predicted reward: tensor([[2.0707]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1320]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21485.0
Loss: 0.005019993055611849
Action 0 - predicted reward: tensor([[0.0129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.1253]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6965.0
Loss: 0.003156108083203435
Action 0 - predicted reward: tensor([[-0.0241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6328]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7010.0
Loss: 0.003701734822243452
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4948]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5527]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6680.0
Loss: 5729.93115234375
KL Divergence: 18.257675170898438
Action 0 - predicted reward: tensor([[2.4723]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5372]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6850.0
Loss: 6029.7705078125
KL Divergence: 18.29722785949707
Action 0 - predicted reward: tensor([[2.4801]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5280]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 6924.7646484375
KL Divergence: 18.162857055664062
Action 0 - predicted reward: tensor([[2.5625]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6185]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 5724.462890625
KL Divergence: 18.11675262451172
13699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9758]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12370.0
Loss: 0.028159311041235924
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15340.0
Loss: 0.03929313272237778
Action 0 - predicted reward: tensor([[-0.1363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3129]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13090.0
Loss: 0.0458083301782608
Action 0 - predicted reward: tensor([[-0.0705]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0956]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14040.0
Loss: 0.023729832842946053
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.6244]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9723]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8500.0
Loss: 0.00651603052392602
Action 0 - predicted reward: tensor([[-0.0314]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.5833]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7270.0
Loss: 0.025070199742913246
Action 0 - predicted reward: tensor([[-0.0137]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9717]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9790.0
Loss: 0.009774557314813137
Action 0 - predicted reward: tensor([[0.0950]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.1713]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12090.0
Loss: 0.02471872977912426
Greedy
Action 0 - predicted reward: tensor([[-0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.1600]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6105.0
Loss: 1.931145743583329e-05
Action 0 - predicted reward: tensor([[0.0207]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0695]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21505.0
Loss: 0.004886971786618233
Action 0 - predicted reward: tensor([[0.0115]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.3264]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6965.0
Loss: 0.0031602918170392513
Action 0 - predicted reward: tensor([[-0.0347]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0500]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7010.0
Loss: 0.003409239463508129
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4756]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6355]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6685.0
Loss: 5431.427734375
KL Divergence: 18.249914169311523
Action 0 - predicted reward: tensor([[2.4602]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8329]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6860.0
Loss: 6029.466796875
KL Divergence: 18.30643653869629
Action 0 - predicted reward: tensor([[2.4667]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3031]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 6626.41845703125
KL Divergence: 18.16330337524414
Action 0 - predicted reward: tensor([[2.5448]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0116]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 5724.70556640625
KL Divergence: 18.120264053344727
13799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0090]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.1104]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12370.0
Loss: 0.02819349430501461
Action 0 - predicted reward: tensor([[0.5593]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[12.7280]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15550.0
Loss: 0.04491936415433884
Action 0 - predicted reward: tensor([[-0.0304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.7485]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13165.0
Loss: 0.042935121804475784
Action 0 - predicted reward: tensor([[0.0232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3025]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14155.0
Loss: 0.02709317021071911
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4901]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0159]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8575.0
Loss: 0.013399736024439335
Action 0 - predicted reward: tensor([[0.0569]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9521]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7270.0
Loss: 0.024587569758296013
Action 0 - predicted reward: tensor([[-0.0393]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.1526]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9825.0
Loss: 0.01057338248938322
Action 0 - predicted reward: tensor([[-0.2310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.4162]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12160.0
Loss: 0.028673654422163963
Greedy
Action 0 - predicted reward: tensor([[0.0460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.9450]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6140.0
Loss: 0.0018179916078224778
Action 0 - predicted reward: tensor([[0.0875]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.2650]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21590.0
Loss: 0.004864231217652559
Action 0 - predicted reward: tensor([[0.0043]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.4382]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6970.0
Loss: 0.0031547872349619865
Action 0 - predicted reward: tensor([[0.0068]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4607]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7010.0
Loss: 0.0034021511673927307
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4985]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5539]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6690.0
Loss: 5431.74072265625
KL Divergence: 18.256473541259766
Action 0 - predicted reward: tensor([[2.4759]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5363]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6865.0
Loss: 6027.70068359375
KL Divergence: 18.302291870117188
Action 0 - predicted reward: tensor([[2.4746]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5474]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 6626.54296875
KL Divergence: 18.160741806030273
Action 0 - predicted reward: tensor([[2.5784]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6312]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 5722.80126953125
KL Divergence: 18.115816116333008
13899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0445]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0494]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12515.0
Loss: 0.0354834720492363
Action 0 - predicted reward: tensor([[0.4051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0035]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15620.0
Loss: 0.043839577585458755
Action 0 - predicted reward: tensor([[-0.0500]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0184]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13165.0
Loss: 0.04067603498697281
Action 0 - predicted reward: tensor([[0.0907]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.4137]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14190.0
Loss: 0.023364337161183357
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0961]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7989]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8645.0
Loss: 0.014889495447278023
Action 0 - predicted reward: tensor([[-0.0677]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.9748]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7275.0
Loss: 0.020944423973560333
Action 0 - predicted reward: tensor([[0.0963]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.1773]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9895.0
Loss: 0.011627737432718277
Action 0 - predicted reward: tensor([[-0.9928]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9189]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12245.0
Loss: 0.02892598696053028
Greedy
Action 0 - predicted reward: tensor([[0.3223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0417]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6140.0
Loss: 3.0184166462277062e-05
Action 0 - predicted reward: tensor([[-2.1613]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8953]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21595.0
Loss: 0.004346626345068216
Action 0 - predicted reward: tensor([[0.0044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-73.7965]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6975.0
Loss: 0.0031637169886380434
Action 0 - predicted reward: tensor([[-0.0065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-79.2378]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7010.0
Loss: 0.00335835968144238
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4789]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7948]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6700.0
Loss: 5431.69677734375
KL Divergence: 18.255523681640625
Action 0 - predicted reward: tensor([[2.4561]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8843]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6865.0
Loss: 6028.35888671875
KL Divergence: 18.30051612854004
Action 0 - predicted reward: tensor([[2.4717]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2602]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6985.0
Loss: 6626.5341796875
KL Divergence: 18.160242080688477
Action 0 - predicted reward: tensor([[2.5811]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6342]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 5722.5771484375
KL Divergence: 18.114234924316406
13999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0217]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.0133]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12595.0
Loss: 0.03115350939333439
Action 0 - predicted reward: tensor([[0.4790]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9251]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15695.0
Loss: 0.04157952964305878
Action 0 - predicted reward: tensor([[-0.0207]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9897]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13175.0
Loss: 0.0370861180126667
Action 0 - predicted reward: tensor([[0.0837]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.0352]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14275.0
Loss: 0.01785506308078766
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0056]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.2857]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8680.0
Loss: 0.014586555771529675
Action 0 - predicted reward: tensor([[0.0267]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.4284]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7320.0
Loss: 0.013765430077910423
Action 0 - predicted reward: tensor([[0.7499]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0627]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9940.0
Loss: 0.010570410639047623
Action 0 - predicted reward: tensor([[-0.1435]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2558]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12285.0
Loss: 0.026955900713801384
Greedy
Action 0 - predicted reward: tensor([[0.2514]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9991]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6140.0
Loss: 1.8756203644443303e-05
Action 0 - predicted reward: tensor([[1.0192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0983]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21610.0
Loss: 0.0038867436815053225
Action 0 - predicted reward: tensor([[-0.0135]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9676]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6975.0
Loss: 0.0031547914259135723
Action 0 - predicted reward: tensor([[0.1193]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0120]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7010.0
Loss: 0.003332634689286351
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4761]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1010]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6700.0
Loss: 5432.826171875
KL Divergence: 18.248722076416016
Action 0 - predicted reward: tensor([[2.4502]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1292]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6865.0
Loss: 6028.921875
KL Divergence: 18.298328399658203
Action 0 - predicted reward: tensor([[2.4846]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5447]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 6625.93017578125
KL Divergence: 18.161619186401367
Action 0 - predicted reward: tensor([[2.5468]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8375]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 5723.509765625
KL Divergence: 18.11688232421875
14099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1203]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1230]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12665.0
Loss: 0.032628994435071945
Action 0 - predicted reward: tensor([[-0.1140]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.9654]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15775.0
Loss: 0.042507048696279526
Action 0 - predicted reward: tensor([[-0.1962]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0274]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13290.0
Loss: 0.03547203168272972
Action 0 - predicted reward: tensor([[-0.0309]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.6011]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14330.0
Loss: 0.014346004463732243
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0859]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.0902]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8715.0
Loss: 0.022101251408457756
Action 0 - predicted reward: tensor([[0.0917]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2742]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7325.0
Loss: 0.008325846865773201
Action 0 - predicted reward: tensor([[-0.2115]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.1106]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9945.0
Loss: 0.010252618230879307
Action 0 - predicted reward: tensor([[-0.0371]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.9610]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12360.0
Loss: 0.0290914848446846
Greedy
Action 0 - predicted reward: tensor([[0.1086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0843]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6140.0
Loss: 1.5794244973221794e-05
Action 0 - predicted reward: tensor([[-0.8087]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9058]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21610.0
Loss: 0.003962655086070299
Action 0 - predicted reward: tensor([[-0.0085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.8413]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6980.0
Loss: 0.0031467503868043423
Action 0 - predicted reward: tensor([[0.0399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3610]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7010.0
Loss: 0.0033019643742591143
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4679]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5252]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6700.0
Loss: 5431.7607421875
KL Divergence: 18.24498748779297
Action 0 - predicted reward: tensor([[2.4662]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5214]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6865.0
Loss: 6028.8974609375
KL Divergence: 18.292821884155273
Action 0 - predicted reward: tensor([[2.4935]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5549]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 6625.9697265625
KL Divergence: 18.15367889404297
Action 0 - predicted reward: tensor([[2.5577]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6222]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 5724.3095703125
KL Divergence: 18.114124298095703
14199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9982]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12710.0
Loss: 0.028840048238635063
Action 0 - predicted reward: tensor([[0.4959]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9005]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15920.0
Loss: 0.0480768047273159
Action 0 - predicted reward: tensor([[-0.4528]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7953]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13395.0
Loss: 0.03169563040137291
Action 0 - predicted reward: tensor([[0.0676]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9898]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14435.0
Loss: 0.021669136360287666
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.2087]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8715.0
Loss: 0.018280548974871635
Action 0 - predicted reward: tensor([[0.0467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.4195]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7405.0
Loss: 0.006923859938979149
Action 0 - predicted reward: tensor([[0.1010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.4844]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9980.0
Loss: 0.011294419877231121
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12470.0
Loss: 0.029422592371702194
Greedy
Action 0 - predicted reward: tensor([[0.1587]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9934]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6140.0
Loss: 1.661558053456247e-05
Action 0 - predicted reward: tensor([[-0.0335]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.2049]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21650.0
Loss: 0.004092731047421694
Action 0 - predicted reward: tensor([[-0.0081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.6793]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7015.0
Loss: 0.005478031933307648
Action 0 - predicted reward: tensor([[0.0184]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9959]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7010.0
Loss: 0.0032729492522776127
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4495]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6329]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6705.0
Loss: 5431.654296875
KL Divergence: 18.240020751953125
Action 0 - predicted reward: tensor([[2.4666]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5292]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6870.0
Loss: 6029.78466796875
KL Divergence: 18.290111541748047
Action 0 - predicted reward: tensor([[2.4615]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2788]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7020.0
Loss: 6925.1318359375
KL Divergence: 18.147096633911133
Action 0 - predicted reward: tensor([[2.5405]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9543]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6225.0
Loss: 5724.861328125
KL Divergence: 18.119394302368164
14299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1361]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9045]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12780.0
Loss: 0.025265373289585114
Action 0 - predicted reward: tensor([[-0.4572]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.6174]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15995.0
Loss: 0.04802276939153671
Action 0 - predicted reward: tensor([[0.0344]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4322]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13430.0
Loss: 0.03010556846857071
Action 0 - predicted reward: tensor([[1.6248]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.1922]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14510.0
Loss: 0.021873656660318375
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0804]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.1037]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8820.0
Loss: 0.025708122178912163
Action 0 - predicted reward: tensor([[0.0874]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2114]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7450.0
Loss: 0.007216206751763821
Action 0 - predicted reward: tensor([[-0.0161]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.1262]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10025.0
Loss: 0.007124832831323147
Action 0 - predicted reward: tensor([[1.1733]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.8567]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12610.0
Loss: 0.042251043021678925
Greedy
Action 0 - predicted reward: tensor([[0.1524]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0458]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6210.0
Loss: 6.258329813135788e-05
Action 0 - predicted reward: tensor([[0.0325]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.9585]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21655.0
Loss: 0.0037474543787539005
Action 0 - predicted reward: tensor([[-0.0066]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.5883]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7015.0
Loss: 0.003182752523571253
Action 0 - predicted reward: tensor([[0.2283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0276]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7010.0
Loss: 0.0032531078904867172
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4442]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0291]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6710.0
Loss: 5431.20458984375
KL Divergence: 18.23900604248047
Action 0 - predicted reward: tensor([[2.4586]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7293]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6880.0
Loss: 6027.34130859375
KL Divergence: 18.292348861694336
Action 0 - predicted reward: tensor([[2.4877]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5417]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7020.0
Loss: 6925.37060546875
KL Divergence: 18.15074920654297
Action 0 - predicted reward: tensor([[2.5689]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6200]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 5724.12109375
KL Divergence: 18.107446670532227
14399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.8292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9269]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12815.0
Loss: 0.022255519405007362
Action 0 - predicted reward: tensor([[0.1804]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8457]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16105.0
Loss: 0.04942808672785759
Action 0 - predicted reward: tensor([[0.0404]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.5027]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13580.0
Loss: 0.0394752062857151
Action 0 - predicted reward: tensor([[-0.0290]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1025]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14620.0
Loss: 0.021425845101475716
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0551]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0961]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8820.0
Loss: 0.02263869158923626
Action 0 - predicted reward: tensor([[0.0398]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.1236]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 0.006534706801176071
Action 0 - predicted reward: tensor([[1.4745]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0405]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10060.0
Loss: 0.004265962168574333
Action 0 - predicted reward: tensor([[0.0318]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.0878]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 12620.0
Loss: 0.02840525656938553
Greedy
Action 0 - predicted reward: tensor([[-0.0029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9641]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6210.0
Loss: 1.7874266632134095e-05
Action 0 - predicted reward: tensor([[0.0047]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.8469]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21660.0
Loss: 0.00367083796299994
Action 0 - predicted reward: tensor([[-0.1436]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0123]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7015.0
Loss: 0.0031535003799945116
Action 0 - predicted reward: tensor([[-0.0212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.3116]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7045.0
Loss: 0.003383914940059185
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4711]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5241]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6715.0
Loss: 5431.65966796875
KL Divergence: 18.23544692993164
Action 0 - predicted reward: tensor([[2.4472]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0553]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6895.0
Loss: 6028.849609375
KL Divergence: 18.29165267944336
Action 0 - predicted reward: tensor([[2.4638]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8875]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7020.0
Loss: 6924.30517578125
KL Divergence: 18.142580032348633
Action 0 - predicted reward: tensor([[2.5604]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6271]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6230.0
Loss: 5723.80224609375
KL Divergence: 18.11183738708496
14499.
Epsilon Greedy 5%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12885.0
Loss: 0.02341989427804947
Action 0 - predicted reward: tensor([[0.0036]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.6340]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16165.0
Loss: 0.0465591698884964
Action 0 - predicted reward: tensor([[-0.1408]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7886]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13615.0
Loss: 0.033909473568201065
Action 0 - predicted reward: tensor([[-0.0473]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9341]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14660.0
Loss: 0.020237797871232033
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5309]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3377]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8960.0
Loss: 0.030328290536999702
Action 0 - predicted reward: tensor([[0.1256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.3549]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7500.0
Loss: 0.006476028356701136
Action 0 - predicted reward: tensor([[-0.0210]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.7747]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10140.0
Loss: 0.003616067348048091
Action 0 - predicted reward: tensor([[-0.0873]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.1563]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12630.0
Loss: 0.028627967461943626
Greedy
Action 0 - predicted reward: tensor([[-0.0109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6954]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6245.0
Loss: 0.002663571387529373
Action 0 - predicted reward: tensor([[0.0303]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.0716]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21695.0
Loss: 0.0004735491529572755
Action 0 - predicted reward: tensor([[0.0032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.0344]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7015.0
Loss: 0.003148212330415845
Action 0 - predicted reward: tensor([[0.0063]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.2861]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7045.0
Loss: 0.0033228478860110044
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4492]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8194]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6715.0
Loss: 5432.1845703125
KL Divergence: 18.237823486328125
Action 0 - predicted reward: tensor([[2.4659]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5347]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6895.0
Loss: 6029.78125
KL Divergence: 18.294889450073242
Action 0 - predicted reward: tensor([[2.4748]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5393]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7020.0
Loss: 6925.53271484375
KL Divergence: 18.143001556396484
Action 0 - predicted reward: tensor([[2.5528]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2788]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6230.0
Loss: 5724.35205078125
KL Divergence: 18.10951805114746
14599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9783]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12930.0
Loss: 0.02174423262476921
Action 0 - predicted reward: tensor([[0.3530]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2612]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16275.0
Loss: 0.051562849432229996
Action 0 - predicted reward: tensor([[-0.1518]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.0984]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13685.0
Loss: 0.0368628092110157
Action 0 - predicted reward: tensor([[0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.7212]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14665.0
Loss: 0.020659590139985085
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0450]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.2982]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8995.0
Loss: 0.026767002418637276
Action 0 - predicted reward: tensor([[-0.0241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.8094]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7540.0
Loss: 0.006786062382161617
Action 0 - predicted reward: tensor([[-0.0728]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.0100]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10145.0
Loss: 0.003438816638663411
Action 0 - predicted reward: tensor([[-0.0262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.4492]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12705.0
Loss: 0.029086008667945862
Greedy
Action 0 - predicted reward: tensor([[-0.2084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.6805]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6280.0
Loss: 0.004819066263735294
Action 0 - predicted reward: tensor([[-0.0065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.6195]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21705.0
Loss: 9.72436901065521e-05
Action 0 - predicted reward: tensor([[-0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.6952]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7015.0
Loss: 0.003141887718811631
Action 0 - predicted reward: tensor([[-0.0291]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0807]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7045.0
Loss: 0.0033038118854165077
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4600]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0712]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6715.0
Loss: 5431.97412109375
KL Divergence: 18.234651565551758
Action 0 - predicted reward: tensor([[2.4744]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5372]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6900.0
Loss: 6028.826171875
KL Divergence: 18.290334701538086
Action 0 - predicted reward: tensor([[2.4689]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3118]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7020.0
Loss: 6626.06298828125
KL Divergence: 18.142107009887695
Action 0 - predicted reward: tensor([[2.5426]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0458]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6235.0
Loss: 5724.77001953125
KL Divergence: 18.10841178894043
14699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.1188]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1048]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13075.0
Loss: 0.021968448534607887
Action 0 - predicted reward: tensor([[0.0306]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.4938]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16310.0
Loss: 0.04699835926294327
Action 0 - predicted reward: tensor([[0.0152]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.2949]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13815.0
Loss: 0.03750664368271828
Action 0 - predicted reward: tensor([[-0.0104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.2276]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14770.0
Loss: 0.020459504798054695
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1184]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6732]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9105.0
Loss: 0.027179628610610962
Action 0 - predicted reward: tensor([[0.9861]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0286]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 0.0065384116023778915
Action 0 - predicted reward: tensor([[-0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.0955]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10215.0
Loss: 0.012211937457323074
Action 0 - predicted reward: tensor([[-0.3016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[9.5430]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 12810.0
Loss: 0.035360611975193024
Greedy
Action 0 - predicted reward: tensor([[0.0816]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0637]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6315.0
Loss: 0.00860007107257843
Action 0 - predicted reward: tensor([[-0.0149]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.3556]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21750.0
Loss: 9.196771861752495e-05
Action 0 - predicted reward: tensor([[-0.0883]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9951]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7015.0
Loss: 0.003140461165457964
Action 0 - predicted reward: tensor([[-0.0248]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.6877]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7045.0
Loss: 0.0032867875415831804
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4444]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7434]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6715.0
Loss: 5431.7763671875
KL Divergence: 18.232404708862305
Action 0 - predicted reward: tensor([[2.4535]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6441]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6900.0
Loss: 6029.54931640625
KL Divergence: 18.282583236694336
Action 0 - predicted reward: tensor([[2.4706]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5340]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7055.0
Loss: 6924.46435546875
KL Divergence: 18.138477325439453
Action 0 - predicted reward: tensor([[2.5419]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1033]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5725.1162109375
KL Divergence: 18.103269577026367
14799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3400]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9849]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13080.0
Loss: 0.020551899448037148
Action 0 - predicted reward: tensor([[-4.8506]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7813]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16425.0
Loss: 0.04803958535194397
Action 0 - predicted reward: tensor([[0.0351]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0118]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13890.0
Loss: 0.029786890372633934
Action 0 - predicted reward: tensor([[0.0355]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.4692]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14810.0
Loss: 0.019963029772043228
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.3560]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9105.0
Loss: 0.025543704628944397
Action 0 - predicted reward: tensor([[0.1620]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9392]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7655.0
Loss: 0.0065508815459907055
Action 0 - predicted reward: tensor([[0.1998]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0994]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10250.0
Loss: 0.017550524324178696
Action 0 - predicted reward: tensor([[-0.0153]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.3564]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12930.0
Loss: 0.02580680511891842
Greedy
Action 0 - predicted reward: tensor([[0.1533]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0757]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6315.0
Loss: 0.0076181283220648766
Action 0 - predicted reward: tensor([[-0.1786]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0895]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21755.0
Loss: 4.4518968934426084e-05
Action 0 - predicted reward: tensor([[-0.1748]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0032]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7050.0
Loss: 0.005316421389579773
Action 0 - predicted reward: tensor([[-0.0227]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.4155]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7045.0
Loss: 0.0032591698691248894
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4598]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5206]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6720.0
Loss: 5431.2529296875
KL Divergence: 18.23271942138672
Action 0 - predicted reward: tensor([[2.4512]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0973]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6920.0
Loss: 6029.67626953125
KL Divergence: 18.282867431640625
Action 0 - predicted reward: tensor([[2.4774]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5355]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7055.0
Loss: 6924.9697265625
KL Divergence: 18.139495849609375
Action 0 - predicted reward: tensor([[2.5624]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6160]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5724.9853515625
KL Divergence: 18.106752395629883
14899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2432]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1569]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13260.0
Loss: 0.03359387069940567
Action 0 - predicted reward: tensor([[-0.2924]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.1965]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16465.0
Loss: 0.04755375534296036
Action 0 - predicted reward: tensor([[-0.0185]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.1902]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13970.0
Loss: 0.02962385304272175
Action 0 - predicted reward: tensor([[0.0424]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.7748]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14895.0
Loss: 0.01998988352715969
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3738]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1126]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9140.0
Loss: 0.024320563301444054
Action 0 - predicted reward: tensor([[0.0273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.1978]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7690.0
Loss: 0.011946570128202438
Action 0 - predicted reward: tensor([[-0.0384]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1260]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10355.0
Loss: 0.01732722483575344
Action 0 - predicted reward: tensor([[1.0345]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3996]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13040.0
Loss: 0.026915516704320908
Greedy
Action 0 - predicted reward: tensor([[0.0296]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.8974]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6350.0
Loss: 0.007879183627665043
Action 0 - predicted reward: tensor([[-0.0441]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.9217]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21760.0
Loss: 3.942968032788485e-05
Action 0 - predicted reward: tensor([[-0.1374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0009]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7050.0
Loss: 0.00023955004871822894
Action 0 - predicted reward: tensor([[-0.0090]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.3817]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7080.0
Loss: 0.0066975876688957214
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4748]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5216]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6720.0
Loss: 5431.6083984375
KL Divergence: 18.230649948120117
Action 0 - predicted reward: tensor([[2.4639]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5250]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6925.0
Loss: 6028.873046875
KL Divergence: 18.28179359436035
Action 0 - predicted reward: tensor([[2.4553]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0627]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7055.0
Loss: 6924.71826171875
KL Divergence: 18.13453483581543
Action 0 - predicted reward: tensor([[2.5565]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6089]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5726.39794921875
KL Divergence: 18.09696388244629
14999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4732]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8860]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13405.0
Loss: 0.030543090775609016
Action 0 - predicted reward: tensor([[0.3964]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1572]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16580.0
Loss: 0.04901852458715439
Action 0 - predicted reward: tensor([[-0.0724]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9689]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14015.0
Loss: 0.032622236758470535
Action 0 - predicted reward: tensor([[-0.0022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.5886]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14935.0
Loss: 0.020968757569789886
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0247]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.1391]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9250.0
Loss: 0.031156960874795914
Action 0 - predicted reward: tensor([[-0.0182]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.4502]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7725.0
Loss: 0.012050346471369267
Action 0 - predicted reward: tensor([[0.0720]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.7047]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10425.0
Loss: 0.020386317744851112
Action 0 - predicted reward: tensor([[-0.2456]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.3093]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13115.0
Loss: 0.03368464112281799
Greedy
Action 0 - predicted reward: tensor([[0.0171]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0080]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6350.0
Loss: 0.0077121988870203495
Action 0 - predicted reward: tensor([[-0.2039]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0362]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21775.0
Loss: 3.917429057764821e-05
Action 0 - predicted reward: tensor([[0.0716]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3227]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7050.0
Loss: 2.845343442459125e-05
Action 0 - predicted reward: tensor([[0.0211]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.5511]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 0.006415276788175106
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4578]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5188]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6720.0
Loss: 5431.9345703125
KL Divergence: 18.225744247436523
Action 0 - predicted reward: tensor([[2.4789]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5321]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6930.0
Loss: 5730.8984375
KL Divergence: 18.285959243774414
Action 0 - predicted reward: tensor([[2.4931]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5431]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7055.0
Loss: 6925.4990234375
KL Divergence: 18.133047103881836
Action 0 - predicted reward: tensor([[2.5653]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6166]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5723.68310546875
KL Divergence: 18.09789276123047
15099.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 13550.0
Loss: 0.028515180572867393
Action 0 - predicted reward: tensor([[4.0484]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9479]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16585.0
Loss: 0.04357069730758667
Action 0 - predicted reward: tensor([[0.1609]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0889]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14055.0
Loss: 0.032980676740407944
Action 0 - predicted reward: tensor([[0.0457]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.8504]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14975.0
Loss: 0.017579106613993645
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0845]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.1714]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9320.0
Loss: 0.030973605811595917
Action 0 - predicted reward: tensor([[-0.1085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5091]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7725.0
Loss: 0.004234803840517998
Action 0 - predicted reward: tensor([[0.0305]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.7641]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10430.0
Loss: 0.020088359713554382
Action 0 - predicted reward: tensor([[-0.0427]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.5496]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13240.0
Loss: 0.034498780965805054
Greedy
Action 0 - predicted reward: tensor([[0.2069]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8984]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6350.0
Loss: 0.004617272410541773
Action 0 - predicted reward: tensor([[-0.0027]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.0708]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21780.0
Loss: 4.107297718292102e-05
Action 0 - predicted reward: tensor([[0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.5135]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7050.0
Loss: 1.9423639969318174e-05
Action 0 - predicted reward: tensor([[-0.0273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5767]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 0.006393294781446457
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4707]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5249]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6720.0
Loss: 5431.83251953125
KL Divergence: 18.2269287109375
Action 0 - predicted reward: tensor([[2.4698]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8981]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6940.0
Loss: 5431.14013671875
KL Divergence: 18.2843074798584
Action 0 - predicted reward: tensor([[2.4937]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5434]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7055.0
Loss: 6626.4794921875
KL Divergence: 18.131216049194336
Action 0 - predicted reward: tensor([[2.5787]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6259]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5723.52294921875
KL Divergence: 18.099159240722656
15199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0315]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13635.0
Loss: 0.02757006138563156
Action 0 - predicted reward: tensor([[0.0165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5189]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16705.0
Loss: 0.04572131484746933
Action 0 - predicted reward: tensor([[-0.0415]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.9425]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14060.0
Loss: 0.030244076624512672
Action 0 - predicted reward: tensor([[0.0462]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0235]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15090.0
Loss: 0.020038606598973274
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0221]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.5818]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9430.0
Loss: 0.03459813445806503
Action 0 - predicted reward: tensor([[0.1081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0430]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7830.0
Loss: 0.010083874687552452
Action 0 - predicted reward: tensor([[0.0429]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1653]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10505.0
Loss: 0.019360750913619995
Action 0 - predicted reward: tensor([[-1.2855]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0657]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13385.0
Loss: 0.0417056530714035
Greedy
Action 0 - predicted reward: tensor([[0.2208]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0378]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.008628963492810726
Action 0 - predicted reward: tensor([[-0.0062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.3297]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21785.0
Loss: 3.1965300877345726e-05
Action 0 - predicted reward: tensor([[0.0527]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0066]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7050.0
Loss: 2.5291807105531916e-05
Action 0 - predicted reward: tensor([[0.2747]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0246]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 0.006360077299177647
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4536]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0251]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6725.0
Loss: 5432.0009765625
KL Divergence: 18.224369049072266
Action 0 - predicted reward: tensor([[2.5023]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5631]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6940.0
Loss: 5431.94921875
KL Divergence: 18.286588668823242
Action 0 - predicted reward: tensor([[2.4596]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3395]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7055.0
Loss: 6626.5595703125
KL Divergence: 18.13873291015625
Action 0 - predicted reward: tensor([[2.5783]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6366]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5423.455078125
KL Divergence: 18.110973358154297
15299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0390]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1044]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13680.0
Loss: 0.030706502497196198
Action 0 - predicted reward: tensor([[-2.3391]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9406]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16720.0
Loss: 0.04299009218811989
Action 0 - predicted reward: tensor([[0.0938]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.6070]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14060.0
Loss: 0.029061906039714813
Action 0 - predicted reward: tensor([[0.0360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.4885]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15245.0
Loss: 0.020254170522093773
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2655]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0906]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9435.0
Loss: 0.033512331545352936
Action 0 - predicted reward: tensor([[0.1403]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9835]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7835.0
Loss: 0.003433424513787031
Action 0 - predicted reward: tensor([[0.0107]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.8926]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10550.0
Loss: 0.019091177731752396
Action 0 - predicted reward: tensor([[0.0684]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.0229]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13425.0
Loss: 0.03626590594649315
Greedy
Action 0 - predicted reward: tensor([[0.0208]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.7216]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.0073882644064724445
Action 0 - predicted reward: tensor([[0.0550]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0890]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21785.0
Loss: 2.7602156478678808e-05
Action 0 - predicted reward: tensor([[-0.0253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.4688]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7050.0
Loss: 1.6148482245625928e-05
Action 0 - predicted reward: tensor([[0.0143]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.5752]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 0.006345055997371674
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4691]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5289]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6735.0
Loss: 5431.74560546875
KL Divergence: 18.224796295166016
Action 0 - predicted reward: tensor([[2.4896]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0890]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6940.0
Loss: 5430.8486328125
KL Divergence: 18.279531478881836
Action 0 - predicted reward: tensor([[2.4931]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5375]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7055.0
Loss: 6626.33447265625
KL Divergence: 18.13446617126465
Action 0 - predicted reward: tensor([[2.5594]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2362]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5422.8115234375
KL Divergence: 18.10082244873047
15399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1606]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9821]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13720.0
Loss: 0.030935578048229218
Action 0 - predicted reward: tensor([[-0.2113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.0434]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16860.0
Loss: 0.051104575395584106
Action 0 - predicted reward: tensor([[-0.0057]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.7906]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14100.0
Loss: 0.030467012897133827
Action 0 - predicted reward: tensor([[-0.1021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.6646]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15325.0
Loss: 0.018282974138855934
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5506]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9654]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9470.0
Loss: 0.03076581470668316
Action 0 - predicted reward: tensor([[-0.2937]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0213]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7870.0
Loss: 0.003388028359040618
Action 0 - predicted reward: tensor([[0.6168]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1452]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10655.0
Loss: 0.02882392145693302
Action 0 - predicted reward: tensor([[-0.3643]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8668]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13575.0
Loss: 0.034987013787031174
Greedy
Action 0 - predicted reward: tensor([[-0.0079]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-74.7467]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.0071324631571769714
Action 0 - predicted reward: tensor([[0.4785]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.2206]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21830.0
Loss: 6.602716166526079e-05
Action 0 - predicted reward: tensor([[-0.0284]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9645]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7050.0
Loss: 1.2616777894436382e-05
Action 0 - predicted reward: tensor([[-0.0205]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.6278]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 0.00633428618311882
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4614]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5205]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6745.0
Loss: 5431.36767578125
KL Divergence: 18.22679901123047
Action 0 - predicted reward: tensor([[2.5176]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5744]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6950.0
Loss: 5430.0712890625
KL Divergence: 18.28200912475586
Action 0 - predicted reward: tensor([[2.4674]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0898]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7055.0
Loss: 6625.81103515625
KL Divergence: 18.127891540527344
Action 0 - predicted reward: tensor([[2.5474]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8977]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5425.013671875
KL Divergence: 18.09476661682129
15499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0167]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.7218]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13760.0
Loss: 0.03006644919514656
Action 0 - predicted reward: tensor([[-0.4283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8871]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16905.0
Loss: 0.052464354783296585
Action 0 - predicted reward: tensor([[0.0012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7394]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14180.0
Loss: 0.02660500817000866
Action 0 - predicted reward: tensor([[-0.0030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8787]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15340.0
Loss: 0.01718410663306713
Epsilon Greedy 1%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9470.0
Loss: 0.030057519674301147
Action 0 - predicted reward: tensor([[0.0512]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0185]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7905.0
Loss: 0.0032808706164360046
Action 0 - predicted reward: tensor([[-0.0542]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.6035]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10690.0
Loss: 0.027422847226262093
Action 0 - predicted reward: tensor([[-0.5194]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.6956]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13685.0
Loss: 0.034438297152519226
Greedy
Action 0 - predicted reward: tensor([[0.0372]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-86.0670]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.006976185832172632
Action 0 - predicted reward: tensor([[-1.4509]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0192]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21835.0
Loss: 0.00012318928202148527
Action 0 - predicted reward: tensor([[-0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.0439]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 1.4304561773315072e-05
Action 0 - predicted reward: tensor([[0.1156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9433]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 0.006224245764315128
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4584]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5213]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6750.0
Loss: 5431.3466796875
KL Divergence: 18.22062110900879
Action 0 - predicted reward: tensor([[2.5203]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8592]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6950.0
Loss: 5428.7099609375
KL Divergence: 18.286653518676758
Action 0 - predicted reward: tensor([[2.4836]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1308]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7055.0
Loss: 6625.9140625
KL Divergence: 18.132923126220703
Action 0 - predicted reward: tensor([[2.5444]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1164]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5425.42529296875
KL Divergence: 18.093347549438477
15599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0510]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.0748]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13835.0
Loss: 0.0345926471054554
Action 0 - predicted reward: tensor([[-0.3250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.3676]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16905.0
Loss: 0.04611916467547417
Action 0 - predicted reward: tensor([[0.0432]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.4784]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14225.0
Loss: 0.026036057621240616
Action 0 - predicted reward: tensor([[-0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1667]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15420.0
Loss: 0.01691323146224022
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0441]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.9521]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9540.0
Loss: 0.03322495147585869
Action 0 - predicted reward: tensor([[0.1038]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8543]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7905.0
Loss: 0.0032494952902197838
Action 0 - predicted reward: tensor([[-0.1118]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0772]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10730.0
Loss: 0.0263360645622015
Action 0 - predicted reward: tensor([[-0.3240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0685]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13685.0
Loss: 0.030518298968672752
Greedy
Action 0 - predicted reward: tensor([[-0.0138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.3948]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.007006341125816107
Action 0 - predicted reward: tensor([[0.0101]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.3588]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21845.0
Loss: 3.4333068470004946e-05
Action 0 - predicted reward: tensor([[-0.0066]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-81.2469]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 1.0946054317173548e-05
Action 0 - predicted reward: tensor([[0.0822]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9980]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 0.006292091216892004
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4495]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5086]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6750.0
Loss: 5431.85400390625
KL Divergence: 18.215099334716797
Action 0 - predicted reward: tensor([[2.5186]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1760]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6970.0
Loss: 5428.85302734375
KL Divergence: 18.283275604248047
Action 0 - predicted reward: tensor([[2.4784]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5417]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6626.69921875
KL Divergence: 18.136022567749023
Action 0 - predicted reward: tensor([[2.5727]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6190]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5426.552734375
KL Divergence: 18.091901779174805
15699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0536]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.7308]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13955.0
Loss: 0.03140752390027046
Action 0 - predicted reward: tensor([[-0.2907]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.1250]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17010.0
Loss: 0.05015682801604271
Action 0 - predicted reward: tensor([[-0.0531]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0606]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14290.0
Loss: 0.025133218616247177
Action 0 - predicted reward: tensor([[0.0079]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2807]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15595.0
Loss: 0.01828267239034176
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0176]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4947]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9550.0
Loss: 0.0333268977701664
Action 0 - predicted reward: tensor([[0.0078]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5175]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7980.0
Loss: 0.0034437524154782295
Action 0 - predicted reward: tensor([[-0.0533]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0136]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10740.0
Loss: 0.02591143548488617
Action 0 - predicted reward: tensor([[-0.1852]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8503]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13790.0
Loss: 0.03681492432951927
Greedy
Action 0 - predicted reward: tensor([[0.0081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.4569]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.006985059008002281
Action 0 - predicted reward: tensor([[0.0569]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.8383]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21845.0
Loss: 2.7795327696367167e-05
Action 0 - predicted reward: tensor([[0.0095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.0366]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 9.492860044701956e-06
Action 0 - predicted reward: tensor([[0.0394]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9887]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 0.006326761096715927
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4614]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5110]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6750.0
Loss: 5430.7373046875
KL Divergence: 18.215496063232422
Action 0 - predicted reward: tensor([[2.5213]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2459]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6980.0
Loss: 5428.3408203125
KL Divergence: 18.283639907836914
Action 0 - predicted reward: tensor([[2.4778]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1402]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6621.37353515625
KL Divergence: 18.13245964050293
Action 0 - predicted reward: tensor([[2.5543]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8760]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5422.990234375
KL Divergence: 18.087146759033203
15799.
Epsilon Greedy 5%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14100.0
Loss: 0.03167365491390228
Action 0 - predicted reward: tensor([[-1.2354]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0879]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17165.0
Loss: 0.04734203591942787
Action 0 - predicted reward: tensor([[-0.0163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.2122]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14340.0
Loss: 0.029059045016765594
Action 0 - predicted reward: tensor([[-0.0079]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0103]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15670.0
Loss: 0.020273976027965546
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2136]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0231]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9555.0
Loss: 0.02969410829246044
Action 0 - predicted reward: tensor([[0.0899]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.8052]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7980.0
Loss: 0.003230354515835643
Action 0 - predicted reward: tensor([[0.0645]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.7344]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10745.0
Loss: 0.025622596964240074
Action 0 - predicted reward: tensor([[0.0810]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.8667]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13865.0
Loss: 0.03451025113463402
Greedy
Action 0 - predicted reward: tensor([[0.0315]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-83.1556]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.007002589758485556
Action 0 - predicted reward: tensor([[-0.0060]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.4942]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21855.0
Loss: 2.469011815264821e-05
Action 0 - predicted reward: tensor([[0.0433]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9736]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7090.0
Loss: 8.977438483270817e-06
Action 0 - predicted reward: tensor([[0.0066]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.6025]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 0.006280694622546434
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4343]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5505]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6750.0
Loss: 5431.43212890625
KL Divergence: 18.211977005004883
Action 0 - predicted reward: tensor([[2.5140]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1442]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6980.0
Loss: 5429.685546875
KL Divergence: 18.285221099853516
Action 0 - predicted reward: tensor([[2.4839]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5558]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6327.98828125
KL Divergence: 18.127744674682617
Action 0 - predicted reward: tensor([[2.5571]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6495]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5423.3095703125
KL Divergence: 18.086162567138672
15899.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14175.0
Loss: 0.026656320318579674
Action 0 - predicted reward: tensor([[-0.0274]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.3335]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17340.0
Loss: 0.046841833740472794
Action 0 - predicted reward: tensor([[-0.2511]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1465]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14415.0
Loss: 0.021923411637544632
Action 0 - predicted reward: tensor([[0.0338]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8813]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15715.0
Loss: 0.01836145482957363
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0469]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7817]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9590.0
Loss: 0.029925042763352394
Action 0 - predicted reward: tensor([[-0.2670]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.4878]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8015.0
Loss: 0.00800444558262825
Action 0 - predicted reward: tensor([[0.7133]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0654]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10750.0
Loss: 0.026009993627667427
Action 0 - predicted reward: tensor([[0.0190]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.0479]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13865.0
Loss: 0.03163902088999748
Greedy
Action 0 - predicted reward: tensor([[-0.0382]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-75.1724]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.006871642079204321
Action 0 - predicted reward: tensor([[-0.6439]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[7.0902]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21855.0
Loss: 4.583308327710256e-05
Action 0 - predicted reward: tensor([[-0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-96.6664]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7090.0
Loss: 8.551988685212564e-06
Action 0 - predicted reward: tensor([[0.0138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.8882]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 0.006200787145644426
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4393]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9501]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6750.0
Loss: 5431.0341796875
KL Divergence: 18.21192741394043
Action 0 - predicted reward: tensor([[2.5427]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5995]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 5428.80810546875
KL Divergence: 18.285293579101562
Action 0 - predicted reward: tensor([[2.4844]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2731]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6029.017578125
KL Divergence: 18.125652313232422
Action 0 - predicted reward: tensor([[2.5787]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6353]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5423.04345703125
KL Divergence: 18.08180809020996
15999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0525]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9992]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14300.0
Loss: 0.027807418256998062
Action 0 - predicted reward: tensor([[-0.0911]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5331]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17340.0
Loss: 0.04232219234108925
Action 0 - predicted reward: tensor([[0.1292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7836]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14455.0
Loss: 0.020754490047693253
Action 0 - predicted reward: tensor([[-1.4705]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0156]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15750.0
Loss: 0.01681990548968315
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0748]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9941]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9630.0
Loss: 0.02615508623421192
Action 0 - predicted reward: tensor([[0.8308]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9970]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8125.0
Loss: 0.009890470653772354
Action 0 - predicted reward: tensor([[0.3542]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.7658]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10750.0
Loss: 0.025348620489239693
Action 0 - predicted reward: tensor([[-0.4131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0484]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14045.0
Loss: 0.04839060828089714
Greedy
Action 0 - predicted reward: tensor([[-0.0079]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-66.3965]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.0034437773283571005
Action 0 - predicted reward: tensor([[-0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.1141]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21855.0
Loss: 3.383686998859048e-05
Action 0 - predicted reward: tensor([[-0.0244]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0606]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7090.0
Loss: 7.664437362109311e-06
Action 0 - predicted reward: tensor([[0.0682]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0096]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 0.0062530143186450005
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4367]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6633]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6750.0
Loss: 5432.0419921875
KL Divergence: 18.211950302124023
Action 0 - predicted reward: tensor([[2.5236]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5843]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6990.0
Loss: 5430.15380859375
KL Divergence: 18.27596092224121
Action 0 - predicted reward: tensor([[2.4937]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2371]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6028.2216796875
KL Divergence: 18.134479522705078
Action 0 - predicted reward: tensor([[2.5719]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9223]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5420.3701171875
KL Divergence: 18.087398529052734
16099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0808]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.3066]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14340.0
Loss: 0.02633073925971985
Action 0 - predicted reward: tensor([[-0.0053]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7110]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17410.0
Loss: 0.049849964678287506
Action 0 - predicted reward: tensor([[0.3088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0172]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14465.0
Loss: 0.01815648190677166
Action 0 - predicted reward: tensor([[-0.0195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9165]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15790.0
Loss: 0.014837818220257759
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0606]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9982]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9630.0
Loss: 0.026101628318428993
Action 0 - predicted reward: tensor([[-0.0657]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0440]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8195.0
Loss: 0.009532186202704906
Action 0 - predicted reward: tensor([[-0.6857]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7675]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10785.0
Loss: 0.02315165102481842
Action 0 - predicted reward: tensor([[-0.2822]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8091]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14095.0
Loss: 0.032478928565979004
Greedy
Action 0 - predicted reward: tensor([[0.2683]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0108]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.0033722994849085808
Action 0 - predicted reward: tensor([[-2.7914]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0159]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21870.0
Loss: 2.4735798433539458e-05
Action 0 - predicted reward: tensor([[0.0095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.4844]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7090.0
Loss: 7.32827948013437e-06
Action 0 - predicted reward: tensor([[0.1309]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0173]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 0.0031686723232269287
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4602]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5108]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6750.0
Loss: 5431.7431640625
KL Divergence: 18.208093643188477
Action 0 - predicted reward: tensor([[2.5091]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5687]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7005.0
Loss: 5429.58349609375
KL Divergence: 18.267478942871094
Action 0 - predicted reward: tensor([[2.4906]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4654]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6028.9384765625
KL Divergence: 18.12761878967285
Action 0 - predicted reward: tensor([[2.5582]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3498]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5423.3955078125
KL Divergence: 18.088308334350586
16199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0489]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9329]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14350.0
Loss: 0.02379029616713524
Action 0 - predicted reward: tensor([[-0.4218]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8454]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17455.0
Loss: 0.04402892291545868
Action 0 - predicted reward: tensor([[-0.2113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9517]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14470.0
Loss: 0.017383014783263206
Action 0 - predicted reward: tensor([[-1.3792]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0081]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15860.0
Loss: 0.014491626061499119
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3591]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0212]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9705.0
Loss: 0.02992767095565796
Action 0 - predicted reward: tensor([[-0.0131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.1884]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8205.0
Loss: 0.00817057117819786
Action 0 - predicted reward: tensor([[0.0648]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.4733]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10785.0
Loss: 0.02236122265458107
Action 0 - predicted reward: tensor([[0.0095]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.5989]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14095.0
Loss: 0.030247323215007782
Greedy
Action 0 - predicted reward: tensor([[0.1014]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9969]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.003369018668308854
Action 0 - predicted reward: tensor([[0.0375]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.3013]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21885.0
Loss: 2.232430051662959e-05
Action 0 - predicted reward: tensor([[0.0048]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.7641]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 0.00014846613339614123
Action 0 - predicted reward: tensor([[0.0130]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.7775]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 0.003143024630844593
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4598]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5162]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6755.0
Loss: 5431.943359375
KL Divergence: 18.205989837646484
Action 0 - predicted reward: tensor([[2.4935]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1871]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7020.0
Loss: 5431.37158203125
KL Divergence: 18.271787643432617
Action 0 - predicted reward: tensor([[2.5130]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5735]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6029.0595703125
KL Divergence: 18.131832122802734
Action 0 - predicted reward: tensor([[2.5622]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9720]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5423.02197265625
KL Divergence: 18.079286575317383
16299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0878]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4938]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14535.0
Loss: 0.025226978585124016
Action 0 - predicted reward: tensor([[-0.0282]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.0895]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17525.0
Loss: 0.044082846492528915
Action 0 - predicted reward: tensor([[-0.0772]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9067]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14510.0
Loss: 0.018576981499791145
Action 0 - predicted reward: tensor([[-0.0051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9831]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15860.0
Loss: 0.01406710036098957
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3574]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0425]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9705.0
Loss: 0.029240170493721962
Action 0 - predicted reward: tensor([[-0.0752]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.3026]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8215.0
Loss: 0.007975359447300434
Action 0 - predicted reward: tensor([[-0.0749]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.6149]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10825.0
Loss: 0.019954290241003036
Action 0 - predicted reward: tensor([[0.8988]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0856]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14165.0
Loss: 0.03278978914022446
Greedy
Action 0 - predicted reward: tensor([[0.2673]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9548]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.0033536325208842754
Action 0 - predicted reward: tensor([[1.4217]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0296]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21890.0
Loss: 1.787058135960251e-05
Action 0 - predicted reward: tensor([[-0.0005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.4818]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 1.4871454368403647e-05
Action 0 - predicted reward: tensor([[0.2158]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9970]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7085.0
Loss: 0.0031257783994078636
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4534]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5064]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6765.0
Loss: 5430.2109375
KL Divergence: 18.20488929748535
Action 0 - predicted reward: tensor([[2.4885]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0812]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7020.0
Loss: 5430.68115234375
KL Divergence: 18.270751953125
Action 0 - predicted reward: tensor([[2.5179]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5740]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6027.92822265625
KL Divergence: 18.12559700012207
Action 0 - predicted reward: tensor([[2.5598]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2462]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5422.7509765625
KL Divergence: 18.087783813476562
16399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2227]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0985]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14580.0
Loss: 0.02194937877357006
Action 0 - predicted reward: tensor([[-0.2463]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.5255]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17560.0
Loss: 0.03978939726948738
Action 0 - predicted reward: tensor([[0.0691]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0859]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14655.0
Loss: 0.022575316950678825
Action 0 - predicted reward: tensor([[0.0029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8294]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15900.0
Loss: 0.01592922955751419
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.2982]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9740.0
Loss: 0.029183033853769302
Action 0 - predicted reward: tensor([[0.0077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.4371]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8215.0
Loss: 0.007717838045209646
Action 0 - predicted reward: tensor([[-0.1707]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9543]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10865.0
Loss: 0.018859991803765297
Action 0 - predicted reward: tensor([[-1.0107]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0949]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14165.0
Loss: 0.031054407358169556
Greedy
Action 0 - predicted reward: tensor([[0.3149]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0214]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.0033346202690154314
Action 0 - predicted reward: tensor([[0.0373]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.8416]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21910.0
Loss: 1.807987064239569e-05
Action 0 - predicted reward: tensor([[0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.4721]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 1.2856206922151614e-05
Action 0 - predicted reward: tensor([[-0.0227]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.1339]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7085.0
Loss: 0.0031213227193802595
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4350]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7492]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6770.0
Loss: 5430.58251953125
KL Divergence: 18.2061824798584
Action 0 - predicted reward: tensor([[2.4974]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5618]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7020.0
Loss: 5430.3857421875
KL Divergence: 18.27073860168457
Action 0 - predicted reward: tensor([[2.5257]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5839]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6026.9619140625
KL Divergence: 18.125593185424805
Action 0 - predicted reward: tensor([[2.5542]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0061]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5424.07958984375
KL Divergence: 18.083351135253906
16499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9192]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14620.0
Loss: 0.024362841621041298
Action 0 - predicted reward: tensor([[0.4788]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2904]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17645.0
Loss: 0.03952183201909065
Action 0 - predicted reward: tensor([[-0.0113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.5725]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14725.0
Loss: 0.024786798283457756
Action 0 - predicted reward: tensor([[-0.0324]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8505]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15950.0
Loss: 0.01842912659049034
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2867]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9817]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9740.0
Loss: 0.026237227022647858
Action 0 - predicted reward: tensor([[-0.0264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0383]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8225.0
Loss: 0.007671771105378866
Action 0 - predicted reward: tensor([[0.0148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0223]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10865.0
Loss: 0.015318579040467739
Action 0 - predicted reward: tensor([[0.2160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6708]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14240.0
Loss: 0.035168103873729706
Greedy
Action 0 - predicted reward: tensor([[0.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2839]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.003337462432682514
Action 0 - predicted reward: tensor([[-3.4505]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0222]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21950.0
Loss: 0.004002741537988186
Action 0 - predicted reward: tensor([[-0.0570]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9529]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7160.0
Loss: 1.0648073839547578e-05
Action 0 - predicted reward: tensor([[-0.0166]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.2565]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7120.0
Loss: 0.003597256960347295
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4414]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8441]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6770.0
Loss: 5431.44384765625
KL Divergence: 18.208097457885742
Action 0 - predicted reward: tensor([[2.5046]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5694]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7025.0
Loss: 5431.37841796875
KL Divergence: 18.264951705932617
Action 0 - predicted reward: tensor([[2.5048]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0639]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6027.78515625
KL Divergence: 18.120481491088867
Action 0 - predicted reward: tensor([[2.5892]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6318]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5423.98388671875
KL Divergence: 18.077585220336914
16599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0177]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0715]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14700.0
Loss: 0.02490341290831566
Action 0 - predicted reward: tensor([[0.3172]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9759]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17720.0
Loss: 0.03633837774395943
Action 0 - predicted reward: tensor([[-0.0488]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.6584]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14840.0
Loss: 0.01562962681055069
Action 0 - predicted reward: tensor([[-0.0336]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.3031]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15995.0
Loss: 0.02182498201727867
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0182]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.0905]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9775.0
Loss: 0.02579551748931408
Action 0 - predicted reward: tensor([[0.0107]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.4690]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8240.0
Loss: 0.007619698531925678
Action 0 - predicted reward: tensor([[0.0411]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.2266]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10865.0
Loss: 0.0153852179646492
Action 0 - predicted reward: tensor([[-0.1911]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.4426]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14315.0
Loss: 0.03554759547114372
Greedy
Action 0 - predicted reward: tensor([[0.2725]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9911]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.0033045816235244274
Action 0 - predicted reward: tensor([[0.7252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1438]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21955.0
Loss: 0.0038396278396248817
Action 0 - predicted reward: tensor([[0.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-85.8255]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 8.83477787283482e-06
Action 0 - predicted reward: tensor([[0.1226]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7120.0
Loss: 0.0030722490046173334
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4372]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7608]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6770.0
Loss: 5431.77294921875
KL Divergence: 18.20186424255371
Action 0 - predicted reward: tensor([[2.4832]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0626]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7025.0
Loss: 5431.83935546875
KL Divergence: 18.259075164794922
Action 0 - predicted reward: tensor([[2.5207]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5780]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6027.2685546875
KL Divergence: 18.119163513183594
Action 0 - predicted reward: tensor([[2.5614]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0432]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5423.1396484375
KL Divergence: 18.079639434814453
16699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0708]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.8150]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14915.0
Loss: 0.034146059304475784
Action 0 - predicted reward: tensor([[-0.1122]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.0163]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17845.0
Loss: 0.04208683967590332
Action 0 - predicted reward: tensor([[0.1714]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4569]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14995.0
Loss: 0.016125887632369995
Action 0 - predicted reward: tensor([[0.1186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.2027]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16030.0
Loss: 0.02221968211233616
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3340]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9819]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9810.0
Loss: 0.02273581549525261
Action 0 - predicted reward: tensor([[-0.0022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.2838]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8310.0
Loss: 0.009904582984745502
Action 0 - predicted reward: tensor([[-0.3114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0004]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10865.0
Loss: 0.015219156630337238
Action 0 - predicted reward: tensor([[0.6153]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9618]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14385.0
Loss: 0.0404009148478508
Greedy
Action 0 - predicted reward: tensor([[-0.0134]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.2373]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.00330588361248374
Action 0 - predicted reward: tensor([[-1.7629]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0066]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21965.0
Loss: 0.00375355058349669
Action 0 - predicted reward: tensor([[-0.6244]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0134]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7160.0
Loss: 8.460543540422805e-06
Action 0 - predicted reward: tensor([[-0.0118]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7954]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7120.0
Loss: 0.003132783342152834
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4445]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7718]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6770.0
Loss: 5431.1845703125
KL Divergence: 18.211610794067383
Action 0 - predicted reward: tensor([[2.4901]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9345]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7025.0
Loss: 5431.25732421875
KL Divergence: 18.261917114257812
Action 0 - predicted reward: tensor([[2.5139]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5819]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6027.32177734375
KL Divergence: 18.129920959472656
Action 0 - predicted reward: tensor([[2.5525]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0534]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5424.96533203125
KL Divergence: 18.0765438079834
16799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.7355]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-83.9852]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14950.0
Loss: 0.031122343614697456
Action 0 - predicted reward: tensor([[0.0719]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.8836]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 17920.0
Loss: 0.049773525446653366
Action 0 - predicted reward: tensor([[0.0191]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.1705]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15105.0
Loss: 0.017307285219430923
Action 0 - predicted reward: tensor([[-0.0372]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9517]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16070.0
Loss: 0.0213229451328516
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0916]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.9525]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9850.0
Loss: 0.020211409777402878
Action 0 - predicted reward: tensor([[0.0342]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.1542]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8310.0
Loss: 0.008253608830273151
Action 0 - predicted reward: tensor([[-0.0198]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.5602]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10870.0
Loss: 0.016467751935124397
Action 0 - predicted reward: tensor([[0.0199]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.0212]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 14425.0
Loss: 0.039596688002347946
Greedy
Action 0 - predicted reward: tensor([[-0.0442]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.6087]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.003320524701848626
Action 0 - predicted reward: tensor([[0.0235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 21965.0
Loss: 0.0037079118192195892
Action 0 - predicted reward: tensor([[0.0636]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9875]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7195.0
Loss: 0.0002855459460988641
Action 0 - predicted reward: tensor([[0.2854]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0257]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7120.0
Loss: 0.0031364744063466787
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4424]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0126]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6770.0
Loss: 5431.5595703125
KL Divergence: 18.20606803894043
Action 0 - predicted reward: tensor([[2.4903]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1638]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7030.0
Loss: 5431.60009765625
KL Divergence: 18.25907325744629
Action 0 - predicted reward: tensor([[2.5151]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1853]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6022.40576171875
KL Divergence: 18.11762809753418
Action 0 - predicted reward: tensor([[2.5601]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4331]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5422.443359375
KL Divergence: 18.073909759521484
16899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1470]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6971]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14990.0
Loss: 0.03425076603889465
Action 0 - predicted reward: tensor([[-0.6143]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.8520]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17930.0
Loss: 0.03906060755252838
Action 0 - predicted reward: tensor([[-0.0297]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.8549]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15150.0
Loss: 0.01673692837357521
Action 0 - predicted reward: tensor([[-0.0309]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9621]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16165.0
Loss: 0.02190411649644375
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2451]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0486]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9920.0
Loss: 0.016375061124563217
Action 0 - predicted reward: tensor([[-0.0421]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9489]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8310.0
Loss: 0.008254554122686386
Action 0 - predicted reward: tensor([[-0.1276]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0691]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10870.0
Loss: 0.015655381605029106
Action 0 - predicted reward: tensor([[-18.0592]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-79.6143]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14530.0
Loss: 0.046707700937986374
Greedy
Action 0 - predicted reward: tensor([[-0.0167]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.1161]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.0033051418140530586
Action 0 - predicted reward: tensor([[0.6238]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0482]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21970.0
Loss: 0.0036467930767685175
Action 0 - predicted reward: tensor([[0.0001]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.3385]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7195.0
Loss: 1.77037509274669e-05
Action 0 - predicted reward: tensor([[-0.0221]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9461]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7125.0
Loss: 0.0031304103322327137
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4560]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5102]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6775.0
Loss: 5431.541015625
KL Divergence: 18.207500457763672
Action 0 - predicted reward: tensor([[2.4819]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9166]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 5431.4990234375
KL Divergence: 18.262144088745117
Action 0 - predicted reward: tensor([[2.5055]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1762]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6026.27294921875
KL Divergence: 18.115365982055664
Action 0 - predicted reward: tensor([[2.5661]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8653]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5421.677734375
KL Divergence: 18.080617904663086
16999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2721]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8757]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15095.0
Loss: 0.034800607711076736
Action 0 - predicted reward: tensor([[0.2067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9381]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18000.0
Loss: 0.0425252765417099
Action 0 - predicted reward: tensor([[0.0534]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0579]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15155.0
Loss: 0.016506467014551163
Action 0 - predicted reward: tensor([[-0.0436]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8886]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16240.0
Loss: 0.022016402333974838
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.6707]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9990.0
Loss: 0.016200484707951546
Action 0 - predicted reward: tensor([[-0.6294]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9330]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8310.0
Loss: 0.008002197369933128
Action 0 - predicted reward: tensor([[-0.0322]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.8241]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10870.0
Loss: 0.018912741914391518
Action 0 - predicted reward: tensor([[0.0108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.0888]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14575.0
Loss: 0.04798376187682152
Greedy
Action 0 - predicted reward: tensor([[-0.0098]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.2318]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.0032782957423478365
Action 0 - predicted reward: tensor([[-1.8432]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9869]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21975.0
Loss: 0.0035612413194030523
Action 0 - predicted reward: tensor([[-0.1073]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9745]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7195.0
Loss: 1.2102404070901684e-05
Action 0 - predicted reward: tensor([[-0.0129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-68.1590]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7125.0
Loss: 0.003156467340886593
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4367]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1331]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6790.0
Loss: 5431.22705078125
KL Divergence: 18.200437545776367
Action 0 - predicted reward: tensor([[2.4804]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0921]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7040.0
Loss: 5430.71630859375
KL Divergence: 18.255096435546875
Action 0 - predicted reward: tensor([[2.5069]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1568]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6027.85693359375
KL Divergence: 18.114940643310547
Action 0 - predicted reward: tensor([[2.5886]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6380]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5423.2998046875
KL Divergence: 18.070308685302734
17099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0162]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.0432]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15235.0
Loss: 0.03662155941128731
Action 0 - predicted reward: tensor([[0.1573]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.8856]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18060.0
Loss: 0.036016207188367844
Action 0 - predicted reward: tensor([[0.0023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.8082]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15280.0
Loss: 0.016400782391428947
Action 0 - predicted reward: tensor([[-0.2519]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9392]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16275.0
Loss: 0.022859755903482437
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.7478]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9990.0
Loss: 0.016055185347795486
Action 0 - predicted reward: tensor([[0.1770]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9673]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8415.0
Loss: 0.01585770584642887
Action 0 - predicted reward: tensor([[-0.0062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.9062]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10870.0
Loss: 0.014739413745701313
Action 0 - predicted reward: tensor([[0.0273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.9004]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14680.0
Loss: 0.040429018437862396
Greedy
Action 0 - predicted reward: tensor([[0.0010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.3849]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.0032886434346437454
Action 0 - predicted reward: tensor([[-0.0663]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0137]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21975.0
Loss: 0.0035584259312599897
Action 0 - predicted reward: tensor([[-0.1736]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0285]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7195.0
Loss: 9.765837603481486e-06
Action 0 - predicted reward: tensor([[-0.1356]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0131]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7130.0
Loss: 0.0031305188313126564
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4437]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8748]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6790.0
Loss: 5431.93896484375
KL Divergence: 18.196882247924805
Action 0 - predicted reward: tensor([[2.4973]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5630]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7045.0
Loss: 5431.27099609375
KL Divergence: 18.2501220703125
Action 0 - predicted reward: tensor([[2.5221]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9814]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6025.5634765625
KL Divergence: 18.119050979614258
Action 0 - predicted reward: tensor([[2.5938]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6499]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5421.234375
KL Divergence: 18.067337036132812
17199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0345]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.5092]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15315.0
Loss: 0.03573119267821312
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 18105.0
Loss: 0.02937501110136509
Action 0 - predicted reward: tensor([[0.1152]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0883]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15355.0
Loss: 0.013972616754472256
Action 0 - predicted reward: tensor([[0.0220]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0721]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16325.0
Loss: 0.021959956735372543
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.8130]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10025.0
Loss: 0.01282786950469017
Action 0 - predicted reward: tensor([[0.1564]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0318]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8415.0
Loss: 0.011903546750545502
Action 0 - predicted reward: tensor([[0.1470]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4522]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10905.0
Loss: 0.014260372146964073
Action 0 - predicted reward: tensor([[-1.2252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9688]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14730.0
Loss: 0.041898537427186966
Greedy
Action 0 - predicted reward: tensor([[0.0018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.1860]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.003277120878919959
Action 0 - predicted reward: tensor([[-0.9203]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0337]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21975.0
Loss: 0.003483818843960762
Action 0 - predicted reward: tensor([[0.0018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.9211]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7195.0
Loss: 8.559044545108918e-06
Action 0 - predicted reward: tensor([[0.0079]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9935]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7140.0
Loss: 0.0031266517471522093
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4476]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7366]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6795.0
Loss: 5431.8076171875
KL Divergence: 18.20172882080078
Action 0 - predicted reward: tensor([[2.4727]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1017]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7065.0
Loss: 5432.046875
KL Divergence: 18.254302978515625
Action 0 - predicted reward: tensor([[2.5330]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0768]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6025.0693359375
KL Divergence: 18.11631965637207
Action 0 - predicted reward: tensor([[2.5922]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6436]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6240.0
Loss: 5421.326171875
KL Divergence: 18.074121475219727
17299.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15385.0
Loss: 0.0382247120141983
Action 0 - predicted reward: tensor([[0.0564]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.0954]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18210.0
Loss: 0.030145401135087013
Action 0 - predicted reward: tensor([[0.0804]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0217]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15355.0
Loss: 0.013168626464903355
Action 0 - predicted reward: tensor([[0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.7422]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16470.0
Loss: 0.021574480459094048
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0308]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.2301]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10045.0
Loss: 0.009832470677793026
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8415.0
Loss: 0.011932875029742718
Action 0 - predicted reward: tensor([[-0.2381]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7682]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10905.0
Loss: 0.014162654057145119
Action 0 - predicted reward: tensor([[0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.5105]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14735.0
Loss: 0.03064865991473198
Greedy
Action 0 - predicted reward: tensor([[-0.0091]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.0540]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.0032716079149395227
Action 0 - predicted reward: tensor([[-1.8617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9668]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21975.0
Loss: 0.0034591900184750557
Action 0 - predicted reward: tensor([[-0.0815]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9939]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7200.0
Loss: 8.295330189866945e-06
Action 0 - predicted reward: tensor([[-0.0126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.0463]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7140.0
Loss: 0.0031360492575913668
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4611]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6400]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6795.0
Loss: 5432.03076171875
KL Divergence: 18.20551872253418
Action 0 - predicted reward: tensor([[2.4915]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5596]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7070.0
Loss: 5431.28515625
KL Divergence: 18.24554443359375
Action 0 - predicted reward: tensor([[2.5573]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6010]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6024.88232421875
KL Divergence: 18.117399215698242
Action 0 - predicted reward: tensor([[2.5794]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1320]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5418.39501953125
KL Divergence: 18.07672882080078
17399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3269]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2038]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15425.0
Loss: 0.04072316363453865
Action 0 - predicted reward: tensor([[0.0021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.0365]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18255.0
Loss: 0.024096297100186348
Action 0 - predicted reward: tensor([[-0.0298]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.1737]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15405.0
Loss: 0.013487033545970917
Action 0 - predicted reward: tensor([[0.0147]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.4145]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16505.0
Loss: 0.025467239320278168
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9910]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10045.0
Loss: 0.009481173940002918
Action 0 - predicted reward: tensor([[0.0414]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.2371]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8415.0
Loss: 0.008566649630665779
Action 0 - predicted reward: tensor([[-0.0038]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.6592]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10940.0
Loss: 0.015929127112030983
Action 0 - predicted reward: tensor([[-1.0412]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9803]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14750.0
Loss: 0.02918870560824871
Greedy
Action 0 - predicted reward: tensor([[0.1230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0098]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 0.0032475420739501715
Action 0 - predicted reward: tensor([[-0.4985]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2512]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 21980.0
Loss: 0.0034284477587789297
Action 0 - predicted reward: tensor([[-0.0004]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.3643]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7200.0
Loss: 8.714690920896828e-06
Action 0 - predicted reward: tensor([[0.1154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0178]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7140.0
Loss: 0.003135154489427805
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4963]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5447]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6800.0
Loss: 5431.42822265625
KL Divergence: 18.203516006469727
Action 0 - predicted reward: tensor([[2.4888]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1464]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7070.0
Loss: 5431.6728515625
KL Divergence: 18.250917434692383
Action 0 - predicted reward: tensor([[2.5414]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1644]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6022.732421875
KL Divergence: 18.114561080932617
Action 0 - predicted reward: tensor([[2.5806]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8901]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5419.27734375
KL Divergence: 18.073230743408203
17499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1557]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2062]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15500.0
Loss: 0.0415993370115757
Action 0 - predicted reward: tensor([[0.2788]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2500]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18295.0
Loss: 0.024045061320066452
Action 0 - predicted reward: tensor([[0.0946]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0533]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15565.0
Loss: 0.013511297293007374
Action 0 - predicted reward: tensor([[-0.0159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0101]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16610.0
Loss: 0.027143817394971848
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2732]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2462]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10045.0
Loss: 0.009468796662986279
Action 0 - predicted reward: tensor([[-0.0024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.6333]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8420.0
Loss: 0.00797993317246437
Action 0 - predicted reward: tensor([[0.0333]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0267]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10985.0
Loss: 0.01869264245033264
Action 0 - predicted reward: tensor([[-9.0352]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9984]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14755.0
Loss: 0.02890542522072792
Greedy
Action 0 - predicted reward: tensor([[-0.0107]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.9378]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 0.0032335014548152685
Action 0 - predicted reward: tensor([[-1.5102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1228]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22020.0
Loss: 0.0034416434355080128
Action 0 - predicted reward: tensor([[-0.0332]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9677]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7205.0
Loss: 7.2981924859050196e-06
Action 0 - predicted reward: tensor([[0.0188]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.2789]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7140.0
Loss: 2.825127558025997e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4878]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5380]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6805.0
Loss: 5431.23193359375
KL Divergence: 18.193824768066406
Action 0 - predicted reward: tensor([[2.5129]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5713]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7070.0
Loss: 5430.57275390625
KL Divergence: 18.249685287475586
Action 0 - predicted reward: tensor([[2.5773]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6280]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6020.35888671875
KL Divergence: 18.11859893798828
Action 0 - predicted reward: tensor([[2.5841]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8957]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6240.0
Loss: 5417.9599609375
KL Divergence: 18.078828811645508
17599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7165]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15540.0
Loss: 0.045085277408361435
Action 0 - predicted reward: tensor([[-0.0744]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0734]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18380.0
Loss: 0.027550041675567627
Action 0 - predicted reward: tensor([[-0.0147]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5255]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15610.0
Loss: 0.015327418223023415
Action 0 - predicted reward: tensor([[0.0495]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0069]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16755.0
Loss: 0.024236639961600304
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0666]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10080.0
Loss: 0.013292803429067135
Action 0 - predicted reward: tensor([[0.0112]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.4393]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8420.0
Loss: 0.007916632108390331
Action 0 - predicted reward: tensor([[0.0093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.6737]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11020.0
Loss: 0.016559967771172523
Action 0 - predicted reward: tensor([[-1.0855]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9803]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14765.0
Loss: 0.02758404240012169
Greedy
Action 0 - predicted reward: tensor([[0.1231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0140]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 6.829056655988097e-05
Action 0 - predicted reward: tensor([[0.1948]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9555]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22020.0
Loss: 0.003089939709752798
Action 0 - predicted reward: tensor([[-0.0264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8399]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7205.0
Loss: 6.6728825913742185e-06
Action 0 - predicted reward: tensor([[0.0167]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.5588]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7140.0
Loss: 8.658915248815902e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4537]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7475]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6810.0
Loss: 5431.91943359375
KL Divergence: 18.200925827026367
Action 0 - predicted reward: tensor([[2.5001]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5603]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7075.0
Loss: 5432.0390625
KL Divergence: 18.245229721069336
Action 0 - predicted reward: tensor([[2.5751]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6361]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6020.830078125
KL Divergence: 18.115463256835938
Action 0 - predicted reward: tensor([[2.5928]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6472]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6275.0
Loss: 5719.12646484375
KL Divergence: 18.063058853149414
17699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0601]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.5725]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15585.0
Loss: 0.04128783196210861
Action 0 - predicted reward: tensor([[-0.9605]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6984]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18430.0
Loss: 0.021034715697169304
Action 0 - predicted reward: tensor([[0.0545]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0036]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15610.0
Loss: 0.013074022717773914
Action 0 - predicted reward: tensor([[0.0320]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6995]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16830.0
Loss: 0.017318177968263626
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7076]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10085.0
Loss: 0.013066605664789677
Action 0 - predicted reward: tensor([[0.0442]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8566]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8490.0
Loss: 0.008250150829553604
Action 0 - predicted reward: tensor([[-0.3321]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7220]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11020.0
Loss: 0.016257815062999725
Action 0 - predicted reward: tensor([[-0.0766]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9976]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14805.0
Loss: 0.029162831604480743
Greedy
Action 0 - predicted reward: tensor([[-0.0145]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0225]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 1.2369730029604398e-05
Action 0 - predicted reward: tensor([[0.4886]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9866]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22035.0
Loss: 0.00336196250282228
Action 0 - predicted reward: tensor([[0.0086]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.2868]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7210.0
Loss: 1.2757424883602653e-05
Action 0 - predicted reward: tensor([[-0.0126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.8792]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7140.0
Loss: 6.839390152890701e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4649]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5308]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6815.0
Loss: 5432.0068359375
KL Divergence: 18.192386627197266
Action 0 - predicted reward: tensor([[2.4881]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5481]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7100.0
Loss: 5431.20458984375
KL Divergence: 18.24542999267578
Action 0 - predicted reward: tensor([[2.5652]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6309]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6021.44287109375
KL Divergence: 18.11212921142578
Action 0 - predicted reward: tensor([[2.5936]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6440]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6275.0
Loss: 5719.97314453125
KL Divergence: 18.071361541748047
17799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0440]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.0172]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15655.0
Loss: 0.04022423177957535
Action 0 - predicted reward: tensor([[-0.6352]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9646]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18465.0
Loss: 0.019997121766209602
Action 0 - predicted reward: tensor([[-0.0115]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.9605]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15685.0
Loss: 0.013133103027939796
Action 0 - predicted reward: tensor([[0.0264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.9837]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16910.0
Loss: 0.014917785301804543
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5161]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0540]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10090.0
Loss: 0.012956310994923115
Action 0 - predicted reward: tensor([[0.3138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0461]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8495.0
Loss: 0.007860121317207813
Action 0 - predicted reward: tensor([[0.0065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.9429]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11055.0
Loss: 0.016263069584965706
Action 0 - predicted reward: tensor([[-7.0237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9796]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14810.0
Loss: 0.02311467006802559
Greedy
Action 0 - predicted reward: tensor([[0.0062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.8156]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 8.866933967510704e-06
Action 0 - predicted reward: tensor([[-0.1530]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0498]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22040.0
Loss: 0.003341080155223608
Action 0 - predicted reward: tensor([[0.0045]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9874]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7210.0
Loss: 7.490345979022095e-06
Action 0 - predicted reward: tensor([[0.0171]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.0096]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7145.0
Loss: 6.257640507101314e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4882]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5337]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6815.0
Loss: 5431.7158203125
KL Divergence: 18.194538116455078
Action 0 - predicted reward: tensor([[2.4957]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5580]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7100.0
Loss: 5431.98291015625
KL Divergence: 18.24129867553711
Action 0 - predicted reward: tensor([[2.5756]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6280]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6020.9755859375
KL Divergence: 18.10551643371582
Action 0 - predicted reward: tensor([[2.5628]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9078]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
Loss: 5722.3359375
KL Divergence: 18.065425872802734
17899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.0584]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15705.0
Loss: 0.036372363567352295
Action 0 - predicted reward: tensor([[0.2746]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1752]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18510.0
Loss: 0.013998338021337986
Action 0 - predicted reward: tensor([[0.2256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2555]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15730.0
Loss: 0.015458046458661556
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 16945.0
Loss: 0.016737502068281174
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0158]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.9235]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10090.0
Loss: 0.012959134764969349
Action 0 - predicted reward: tensor([[0.0164]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0732]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8565.0
Loss: 0.007857714779675007
Action 0 - predicted reward: tensor([[-0.2296]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0212]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11125.0
Loss: 0.019520269706845284
Action 0 - predicted reward: tensor([[0.1221]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.7408]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14880.0
Loss: 0.024155044928193092
Greedy
Action 0 - predicted reward: tensor([[-0.0067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.4984]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 7.331958386203041e-06
Action 0 - predicted reward: tensor([[-0.5256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9877]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22045.0
Loss: 0.0033125283662229776
Action 0 - predicted reward: tensor([[-0.0357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9705]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7210.0
Loss: 8.026764589885715e-06
Action 0 - predicted reward: tensor([[0.0018]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.0694]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7185.0
Loss: 1.4858002941764425e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4687]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0723]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6815.0
Loss: 5431.8486328125
KL Divergence: 18.193862915039062
Action 0 - predicted reward: tensor([[2.4958]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5563]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7100.0
Loss: 5431.12255859375
KL Divergence: 18.24759292602539
Action 0 - predicted reward: tensor([[2.5544]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6161]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6023.98974609375
KL Divergence: 18.11418914794922
Action 0 - predicted reward: tensor([[2.5524]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0071]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
Loss: 5723.9794921875
KL Divergence: 18.067277908325195
17999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0821]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.2850]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15865.0
Loss: 0.04310864582657814
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 18520.0
Loss: 0.013456041924655437
Action 0 - predicted reward: tensor([[-0.0414]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.6389]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15870.0
Loss: 0.014857024885714054
Action 0 - predicted reward: tensor([[0.0388]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0222]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17015.0
Loss: 0.018023669719696045
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0247]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.5538]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10090.0
Loss: 0.012853479012846947
Action 0 - predicted reward: tensor([[0.0186]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.4846]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8600.0
Loss: 0.007765533868223429
Action 0 - predicted reward: tensor([[-0.0762]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9590]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11125.0
Loss: 0.019357506185770035
Action 0 - predicted reward: tensor([[-13.7885]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9729]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14955.0
Loss: 0.02810233272612095
Greedy
Action 0 - predicted reward: tensor([[0.2333]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9771]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 6.8904391810065135e-06
Action 0 - predicted reward: tensor([[0.4593]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7440]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22060.0
Loss: 0.0033007764723151922
Action 0 - predicted reward: tensor([[0.1764]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0018]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7210.0
Loss: 6.5740673562686425e-06
Action 0 - predicted reward: tensor([[0.0951]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0041]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7225.0
Loss: 0.004296913277357817
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4680]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8101]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6815.0
Loss: 5431.45068359375
KL Divergence: 18.19532585144043
Action 0 - predicted reward: tensor([[2.4752]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2384]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7100.0
Loss: 5431.38427734375
KL Divergence: 18.239957809448242
Action 0 - predicted reward: tensor([[2.5850]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6305]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6021.22509765625
KL Divergence: 18.111570358276367
Action 0 - predicted reward: tensor([[2.5392]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9036]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
Loss: 5726.09326171875
KL Divergence: 18.061725616455078
18099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0064]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.2018]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15880.0
Loss: 0.034208688884973526
Action 0 - predicted reward: tensor([[-0.0601]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8549]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18535.0
Loss: 0.007576539646834135
Action 0 - predicted reward: tensor([[0.2477]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1788]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15985.0
Loss: 0.013949528336524963
Action 0 - predicted reward: tensor([[-0.0946]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.8190]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17065.0
Loss: 0.022137083113193512
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1883]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0002]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10090.0
Loss: 0.012919786386191845
Action 0 - predicted reward: tensor([[0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.4505]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8600.0
Loss: 0.007725922856479883
Action 0 - predicted reward: tensor([[0.1027]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9918]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11130.0
Loss: 0.01935715228319168
Action 0 - predicted reward: tensor([[-0.0863]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.2938]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14960.0
Loss: 0.025972645729780197
Greedy
Action 0 - predicted reward: tensor([[0.0756]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9831]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 1.7796113752410747e-05
Action 0 - predicted reward: tensor([[-0.0125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8446]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22070.0
Loss: 0.0032608334440737963
Action 0 - predicted reward: tensor([[-0.0213]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0871]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7210.0
Loss: 5.710543518944178e-06
Action 0 - predicted reward: tensor([[0.0085]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.9775]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7230.0
Loss: 0.003957338165491819
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4535]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7486]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6815.0
Loss: 5432.41455078125
KL Divergence: 18.18797492980957
Action 0 - predicted reward: tensor([[2.4835]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0781]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7100.0
Loss: 5430.87744140625
KL Divergence: 18.24102020263672
Action 0 - predicted reward: tensor([[2.5808]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6343]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 6019.73876953125
KL Divergence: 18.11503791809082
Action 0 - predicted reward: tensor([[2.5392]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1713]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
Loss: 5725.544921875
KL Divergence: 18.05292320251465
18199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1381]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6969]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15960.0
Loss: 0.03805084899067879
Action 0 - predicted reward: tensor([[-0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.0011]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18610.0
Loss: 0.007148647680878639
Action 0 - predicted reward: tensor([[-0.0509]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8887]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16095.0
Loss: 0.01725204475224018
Action 0 - predicted reward: tensor([[0.0588]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.0598]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17175.0
Loss: 0.021583110094070435
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1693]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1029]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10095.0
Loss: 0.010882142931222916
Action 0 - predicted reward: tensor([[0.0726]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.2571]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8670.0
Loss: 0.01235134620219469
Action 0 - predicted reward: tensor([[0.7943]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1347]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11130.0
Loss: 0.019220665097236633
Action 0 - predicted reward: tensor([[-0.1208]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.1261]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15005.0
Loss: 0.028093626722693443
Greedy
Action 0 - predicted reward: tensor([[0.1232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0009]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 6.926439709786791e-06
Action 0 - predicted reward: tensor([[-0.4891]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9574]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22080.0
Loss: 0.0032567523885518312
Action 0 - predicted reward: tensor([[-0.1874]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0280]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7210.0
Loss: 5.471528311318252e-06
Action 0 - predicted reward: tensor([[0.1012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0465]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7235.0
Loss: 0.0038050198927521706
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4909]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5414]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6815.0
Loss: 5431.0107421875
KL Divergence: 18.186471939086914
Action 0 - predicted reward: tensor([[2.5034]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5575]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7145.0
Loss: 5730.94482421875
KL Divergence: 18.24180793762207
Action 0 - predicted reward: tensor([[2.5546]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2589]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 6021.41064453125
KL Divergence: 18.109617233276367
Action 0 - predicted reward: tensor([[2.5488]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9514]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
Loss: 5723.78173828125
KL Divergence: 18.063398361206055
18299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1260]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0564]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15980.0
Loss: 0.03319608047604561
Action 0 - predicted reward: tensor([[-0.0336]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1012]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18615.0
Loss: 0.006787347141653299
Action 0 - predicted reward: tensor([[-0.1742]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9218]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16165.0
Loss: 0.013315923511981964
Action 0 - predicted reward: tensor([[-0.0518]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0901]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17250.0
Loss: 0.023592690005898476
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1798]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9680]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10135.0
Loss: 0.006852496415376663
Action 0 - predicted reward: tensor([[-0.0035]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.7757]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8710.0
Loss: 0.011430720798671246
Action 0 - predicted reward: tensor([[-0.1075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9854]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11130.0
Loss: 0.016026843339204788
Action 0 - predicted reward: tensor([[0.0032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.2276]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15050.0
Loss: 0.027712365612387657
Greedy
Action 0 - predicted reward: tensor([[-0.0055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.1093]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 8.043812158575747e-06
Action 0 - predicted reward: tensor([[0.0010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.0768]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22090.0
Loss: 0.0032368407119065523
Action 0 - predicted reward: tensor([[-0.2528]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9959]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7210.0
Loss: 4.902725777355954e-06
Action 0 - predicted reward: tensor([[0.0710]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1733]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7235.0
Loss: 0.0036655336152762175
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4670]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0401]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6815.0
Loss: 5432.01171875
KL Divergence: 18.197154998779297
Action 0 - predicted reward: tensor([[2.4818]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5518]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7150.0
Loss: 5725.55908203125
KL Divergence: 18.23691749572754
Action 0 - predicted reward: tensor([[2.5771]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1500]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 5717.8896484375
KL Divergence: 18.11011505126953
Action 0 - predicted reward: tensor([[2.5840]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6333]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6275.0
Loss: 5721.66845703125
KL Divergence: 18.055240631103516
18399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0602]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.8863]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16050.0
Loss: 0.03503495082259178
Action 0 - predicted reward: tensor([[0.0752]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6671]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18650.0
Loss: 0.007103677839040756
Action 0 - predicted reward: tensor([[-0.0022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0263]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16205.0
Loss: 0.016741180792450905
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17265.0
Loss: 0.022182529792189598
Epsilon Greedy 1%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10135.0
Loss: 0.0035180423874408007
Action 0 - predicted reward: tensor([[0.4785]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9469]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8750.0
Loss: 0.011504842899739742
Action 0 - predicted reward: tensor([[-0.0498]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.7039]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11130.0
Loss: 0.01602604053914547
Action 0 - predicted reward: tensor([[0.1063]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.4511]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15120.0
Loss: 0.032930344343185425
Greedy
Action 0 - predicted reward: tensor([[0.0036]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.8804]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 6.917644441273296e-06
Action 0 - predicted reward: tensor([[-0.0078]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.1361]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22095.0
Loss: 0.003226533532142639
Action 0 - predicted reward: tensor([[-0.0046]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.0345]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 0.00013257210957817733
Action 0 - predicted reward: tensor([[0.0051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.7722]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7235.0
Loss: 0.0035379452165216208
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4814]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5425]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6820.0
Loss: 5431.76416015625
KL Divergence: 18.18887710571289
Action 0 - predicted reward: tensor([[2.4821]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5510]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7150.0
Loss: 5730.76708984375
KL Divergence: 18.237470626831055
Action 0 - predicted reward: tensor([[2.5916]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6523]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5718.3583984375
KL Divergence: 18.112577438354492
Action 0 - predicted reward: tensor([[2.5849]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6298]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6275.0
Loss: 5723.5810546875
KL Divergence: 18.062108993530273
18499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2873]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.8798]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16125.0
Loss: 0.036481332033872604
Action 0 - predicted reward: tensor([[1.7046]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0588]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18725.0
Loss: 0.004567883443087339
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16280.0
Loss: 0.021036479622125626
Action 0 - predicted reward: tensor([[-0.1698]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.5277]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17350.0
Loss: 0.027623195201158524
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0229]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.9147]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10170.0
Loss: 0.0033527256455272436
Action 0 - predicted reward: tensor([[-0.0142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.7640]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8755.0
Loss: 0.011667581275105476
Action 0 - predicted reward: tensor([[0.0960]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9320]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11130.0
Loss: 0.01431563962250948
Action 0 - predicted reward: tensor([[-1.9379]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0150]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15190.0
Loss: 0.03772144019603729
Greedy
Action 0 - predicted reward: tensor([[-0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.3881]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 6.8927342908864375e-06
Action 0 - predicted reward: tensor([[-0.0155]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.5743]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22100.0
Loss: 0.0031908401288092136
Action 0 - predicted reward: tensor([[0.1122]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9586]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7245.0
Loss: 9.119295100390445e-06
Action 0 - predicted reward: tensor([[-0.0220]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.7308]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7240.0
Loss: 0.00349230389110744
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4968]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5540]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6820.0
Loss: 5431.5830078125
KL Divergence: 18.189353942871094
Action 0 - predicted reward: tensor([[2.4984]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5563]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7150.0
Loss: 5729.0546875
KL Divergence: 18.243167877197266
Action 0 - predicted reward: tensor([[2.5836]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0243]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 5717.56494140625
KL Divergence: 18.10732650756836
Action 0 - predicted reward: tensor([[2.5488]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0007]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6280.0
Loss: 5724.84814453125
KL Divergence: 18.04920768737793
18599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0701]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9429]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16235.0
Loss: 0.03540829196572304
Action 0 - predicted reward: tensor([[-0.7361]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0182]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18735.0
Loss: 0.0030928244814276695
Action 0 - predicted reward: tensor([[0.1491]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0890]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16425.0
Loss: 0.02688697539269924
Action 0 - predicted reward: tensor([[0.2076]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0301]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17425.0
Loss: 0.024257726967334747
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0792]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.2158]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10205.0
Loss: 0.007572762668132782
Action 0 - predicted reward: tensor([[0.0161]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.7921]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8795.0
Loss: 0.014713849872350693
Action 0 - predicted reward: tensor([[0.2889]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9868]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11165.0
Loss: 0.011077973060309887
Action 0 - predicted reward: tensor([[0.0131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.0803]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15230.0
Loss: 0.03374537453055382
Greedy
Action 0 - predicted reward: tensor([[-0.0199]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.6967]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 6.104643489379669e-06
Action 0 - predicted reward: tensor([[2.0454]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1210]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22105.0
Loss: 0.0031893523409962654
Action 0 - predicted reward: tensor([[0.0371]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9889]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7245.0
Loss: 7.2926336542877834e-06
Action 0 - predicted reward: tensor([[0.0175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.3492]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 0.003445269074290991
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4748]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9169]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6825.0
Loss: 5431.5322265625
KL Divergence: 18.183712005615234
Action 0 - predicted reward: tensor([[2.4766]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9500]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7160.0
Loss: 5730.9013671875
KL Divergence: 18.23427391052246
Action 0 - predicted reward: tensor([[2.5994]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6537]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5719.01708984375
KL Divergence: 18.108694076538086
Action 0 - predicted reward: tensor([[2.5759]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6205]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6280.0
Loss: 5723.5712890625
KL Divergence: 18.046661376953125
18699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0631]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.5534]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16340.0
Loss: 0.041849568486213684
Action 0 - predicted reward: tensor([[0.1128]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.8392]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18820.0
Loss: 0.00949911493808031
Action 0 - predicted reward: tensor([[-0.0215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8131]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16435.0
Loss: 0.02983865886926651
Action 0 - predicted reward: tensor([[0.0090]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9883]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17495.0
Loss: 0.030604291707277298
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5472]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1150]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10245.0
Loss: 0.00694418977946043
Action 0 - predicted reward: tensor([[0.0236]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.4409]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8835.0
Loss: 0.017296070232987404
Action 0 - predicted reward: tensor([[-0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9927]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11165.0
Loss: 0.009756258688867092
Action 0 - predicted reward: tensor([[4.1525]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9638]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15420.0
Loss: 0.0426919162273407
Greedy
Action 0 - predicted reward: tensor([[0.1669]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9761]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 5.7485190154693555e-06
Action 0 - predicted reward: tensor([[-0.0061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.4866]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22105.0
Loss: 0.0033717569895088673
Action 0 - predicted reward: tensor([[-0.0017]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.4437]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 6.311730430752505e-06
Action 0 - predicted reward: tensor([[-0.0237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9601]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7280.0
Loss: 8.772292494541034e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4759]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6906]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6830.0
Loss: 5431.138671875
KL Divergence: 18.185375213623047
Action 0 - predicted reward: tensor([[2.4851]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5534]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7170.0
Loss: 5730.3955078125
KL Divergence: 18.240392684936523
Action 0 - predicted reward: tensor([[2.5925]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4450]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 5715.56201171875
KL Divergence: 18.109704971313477
Action 0 - predicted reward: tensor([[2.5491]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8342]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6280.0
Loss: 5723.57568359375
KL Divergence: 18.055490493774414
18799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0301]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.7690]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16380.0
Loss: 0.03917713835835457
Action 0 - predicted reward: tensor([[-0.1917]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.9741]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18930.0
Loss: 0.009499536827206612
Action 0 - predicted reward: tensor([[0.0068]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.3395]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16590.0
Loss: 0.02846480906009674
Action 0 - predicted reward: tensor([[-0.1033]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.8334]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17575.0
Loss: 0.028960471972823143
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.3683]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10245.0
Loss: 0.006658915895968676
Action 0 - predicted reward: tensor([[-0.0478]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0530]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8840.0
Loss: 0.014615132473409176
Action 0 - predicted reward: tensor([[-0.0231]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.0819]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11165.0
Loss: 0.006538185756653547
Action 0 - predicted reward: tensor([[-0.0074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.9989]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15455.0
Loss: 0.04230332374572754
Greedy
Action 0 - predicted reward: tensor([[-0.0072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6240]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 5.5157020142360125e-06
Action 0 - predicted reward: tensor([[-0.0355]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8339]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22110.0
Loss: 0.003181047737598419
Action 0 - predicted reward: tensor([[-0.0338]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.3208]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 6.481228865595767e-06
Action 0 - predicted reward: tensor([[-0.0185]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.7677]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 2.3277134459931403e-05
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4977]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5547]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6835.0
Loss: 5431.5498046875
KL Divergence: 18.188167572021484
Action 0 - predicted reward: tensor([[2.4750]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1292]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7175.0
Loss: 5730.8798828125
KL Divergence: 18.23689842224121
Action 0 - predicted reward: tensor([[2.5981]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5636]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 5415.84228515625
KL Divergence: 18.100894927978516
Action 0 - predicted reward: tensor([[2.5476]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9974]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6280.0
Loss: 5723.87451171875
KL Divergence: 18.049617767333984
18899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8451]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16425.0
Loss: 0.0366300567984581
Action 0 - predicted reward: tensor([[-0.0700]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.7775]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18970.0
Loss: 0.0075804307125508785
Action 0 - predicted reward: tensor([[0.0713]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.1695]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16630.0
Loss: 0.024484621360898018
Action 0 - predicted reward: tensor([[0.0910]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9277]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17610.0
Loss: 0.03354242444038391
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0725]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9763]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10245.0
Loss: 0.0067360238172113895
Action 0 - predicted reward: tensor([[-0.0256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.2975]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8840.0
Loss: 0.014524592086672783
Action 0 - predicted reward: tensor([[0.2113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0115]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11200.0
Loss: 0.006520919501781464
Action 0 - predicted reward: tensor([[0.0894]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.1021]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 15475.0
Loss: 0.03604152798652649
Greedy
Action 0 - predicted reward: tensor([[0.0957]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0084]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 5.230528586253058e-06
Action 0 - predicted reward: tensor([[0.0021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.1403]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22110.0
Loss: 0.0031676285434514284
Action 0 - predicted reward: tensor([[-0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.0562]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 7.272395123436581e-06
Action 0 - predicted reward: tensor([[-0.0171]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.0804]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7285.0
Loss: 8.513598913850728e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5070]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5652]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6835.0
Loss: 5430.39501953125
KL Divergence: 18.193702697753906
Action 0 - predicted reward: tensor([[2.4724]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0559]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7180.0
Loss: 5730.0693359375
KL Divergence: 18.235267639160156
Action 0 - predicted reward: tensor([[2.5987]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1978]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 5416.5087890625
KL Divergence: 18.10411834716797
Action 0 - predicted reward: tensor([[2.5766]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6208]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6280.0
Loss: 5724.0009765625
KL Divergence: 18.051210403442383
18999.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16495.0
Loss: 0.04276929423213005
Action 0 - predicted reward: tensor([[0.0019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0735]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18985.0
Loss: 0.007130912505090237
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16640.0
Loss: 0.02360798418521881
Action 0 - predicted reward: tensor([[0.0698]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9927]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17650.0
Loss: 0.03312985599040985
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.6135]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10280.0
Loss: 0.010718914680182934
Action 0 - predicted reward: tensor([[0.0210]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3809]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 8910.0
Loss: 0.02460111491382122
Action 0 - predicted reward: tensor([[-0.0069]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.8640]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11235.0
Loss: 0.009837260469794273
Action 0 - predicted reward: tensor([[-0.2075]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.7860]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15525.0
Loss: 0.036285314708948135
Greedy
Action 0 - predicted reward: tensor([[-0.0130]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.4977]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 6.835733529442223e-06
Action 0 - predicted reward: tensor([[-0.6154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0088]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22125.0
Loss: 0.003170700976625085
Action 0 - predicted reward: tensor([[0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.6576]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 6.65230754748336e-06
Action 0 - predicted reward: tensor([[0.2772]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0059]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7285.0
Loss: 7.132010978239123e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4935]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8207]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6840.0
Loss: 5431.47900390625
KL Divergence: 18.17737579345703
Action 0 - predicted reward: tensor([[2.5130]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5584]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7185.0
Loss: 5730.86181640625
KL Divergence: 18.241363525390625
Action 0 - predicted reward: tensor([[2.6272]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6802]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5413.193359375
KL Divergence: 18.103227615356445
Action 0 - predicted reward: tensor([[2.5430]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0444]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6280.0
Loss: 5724.55419921875
KL Divergence: 18.051034927368164
19099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1318]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.2687]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16510.0
Loss: 0.037198755890131
Action 0 - predicted reward: tensor([[0.1614]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0664]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19095.0
Loss: 0.007354938890784979
Action 0 - predicted reward: tensor([[0.1320]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.9920]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16685.0
Loss: 0.0240066759288311
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[8.3034]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 17700.0
Loss: 0.03931195288896561
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0401]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0396]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10350.0
Loss: 0.010839488357305527
Action 0 - predicted reward: tensor([[0.0160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.2235]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8950.0
Loss: 0.021648986265063286
Action 0 - predicted reward: tensor([[0.0289]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.1912]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11270.0
Loss: 0.008388062939047813
Action 0 - predicted reward: tensor([[-7.4144]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9807]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15640.0
Loss: 0.031623102724552155
Greedy
Action 0 - predicted reward: tensor([[-0.0091]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.6495]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 6.469570507761091e-06
Action 0 - predicted reward: tensor([[-2.7478]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9884]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22130.0
Loss: 0.0031605090480297804
Action 0 - predicted reward: tensor([[-0.0008]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.8687]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 6.367746209434699e-06
Action 0 - predicted reward: tensor([[0.1156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9934]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7285.0
Loss: 6.002253485348774e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5238]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5701]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6855.0
Loss: 5430.994140625
KL Divergence: 18.17813491821289
Action 0 - predicted reward: tensor([[2.5157]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5764]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7185.0
Loss: 5729.61474609375
KL Divergence: 18.23678207397461
Action 0 - predicted reward: tensor([[2.6346]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6885]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5411.4443359375
KL Divergence: 18.101032257080078
Action 0 - predicted reward: tensor([[2.5393]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1073]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6280.0
Loss: 5725.728515625
KL Divergence: 18.04738426208496
19199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0936]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16620.0
Loss: 0.04544641822576523
Action 0 - predicted reward: tensor([[0.0803]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.3379]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19140.0
Loss: 0.006941892206668854
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 16770.0
Loss: 0.023683710023760796
Action 0 - predicted reward: tensor([[0.1187]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8963]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17745.0
Loss: 0.035273659974336624
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1132]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9613]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10385.0
Loss: 0.010598148219287395
Action 0 - predicted reward: tensor([[0.1289]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8950.0
Loss: 0.01783808134496212
Action 0 - predicted reward: tensor([[-0.8966]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0219]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11275.0
Loss: 0.007282867096364498
Action 0 - predicted reward: tensor([[-0.1965]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9640]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15675.0
Loss: 0.033295292407274246
Greedy
Action 0 - predicted reward: tensor([[0.0355]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.6922]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 6.276526164583629e-06
Action 0 - predicted reward: tensor([[1.6699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0266]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22135.0
Loss: 0.003154749982059002
Action 0 - predicted reward: tensor([[0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.2795]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 5.569732365984237e-06
Action 0 - predicted reward: tensor([[0.1999]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9977]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7285.0
Loss: 5.322126071405364e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4777]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2749]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6855.0
Loss: 5431.7822265625
KL Divergence: 18.169225692749023
Action 0 - predicted reward: tensor([[2.4966]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2183]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7200.0
Loss: 5729.2314453125
KL Divergence: 18.235004425048828
Action 0 - predicted reward: tensor([[2.6227]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1480]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 5408.9736328125
KL Divergence: 18.0999755859375
Action 0 - predicted reward: tensor([[2.5590]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6105]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6280.0
Loss: 5725.70068359375
KL Divergence: 18.036394119262695
19299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4323]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[12.9161]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16740.0
Loss: 0.04211067035794258
Action 0 - predicted reward: tensor([[-1.1849]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9747]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19190.0
Loss: 0.006828733719885349
Action 0 - predicted reward: tensor([[0.1136]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.0839]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16845.0
Loss: 0.023353010416030884
Action 0 - predicted reward: tensor([[-0.0265]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9662]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17785.0
Loss: 0.03183549642562866
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0762]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0055]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10420.0
Loss: 0.010098030790686607
Action 0 - predicted reward: tensor([[-0.1525]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0308]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8950.0
Loss: 0.017265809699892998
Action 0 - predicted reward: tensor([[-0.0019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9645]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11275.0
Loss: 0.0067629520781338215
Action 0 - predicted reward: tensor([[-0.6466]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0013]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15685.0
Loss: 0.027262793853878975
Greedy
Action 0 - predicted reward: tensor([[-0.0058]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.4511]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 6.745087830495322e-06
Action 0 - predicted reward: tensor([[-2.5125]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0156]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22135.0
Loss: 0.0031505750957876444
Action 0 - predicted reward: tensor([[0.0096]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0391]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7245.0
Loss: 5.196048277866794e-06
Action 0 - predicted reward: tensor([[-0.0181]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.4375]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7285.0
Loss: 5.191180207475554e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4918]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5525]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6860.0
Loss: 5431.3330078125
KL Divergence: 18.1740779876709
Action 0 - predicted reward: tensor([[2.5030]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5666]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7215.0
Loss: 5730.1708984375
KL Divergence: 18.225862503051758
Action 0 - predicted reward: tensor([[2.6361]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7011]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5408.32666015625
KL Divergence: 18.10736656188965
Action 0 - predicted reward: tensor([[2.5684]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6141]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6280.0
Loss: 5725.484375
KL Divergence: 18.036170959472656
19399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1541]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1595]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16955.0
Loss: 0.03831282630562782
Action 0 - predicted reward: tensor([[0.0289]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.2404]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19230.0
Loss: 0.011996160261332989
Action 0 - predicted reward: tensor([[0.0115]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0102]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16985.0
Loss: 0.027547083795070648
Action 0 - predicted reward: tensor([[0.0252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.6584]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17860.0
Loss: 0.036732617765665054
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1134]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8569]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10455.0
Loss: 0.01001405343413353
Action 0 - predicted reward: tensor([[-0.0371]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.0183]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9030.0
Loss: 0.024722496047616005
Action 0 - predicted reward: tensor([[0.1195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9065]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11310.0
Loss: 0.0068043977953493595
Action 0 - predicted reward: tensor([[-2.9400]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9605]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15700.0
Loss: 0.023203745484352112
Greedy
Action 0 - predicted reward: tensor([[-0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.7377]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 5.791312560177175e-06
Action 0 - predicted reward: tensor([[0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.8898]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22135.0
Loss: 0.003147589275613427
Action 0 - predicted reward: tensor([[-0.0927]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9799]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7245.0
Loss: 4.751478172693169e-06
Action 0 - predicted reward: tensor([[0.0386]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9973]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7285.0
Loss: 4.83322901345673e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5096]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5643]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6865.0
Loss: 5431.44384765625
KL Divergence: 18.17841148376465
Action 0 - predicted reward: tensor([[2.4853]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9953]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7220.0
Loss: 5730.515625
KL Divergence: 18.23291778564453
Action 0 - predicted reward: tensor([[2.6192]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4385]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 5409.4208984375
KL Divergence: 18.103378295898438
Action 0 - predicted reward: tensor([[2.5567]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6089]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6285.0
Loss: 5726.24169921875
KL Divergence: 18.0357608795166
19499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0838]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5830]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17130.0
Loss: 0.053127024322748184
Action 0 - predicted reward: tensor([[0.0911]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0368]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19305.0
Loss: 0.007719013839960098
Action 0 - predicted reward: tensor([[-0.1691]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9071]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17095.0
Loss: 0.02130255103111267
Action 0 - predicted reward: tensor([[-0.0408]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0497]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17945.0
Loss: 0.033421821892261505
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0715]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-78.0975]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10490.0
Loss: 0.01381626445800066
Action 0 - predicted reward: tensor([[0.1271]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8766]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9065.0
Loss: 0.02445092238485813
Action 0 - predicted reward: tensor([[0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.3572]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11380.0
Loss: 0.0036847181618213654
Action 0 - predicted reward: tensor([[0.1698]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1142]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15755.0
Loss: 0.02273241989314556
Greedy
Action 0 - predicted reward: tensor([[0.2948]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9721]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 5.176545528229326e-06
Action 0 - predicted reward: tensor([[-0.0056]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.1133]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22160.0
Loss: 0.0031463010236620903
Action 0 - predicted reward: tensor([[0.0035]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.9225]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 4.4799344323109835e-06
Action 0 - predicted reward: tensor([[-0.0610]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9964]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7285.0
Loss: 4.628606802725699e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5046]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5569]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6865.0
Loss: 5431.2119140625
KL Divergence: 18.17461585998535
Action 0 - predicted reward: tensor([[2.4917]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0380]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7230.0
Loss: 5728.88671875
KL Divergence: 18.240365982055664
Action 0 - predicted reward: tensor([[2.6328]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6936]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5409.6298828125
KL Divergence: 18.101110458374023
Action 0 - predicted reward: tensor([[2.5576]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6115]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6285.0
Loss: 5725.31982421875
KL Divergence: 18.036853790283203
19599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2488]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17245.0
Loss: 0.055172137916088104
Action 0 - predicted reward: tensor([[-0.0056]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.2261]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19340.0
Loss: 0.007351677864789963
Action 0 - predicted reward: tensor([[0.0597]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.2643]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17240.0
Loss: 0.024608470499515533
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17980.0
Loss: 0.03659569472074509
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1544]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9796]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10525.0
Loss: 0.010307380929589272
Action 0 - predicted reward: tensor([[0.4940]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9750]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9065.0
Loss: 0.02016296051442623
Action 0 - predicted reward: tensor([[-0.0061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0900]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11385.0
Loss: 0.003307537641376257
Action 0 - predicted reward: tensor([[0.0780]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.6543]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15760.0
Loss: 0.022890057414770126
Greedy
Action 0 - predicted reward: tensor([[0.1813]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0052]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 4.898052793578245e-06
Action 0 - predicted reward: tensor([[-0.0188]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.5683]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22160.0
Loss: 0.0031398909632116556
Action 0 - predicted reward: tensor([[-0.0045]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.9946]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 0.00374802784062922
Action 0 - predicted reward: tensor([[-0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.3823]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7285.0
Loss: 4.575438197207404e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5001]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5569]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6870.0
Loss: 5431.6455078125
KL Divergence: 18.176111221313477
Action 0 - predicted reward: tensor([[2.4815]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9915]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7230.0
Loss: 5730.7177734375
KL Divergence: 18.219881057739258
Action 0 - predicted reward: tensor([[2.6323]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6876]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5410.78125
KL Divergence: 18.090126037597656
Action 0 - predicted reward: tensor([[2.5448]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0995]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6285.0
Loss: 5724.408203125
KL Divergence: 18.037111282348633
19699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.3676]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17355.0
Loss: 0.05438041687011719
Action 0 - predicted reward: tensor([[-0.0241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0958]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 19485.0
Loss: 0.012811161577701569
Action 0 - predicted reward: tensor([[0.0410]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8907]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17290.0
Loss: 0.024503108114004135
Action 0 - predicted reward: tensor([[0.0708]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.4271]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18090.0
Loss: 0.03842717036604881
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1326]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9693]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10560.0
Loss: 0.013993975706398487
Action 0 - predicted reward: tensor([[0.0485]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.6737]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9065.0
Loss: 0.020050087943673134
Action 0 - predicted reward: tensor([[0.0113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.3420]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11385.0
Loss: 0.0032656951807439327
Action 0 - predicted reward: tensor([[-1.3854]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6313]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15835.0
Loss: 0.028689397498965263
Greedy
Action 0 - predicted reward: tensor([[-0.0080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.6310]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6385.0
Loss: 4.666401764552575e-06
Action 0 - predicted reward: tensor([[-0.6285]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0866]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22160.0
Loss: 0.0031497369054704905
Action 0 - predicted reward: tensor([[0.0065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0615]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7280.0
Loss: 0.0036626385990530252
Action 0 - predicted reward: tensor([[-0.1783]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0310]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7285.0
Loss: 5.5003938541631214e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4843]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9370]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6875.0
Loss: 5431.23974609375
KL Divergence: 18.171653747558594
Action 0 - predicted reward: tensor([[2.4854]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2077]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7235.0
Loss: 5729.0185546875
KL Divergence: 18.223529815673828
Action 0 - predicted reward: tensor([[2.6446]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7076]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5404.5029296875
KL Divergence: 18.099544525146484
Action 0 - predicted reward: tensor([[2.5538]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9479]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6285.0
Loss: 5722.8837890625
KL Divergence: 18.04119873046875
19799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0797]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7790]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17460.0
Loss: 0.06050541624426842
Action 0 - predicted reward: tensor([[-0.0037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0022]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19525.0
Loss: 0.007513406686484814
Action 0 - predicted reward: tensor([[-0.0362]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.1306]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17395.0
Loss: 0.02477949485182762
Action 0 - predicted reward: tensor([[0.0834]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7442]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18160.0
Loss: 0.03709090128540993
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0686]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9553]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10600.0
Loss: 0.012422783300280571
Action 0 - predicted reward: tensor([[0.8019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9923]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9110.0
Loss: 0.02438432164490223
Action 0 - predicted reward: tensor([[-0.0168]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.4629]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11420.0
Loss: 0.007235263008624315
Action 0 - predicted reward: tensor([[4.1718]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7195]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 15885.0
Loss: 0.023561472073197365
Greedy
Action 0 - predicted reward: tensor([[0.2819]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0169]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 4.81126789964037e-06
Action 0 - predicted reward: tensor([[-0.0241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.7958]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22165.0
Loss: 0.0031534896697849035
Action 0 - predicted reward: tensor([[-0.0047]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.1162]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 0.0035746702924370766
Action 0 - predicted reward: tensor([[-0.0176]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9931]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7285.0
Loss: 5.447953753900947e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4815]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9232]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6875.0
Loss: 5431.18701171875
KL Divergence: 18.17252540588379
Action 0 - predicted reward: tensor([[2.5060]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5558]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7245.0
Loss: 5725.2392578125
KL Divergence: 18.22547721862793
Action 0 - predicted reward: tensor([[2.6324]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4192]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7060.0
Loss: 5405.7421875
KL Divergence: 18.094655990600586
Action 0 - predicted reward: tensor([[2.5645]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6106]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6285.0
Loss: 5725.79345703125
KL Divergence: 18.03839683532715
19899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0529]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0733]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17570.0
Loss: 0.06189502030611038
Action 0 - predicted reward: tensor([[0.3050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9888]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19595.0
Loss: 0.0044080945663154125
Action 0 - predicted reward: tensor([[0.0461]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.6763]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17430.0
Loss: 0.02058017998933792
Action 0 - predicted reward: tensor([[0.0399]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0060]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18235.0
Loss: 0.03640883415937424
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0473]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.6539]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10600.0
Loss: 0.010382997803390026
Action 0 - predicted reward: tensor([[0.0020]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.1413]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9155.0
Loss: 0.01736164093017578
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 11425.0
Loss: 0.007729571778327227
Action 0 - predicted reward: tensor([[0.2241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.9014]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15925.0
Loss: 0.027045924216508865
Greedy
Action 0 - predicted reward: tensor([[0.1081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9989]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 4.854935014009243e-06
Action 0 - predicted reward: tensor([[0.0955]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7533]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 22175.0
Loss: 0.0031426316127181053
Action 0 - predicted reward: tensor([[-0.0042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.2968]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 0.0035253576934337616
Action 0 - predicted reward: tensor([[-0.0045]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.9471]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7285.0
Loss: 4.374629043013556e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4788]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9823]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6875.0
Loss: 5431.900390625
KL Divergence: 18.175865173339844
Action 0 - predicted reward: tensor([[2.4993]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5582]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7255.0
Loss: 5730.1591796875
KL Divergence: 18.224760055541992
Action 0 - predicted reward: tensor([[2.6470]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7032]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5407.94873046875
KL Divergence: 18.094850540161133
Action 0 - predicted reward: tensor([[2.5515]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.6030]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6285.0
Loss: 5726.48974609375
KL Divergence: 18.03562355041504
19999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0418]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0104]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17650.0
Loss: 0.05934085324406624
Action 0 - predicted reward: tensor([[0.0526]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.9738]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19670.0
Loss: 0.011416303925216198
Action 0 - predicted reward: tensor([[0.0386]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8024]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17505.0
Loss: 0.021356673911213875
Action 0 - predicted reward: tensor([[-0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.7677]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18285.0
Loss: 0.03320902958512306
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4098]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9832]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10600.0
Loss: 0.010238097049295902
Action 0 - predicted reward: tensor([[-0.0326]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.4120]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9165.0
Loss: 0.013192103244364262
Action 0 - predicted reward: tensor([[0.0548]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.7163]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11460.0
Loss: 0.010634290985763073
Action 0 - predicted reward: tensor([[0.0377]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-112.2363]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15925.0
Loss: 0.027244970202445984
Greedy
Action 0 - predicted reward: tensor([[0.1509]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9988]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6385.0
Loss: 4.7286926019296516e-06
Action 0 - predicted reward: tensor([[-0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.1901]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 22175.0
Loss: 0.003134577302262187
Action 0 - predicted reward: tensor([[0.0056]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.4335]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7280.0
Loss: 0.0034183217212557793
Action 0 - predicted reward: tensor([[0.1647]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9685]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7285.0
Loss: 4.557291049422929e-06
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4763]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1460]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6890.0
Loss: 5431.5068359375
KL Divergence: 18.168237686157227
Action 0 - predicted reward: tensor([[2.4840]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5525]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7255.0
Loss: 5730.3759765625
KL Divergence: 18.21388816833496
Action 0 - predicted reward: tensor([[2.6528]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.7058]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 5406.3994140625
KL Divergence: 18.095645904541016
Action 0 - predicted reward: tensor([[2.5330]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9006]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6285.0
Loss: 5726.1923828125
KL Divergence: 18.023696899414062
