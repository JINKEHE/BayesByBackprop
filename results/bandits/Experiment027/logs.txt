Use GPU: False
1.0.1.post2
99.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1383]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1349]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 280.0
Loss: 0.5881057977676392
Action 0 - predicted reward: tensor([[-0.4114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4504]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 305.0
Loss: 0.49645110964775085
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.9097]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.9989]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 405.0
Loss: 1.264815092086792
Action 0 - predicted reward: tensor([[-3.0550]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.4313]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 375.0
Loss: 0.5219333171844482
Greedy
Action 0 - predicted reward: tensor([[-0.7026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7307]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 390.0
Loss: 2.132138252258301
Action 0 - predicted reward: tensor([[-0.6674]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7303]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 280.0
Loss: 0.33049720525741577
Bayes by Backprop
Action 0 - predicted reward: tensor([[-2.0229]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.0604]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 485.0
Loss: 203769.90625
KL Divergence: 716.1201782226562
Action 0 - predicted reward: tensor([[-1.2957]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.3085]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 410.0
Loss: 133117.078125
KL Divergence: 713.3765869140625
199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.6780]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7806]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 670.0
Loss: 0.5765125155448914
Action 0 - predicted reward: tensor([[-0.0288]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1267]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 545.0
Loss: 0.26586228609085083
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.5312]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.9439]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 650.0
Loss: 0.8963331580162048
Action 0 - predicted reward: tensor([[0.2943]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1616]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 665.0
Loss: 0.20384572446346283
Greedy
Action 0 - predicted reward: tensor([[0.7951]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.5739]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 660.0
Loss: 0.582877516746521
Action 0 - predicted reward: tensor([[-0.1429]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2407]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 525.0
Loss: 0.09195069223642349
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.1513]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1676]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 780.0
Loss: 117156.921875
KL Divergence: 353.8549499511719
Action 0 - predicted reward: tensor([[-0.6899]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6973]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 685.0
Loss: 62279.625
KL Divergence: 353.310791015625
299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2150]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4347]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 895.0
Loss: 0.35948750376701355
Action 0 - predicted reward: tensor([[-0.2467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.2700]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 825.0
Loss: 0.19227133691310883
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3739]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.4982]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 895.0
Loss: 0.08487231284379959
Action 0 - predicted reward: tensor([[0.2052]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.4339]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 935.0
Loss: 0.043602827936410904
Greedy
Action 0 - predicted reward: tensor([[-9.5277]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5244]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 910.0
Loss: 0.16753028333187103
Action 0 - predicted reward: tensor([[-0.0508]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2568]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 780.0
Loss: 0.057733744382858276
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.6688]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6725]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1045.0
Loss: 81228.8828125
KL Divergence: 234.04071044921875
Action 0 - predicted reward: tensor([[-0.5263]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5296]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 960.0
Loss: 59383.0078125
KL Divergence: 234.6957550048828
399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.1390]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1205.0
Loss: 0.11153347790241241
Action 0 - predicted reward: tensor([[-0.0746]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.8281]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1125.0
Loss: 0.044781871140003204
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3867]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3461]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1110.0
Loss: 0.024409431964159012
Action 0 - predicted reward: tensor([[0.2857]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1376]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1205.0
Loss: 0.0068997591733932495
Greedy
Action 0 - predicted reward: tensor([[1.6173]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.9946]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1080.0
Loss: 0.08940935879945755
Action 0 - predicted reward: tensor([[0.0814]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0169]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1015.0
Loss: 0.04348241165280342
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.7094]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7164]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1325.0
Loss: 89832.421875
KL Divergence: 175.0067138671875
Action 0 - predicted reward: tensor([[-0.3551]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3593]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1170.0
Loss: 45293.2734375
KL Divergence: 175.0840606689453
499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.6412]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1370.0
Loss: 0.029382342472672462
Action 0 - predicted reward: tensor([[0.0218]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5904]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1375.0
Loss: 0.01248194556683302
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2342]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.1672]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1430.0
Loss: 0.17357850074768066
Action 0 - predicted reward: tensor([[-0.2202]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.3841]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1430.0
Loss: 0.01024297159165144
Greedy
Action 0 - predicted reward: tensor([[0.0779]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.1214]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1090.0
Loss: 0.02787640504539013
Action 0 - predicted reward: tensor([[0.2305]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1093]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1280.0
Loss: 0.02849426679313183
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.7738]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.7693]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1695.0
Loss: 93843.5546875
KL Divergence: 139.70236206054688
Action 0 - predicted reward: tensor([[-0.3091]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3149]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1440.0
Loss: 41674.3046875
KL Divergence: 139.66966247558594
599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.4581]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1650.0
Loss: 0.22999630868434906
Action 0 - predicted reward: tensor([[0.1379]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3460]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1735.0
Loss: 0.11662310361862183
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.6840]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6075]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1650.0
Loss: 0.10487334430217743
Action 0 - predicted reward: tensor([[0.1103]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0179]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1690.0
Loss: 0.004636568948626518
Greedy
Action 0 - predicted reward: tensor([[0.2247]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0811]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1130.0
Loss: 0.026281848549842834
Action 0 - predicted reward: tensor([[-0.1751]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5724]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1575.0
Loss: 0.019817478954792023
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.6752]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6783]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1990.0
Loss: 87718.1328125
KL Divergence: 116.09854125976562
Action 0 - predicted reward: tensor([[-0.3466]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3463]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1740.0
Loss: 50527.015625
KL Divergence: 116.17153930664062
699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0737]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8053]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1805.0
Loss: 0.0918772742152214
Action 0 - predicted reward: tensor([[0.1327]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.5070]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2010.0
Loss: 0.14551350474357605
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.6714]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.6749]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1855.0
Loss: 0.06306877732276917
Action 0 - predicted reward: tensor([[0.1681]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4706]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1985.0
Loss: 0.1897350698709488
Greedy
Action 0 - predicted reward: tensor([[-0.1878]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6125]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1210.0
Loss: 0.03572768718004227
Action 0 - predicted reward: tensor([[0.0173]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1271]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1865.0
Loss: 0.013199079781770706
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.4480]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4552]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2220.0
Loss: 79041.375
KL Divergence: 99.418212890625
Action 0 - predicted reward: tensor([[-0.3260]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3316]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2040.0
Loss: 47975.6796875
KL Divergence: 99.381103515625
799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4380]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7294]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2010.0
Loss: 0.12610556185245514
Action 0 - predicted reward: tensor([[0.2478]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.5709]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2350.0
Loss: 0.23633848130702972
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.2508]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1945.0
Loss: 0.05625751614570618
Action 0 - predicted reward: tensor([[0.0161]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1736]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2360.0
Loss: 0.2746260166168213
Greedy
Action 0 - predicted reward: tensor([[-0.0585]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9877]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1220.0
Loss: 0.021513555198907852
Action 0 - predicted reward: tensor([[0.0301]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1648]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2130.0
Loss: 0.007512413896620274
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3324]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3350]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2415.0
Loss: 67500.375
KL Divergence: 86.733642578125
Action 0 - predicted reward: tensor([[-0.3192]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3199]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2310.0
Loss: 50123.05078125
KL Divergence: 86.76130676269531
899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.6887]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.7122]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2130.0
Loss: 0.03300538286566734
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2575.0
Loss: 0.18099616467952728
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1170]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6194]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2210.0
Loss: 0.09628112614154816
Action 0 - predicted reward: tensor([[0.1232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.7186]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2475.0
Loss: 0.1747620701789856
Greedy
Action 0 - predicted reward: tensor([[0.0737]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0220]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1365.0
Loss: 0.05209086462855339
Action 0 - predicted reward: tensor([[-0.0665]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2485]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2400.0
Loss: 0.0034390974324196577
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3501]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3493]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2795.0
Loss: 80465.09375
KL Divergence: 77.01142120361328
Action 0 - predicted reward: tensor([[-0.1674]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1667]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2500.0
Loss: 45989.6796875
KL Divergence: 77.05534362792969
999.
Epsilon Greedy 5%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2215.0
Loss: 0.02421061135828495
Action 0 - predicted reward: tensor([[0.8077]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0421]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2780.0
Loss: 0.10286964476108551
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.6662]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.4938]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2300.0
Loss: 0.059642449021339417
Action 0 - predicted reward: tensor([[0.1765]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2131]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2610.0
Loss: 0.05738101527094841
Greedy
Action 0 - predicted reward: tensor([[0.0350]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.8815]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1370.0
Loss: 0.027994800359010696
Action 0 - predicted reward: tensor([[-0.0243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1486]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2635.0
Loss: 0.001341859227977693
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3190]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3244]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3080.0
Loss: 79754.3203125
KL Divergence: 69.1662826538086
Action 0 - predicted reward: tensor([[-0.1940]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1963]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2850.0
Loss: 54187.19921875
KL Divergence: 69.2264404296875
1099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[1.4227]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.8640]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2360.0
Loss: 0.057311128824949265
Action 0 - predicted reward: tensor([[1.1493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6174]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2945.0
Loss: 0.03883010149002075
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0834]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0274]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2355.0
Loss: 0.03769306838512421
Action 0 - predicted reward: tensor([[-0.4859]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5595]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2770.0
Loss: 0.048078980296850204
Greedy
Action 0 - predicted reward: tensor([[-0.3021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4960]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1415.0
Loss: 0.0529586561024189
Action 0 - predicted reward: tensor([[0.0400]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0652]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2920.0
Loss: 0.0006293854676187038
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3340]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3365]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3385.0
Loss: 86317.7578125
KL Divergence: 62.75286865234375
Action 0 - predicted reward: tensor([[-0.1686]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1662]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3115.0
Loss: 58288.5
KL Divergence: 62.829376220703125
1199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-1.1029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.1868]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2540.0
Loss: 0.04747842252254486
Action 0 - predicted reward: tensor([[-0.1401]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.6361]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3125.0
Loss: 0.04450805112719536
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4466]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.5670]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2425.0
Loss: 0.031819261610507965
Action 0 - predicted reward: tensor([[0.4502]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.7611]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2920.0
Loss: 0.07183877378702164
Greedy
Action 0 - predicted reward: tensor([[0.0599]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0785]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1450.0
Loss: 0.030325742438435555
Action 0 - predicted reward: tensor([[-0.0549]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1915]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3200.0
Loss: 0.0014352193102240562
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.2946]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2979]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3655.0
Loss: 85087.703125
KL Divergence: 57.47969436645508
Action 0 - predicted reward: tensor([[-0.1859]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1851]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3420.0
Loss: 62753.15234375
KL Divergence: 57.53044128417969
1299.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2620.0
Loss: 0.0053154481574893
Action 0 - predicted reward: tensor([[0.1185]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6101]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3175.0
Loss: 0.0313883051276207
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.5877]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.7632]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2495.0
Loss: 0.05921483412384987
Action 0 - predicted reward: tensor([[-0.0578]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2107]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3030.0
Loss: 0.05015352740883827
Greedy
Action 0 - predicted reward: tensor([[-0.0826]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0354]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1485.0
Loss: 0.023988112807273865
Action 0 - predicted reward: tensor([[0.0677]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1694]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3495.0
Loss: 0.015007010661065578
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.2149]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2185]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3825.0
Loss: 81530.5390625
KL Divergence: 52.97727584838867
Action 0 - predicted reward: tensor([[-0.2254]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2245]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3725.0
Loss: 68809.359375
KL Divergence: 53.10305404663086
1399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.7093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4153]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2700.0
Loss: 0.016981851309537888
Action 0 - predicted reward: tensor([[1.0403]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.9512]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3400.0
Loss: 0.03630410507321358
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2259]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.0013]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2675.0
Loss: 0.05715120956301689
Action 0 - predicted reward: tensor([[0.4052]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.5780]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3215.0
Loss: 0.045825909823179245
Greedy
Action 0 - predicted reward: tensor([[-0.1317]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1334]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1485.0
Loss: 0.021285759285092354
Action 0 - predicted reward: tensor([[0.0106]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.8979]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3765.0
Loss: 0.014425989240407944
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.1410]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1378]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4005.0
Loss: 76026.8828125
KL Divergence: 49.15345001220703
Action 0 - predicted reward: tensor([[-0.2045]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2070]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3995.0
Loss: 71257.2421875
KL Divergence: 49.226417541503906
1499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0201]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.2816]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2845.0
Loss: 0.034324679523706436
Action 0 - predicted reward: tensor([[-0.1471]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.1926]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3480.0
Loss: 0.02758728712797165
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0191]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.0580]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2780.0
Loss: 0.05952812731266022
Action 0 - predicted reward: tensor([[0.0432]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2350]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 3370.0
Loss: 0.03731423616409302
Greedy
Action 0 - predicted reward: tensor([[0.1597]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.4893]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1525.0
Loss: 0.029912803322076797
Action 0 - predicted reward: tensor([[-0.0941]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5545]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4055.0
Loss: 0.012048821896314621
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.1250]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1419]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4155.0
Loss: 70672.6796875
KL Divergence: 45.86238098144531
Action 0 - predicted reward: tensor([[-0.2177]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2226]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4255.0
Loss: 72117.46875
KL Divergence: 45.902130126953125
1599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0460]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5421]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2855.0
Loss: 0.02004896104335785
Action 0 - predicted reward: tensor([[0.2496]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7206]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3645.0
Loss: 0.04311766102910042
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0505]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3424]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2895.0
Loss: 0.06679844111204147
Action 0 - predicted reward: tensor([[-1.6458]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.2580]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3475.0
Loss: 0.03518173098564148
Greedy
Action 0 - predicted reward: tensor([[0.1619]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9748]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1575.0
Loss: 0.020984429866075516
Action 0 - predicted reward: tensor([[0.1042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6180]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4325.0
Loss: 0.0061489581130445
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.0482]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0699]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4315.0
Loss: 68134.5703125
KL Divergence: 42.96686553955078
Action 0 - predicted reward: tensor([[-0.1496]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1550]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4450.0
Loss: 71238.296875
KL Divergence: 42.99640655517578
1699.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2925.0
Loss: 0.035276543349027634
Action 0 - predicted reward: tensor([[-0.1990]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6159]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3790.0
Loss: 0.028842531144618988
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3738]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.1509]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3045.0
Loss: 0.052395306527614594
Action 0 - predicted reward: tensor([[0.1493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.3952]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3690.0
Loss: 0.0485561341047287
Greedy
Action 0 - predicted reward: tensor([[-0.0493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8319]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1580.0
Loss: 0.01804729364812374
Action 0 - predicted reward: tensor([[0.1138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.0101]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4560.0
Loss: 0.002151763765141368
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.0559]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0568]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4520.0
Loss: 64120.359375
KL Divergence: 40.39979934692383
Action 0 - predicted reward: tensor([[-0.0266]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0280]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4600.0
Loss: 69842.375
KL Divergence: 40.44182586669922
1799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0140]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.2505]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3000.0
Loss: 0.030158914625644684
Action 0 - predicted reward: tensor([[0.0849]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8909]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3905.0
Loss: 0.04832197725772858
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0763]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9150]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3115.0
Loss: 0.040805425494909286
Action 0 - predicted reward: tensor([[0.1812]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2472]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3865.0
Loss: 0.04605865478515625
Greedy
Action 0 - predicted reward: tensor([[-0.1257]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9602]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1580.0
Loss: 0.016992272809147835
Action 0 - predicted reward: tensor([[0.0138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3477]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4810.0
Loss: 0.0007477581384591758
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.0363]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0309]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4700.0
Loss: 61825.44140625
KL Divergence: 38.18762969970703
Action 0 - predicted reward: tensor([[0.0542]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0524]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4715.0
Loss: 66883.15625
KL Divergence: 38.158504486083984
1899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3590]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6086]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3080.0
Loss: 0.020671233534812927
Action 0 - predicted reward: tensor([[0.1299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.4608]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3975.0
Loss: 0.04675794765353203
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.4097]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3235.0
Loss: 0.038059305399656296
Action 0 - predicted reward: tensor([[-0.1733]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7747]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3975.0
Loss: 0.04409962520003319
Greedy
Action 0 - predicted reward: tensor([[0.0712]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.3522]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1615.0
Loss: 0.0205382052809
Action 0 - predicted reward: tensor([[-0.0029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2948]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5110.0
Loss: 0.00032239375286735594
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.1083]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0274]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4880.0
Loss: 58231.00390625
KL Divergence: 36.158775329589844
Action 0 - predicted reward: tensor([[0.0878]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0787]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4910.0
Loss: 65649.8671875
KL Divergence: 36.090206146240234
1999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.8433]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8528]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3190.0
Loss: 0.016444465145468712
Action 0 - predicted reward: tensor([[0.0213]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.6960]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3980.0
Loss: 0.036819588392972946
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0831]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1482]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3320.0
Loss: 0.040290579199790955
Action 0 - predicted reward: tensor([[0.1481]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1392]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4125.0
Loss: 0.05517878383398056
Greedy
Action 0 - predicted reward: tensor([[-0.4244]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4629]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1825.0
Loss: 0.3009333312511444
Action 0 - predicted reward: tensor([[0.0246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5655]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5380.0
Loss: 0.00023378903279080987
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.2080]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1281]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5000.0
Loss: 57093.44140625
KL Divergence: 34.366546630859375
Action 0 - predicted reward: tensor([[0.1630]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1433]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5045.0
Loss: 62119.0390625
KL Divergence: 34.33570861816406
2099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7687]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3230.0
Loss: 0.014839538373053074
Action 0 - predicted reward: tensor([[-0.1476]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4296]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4225.0
Loss: 0.044542260468006134
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.1395]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3500.0
Loss: 0.06682327389717102
Action 0 - predicted reward: tensor([[-0.2354]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9115]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4165.0
Loss: 0.049630746245384216
Greedy
Action 0 - predicted reward: tensor([[0.1076]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.0033]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2315.0
Loss: 0.30018752813339233
Action 0 - predicted reward: tensor([[-0.0080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2468]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5625.0
Loss: 0.00020073696214240044
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.2492]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2273]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5125.0
Loss: 54894.640625
KL Divergence: 32.76047897338867
Action 0 - predicted reward: tensor([[0.2310]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2335]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5150.0
Loss: 60799.26171875
KL Divergence: 32.65303039550781
2199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1307]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.5304]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3310.0
Loss: 0.021706823259592056
Action 0 - predicted reward: tensor([[-0.1428]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.5535]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4370.0
Loss: 0.037747904658317566
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2585]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.4311]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3585.0
Loss: 0.04903815686702728
Action 0 - predicted reward: tensor([[0.0552]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9731]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4275.0
Loss: 0.04870131239295006
Greedy
Action 0 - predicted reward: tensor([[-1.9021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.4357]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2325.0
Loss: 0.15026186406612396
Action 0 - predicted reward: tensor([[-0.0187]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2035]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5850.0
Loss: 0.0001832782436395064
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.2758]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0621]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5295.0
Loss: 57599.0
KL Divergence: 31.261367797851562
Action 0 - predicted reward: tensor([[0.3072]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3157]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5190.0
Loss: 59361.703125
KL Divergence: 31.140254974365234
2299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1489]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0193]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3455.0
Loss: 0.03567146137356758
Action 0 - predicted reward: tensor([[0.1841]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6900]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4480.0
Loss: 0.03295818343758583
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2205]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2556]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3660.0
Loss: 0.04096085950732231
Action 0 - predicted reward: tensor([[-0.1814]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9792]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4345.0
Loss: 0.03350461646914482
Greedy
Action 0 - predicted reward: tensor([[0.4887]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7218]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2325.0
Loss: 0.06130364537239075
Action 0 - predicted reward: tensor([[0.0153]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1355]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6130.0
Loss: 0.00017048435984179378
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.3717]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2495]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5410.0
Loss: 60040.7578125
KL Divergence: 29.923063278198242
Action 0 - predicted reward: tensor([[0.3559]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3084]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5265.0
Loss: 59034.26171875
KL Divergence: 29.796579360961914
2399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1860]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9237]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3570.0
Loss: 0.03210034593939781
Action 0 - predicted reward: tensor([[0.0975]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.2337]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4585.0
Loss: 0.035707294940948486
Epsilon Greedy 1%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3700.0
Loss: 0.040490634739398956
Action 0 - predicted reward: tensor([[0.1365]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.3037]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4380.0
Loss: 0.03901128098368645
Greedy
Action 0 - predicted reward: tensor([[-0.8242]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.5262]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2400.0
Loss: 0.05202965810894966
Action 0 - predicted reward: tensor([[-0.0256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1644]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6410.0
Loss: 0.00016032160783652216
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4296]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2011]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5430.0
Loss: 57192.74609375
KL Divergence: 28.65033531188965
Action 0 - predicted reward: tensor([[0.5326]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5277]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5305.0
Loss: 57606.19140625
KL Divergence: 28.560436248779297
2499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0628]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.6179]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3645.0
Loss: 0.027921166270971298
Action 0 - predicted reward: tensor([[-0.2936]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8074]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4655.0
Loss: 0.04689257964491844
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3014]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2056]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3805.0
Loss: 0.0365174263715744
Action 0 - predicted reward: tensor([[0.1290]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0873]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4455.0
Loss: 0.040205176919698715
Greedy
Action 0 - predicted reward: tensor([[-7.4655]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6527]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2440.0
Loss: 0.37274837493896484
Action 0 - predicted reward: tensor([[-0.0435]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1796]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6700.0
Loss: 0.00028770443168468773
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4396]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4638]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5515.0
Loss: 59770.64453125
KL Divergence: 27.490327835083008
Action 0 - predicted reward: tensor([[0.4549]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3780]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5415.0
Loss: 58446.15234375
KL Divergence: 27.420167922973633
2599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0822]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1725]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3795.0
Loss: 0.03363455459475517
Action 0 - predicted reward: tensor([[-0.1148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0301]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4725.0
Loss: 0.043500714004039764
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2381]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1022]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3845.0
Loss: 0.032520536333322525
Action 0 - predicted reward: tensor([[-0.1064]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6094]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4505.0
Loss: 0.03181147202849388
Greedy
Action 0 - predicted reward: tensor([[-0.1538]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1819]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2545.0
Loss: 0.044552549719810486
Action 0 - predicted reward: tensor([[0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1569]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7020.0
Loss: 0.007900385186076164
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.5365]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5248]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5575.0
Loss: 59442.9609375
KL Divergence: 26.43052864074707
Action 0 - predicted reward: tensor([[0.5797]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5907]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5465.0
Loss: 56470.8984375
KL Divergence: 26.372148513793945
2699.
Epsilon Greedy 5%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3845.0
Loss: 0.024335332214832306
Action 0 - predicted reward: tensor([[-0.0267]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.9732]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4900.0
Loss: 0.05370619148015976
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2305]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.8037]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3895.0
Loss: 0.030928421765565872
Action 0 - predicted reward: tensor([[-0.0377]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0231]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4655.0
Loss: 0.039585184305906296
Greedy
Action 0 - predicted reward: tensor([[-3.8301]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1525]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2585.0
Loss: 0.044047221541404724
Action 0 - predicted reward: tensor([[0.0146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1755]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7270.0
Loss: 0.007550149690359831
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6423]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3365]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5650.0
Loss: 60533.21484375
KL Divergence: 25.466394424438477
Action 0 - predicted reward: tensor([[0.5729]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5850]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5470.0
Loss: 55631.22265625
KL Divergence: 25.437387466430664
2799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2974]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9954]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4065.0
Loss: 0.03036133572459221
Action 0 - predicted reward: tensor([[-0.1575]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.4336]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5190.0
Loss: 0.0760321095585823
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2561]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.3875]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3930.0
Loss: 0.024551933631300926
Action 0 - predicted reward: tensor([[0.1740]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4807]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4845.0
Loss: 0.05134863033890724
Greedy
Action 0 - predicted reward: tensor([[-0.2947]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.8511]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2760.0
Loss: 0.05321023240685463
Action 0 - predicted reward: tensor([[-0.0239]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.9038]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7535.0
Loss: 0.007240081671625376
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6292]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5662]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5730.0
Loss: 61444.46875
KL Divergence: 24.551734924316406
Action 0 - predicted reward: tensor([[0.6819]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5351]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5510.0
Loss: 54246.73046875
KL Divergence: 24.513967514038086
2899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0440]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.1649]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4135.0
Loss: 0.0258350670337677
Action 0 - predicted reward: tensor([[-0.1576]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.2990]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5260.0
Loss: 0.06659442186355591
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2443]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.6408]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4005.0
Loss: 0.02828831411898136
Action 0 - predicted reward: tensor([[-0.0376]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3436]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4915.0
Loss: 0.050309278070926666
Greedy
Action 0 - predicted reward: tensor([[-1.3662]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.3587]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2760.0
Loss: 0.03166923671960831
Action 0 - predicted reward: tensor([[0.0227]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5952]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7800.0
Loss: 0.006949948146939278
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7879]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8495]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5775.0
Loss: 61332.23046875
KL Divergence: 23.723480224609375
Action 0 - predicted reward: tensor([[0.6681]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4633]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5530.0
Loss: 51798.98046875
KL Divergence: 23.679203033447266
2999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0309]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.4003]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4240.0
Loss: 0.026701655238866806
Action 0 - predicted reward: tensor([[0.0795]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.4147]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5370.0
Loss: 0.058147698640823364
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2220]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0615]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4040.0
Loss: 0.027798721566796303
Action 0 - predicted reward: tensor([[0.1656]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0986]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5095.0
Loss: 0.04860498383641243
Greedy
Action 0 - predicted reward: tensor([[-0.4667]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1938]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2760.0
Loss: 0.02042369171977043
Action 0 - predicted reward: tensor([[-0.0779]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3691]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8120.0
Loss: 0.0066728778183460236
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7400]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7852]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5820.0
Loss: 61132.07421875
KL Divergence: 22.94365119934082
Action 0 - predicted reward: tensor([[0.7697]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3724]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5565.0
Loss: 52023.03515625
KL Divergence: 22.89339828491211
3099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2415]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.8167]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4275.0
Loss: 0.02707109972834587
Action 0 - predicted reward: tensor([[0.0154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1975]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5390.0
Loss: 0.057976651936769485
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0859]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.5762]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4180.0
Loss: 0.02854473702609539
Action 0 - predicted reward: tensor([[-0.0548]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7524]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5165.0
Loss: 0.0385703481733799
Greedy
Action 0 - predicted reward: tensor([[-1.6195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.2630]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2760.0
Loss: 0.015542620792984962
Action 0 - predicted reward: tensor([[0.0273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4564]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8340.0
Loss: 0.006406167056411505
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7952]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8500]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5905.0
Loss: 60725.96484375
KL Divergence: 22.197086334228516
Action 0 - predicted reward: tensor([[0.8789]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9543]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5620.0
Loss: 51489.015625
KL Divergence: 22.15447235107422
3199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2698]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7756]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4425.0
Loss: 0.029726888984441757
Action 0 - predicted reward: tensor([[-0.0074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.2580]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5535.0
Loss: 0.06433775275945663
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3418]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5462]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4220.0
Loss: 0.021326420828700066
Action 0 - predicted reward: tensor([[0.0412]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2296]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5235.0
Loss: 0.042140621691942215
Greedy
Action 0 - predicted reward: tensor([[0.3646]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.5245]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2760.0
Loss: 0.012649918906390667
Action 0 - predicted reward: tensor([[0.0226]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7364]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8565.0
Loss: 0.006141513120383024
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8932]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3693]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5990.0
Loss: 62497.875
KL Divergence: 21.499605178833008
Action 0 - predicted reward: tensor([[0.8235]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8464]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5650.0
Loss: 51115.6015625
KL Divergence: 21.462472915649414
3299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0121]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9160]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4475.0
Loss: 0.0375065878033638
Action 0 - predicted reward: tensor([[0.1456]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.1699]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5640.0
Loss: 0.05252084881067276
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0097]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.8891]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4295.0
Loss: 0.02703404426574707
Action 0 - predicted reward: tensor([[-0.1468]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.8090]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5315.0
Loss: 0.04933544620871544
Greedy
Action 0 - predicted reward: tensor([[0.2071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.0636]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2865.0
Loss: 0.029879067093133926
Action 0 - predicted reward: tensor([[-0.0627]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.9666]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8825.0
Loss: 0.00587824871763587
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8945]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9647]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5995.0
Loss: 60712.0234375
KL Divergence: 20.844446182250977
Action 0 - predicted reward: tensor([[0.9060]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9125]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5670.0
Loss: 50226.40625
KL Divergence: 20.81560516357422
3399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.4297]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4520.0
Loss: 0.035729169845581055
Action 0 - predicted reward: tensor([[0.0336]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7244]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5640.0
Loss: 0.04864034429192543
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1324]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9282]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4295.0
Loss: 0.01822816953063011
Action 0 - predicted reward: tensor([[0.1717]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2987]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5350.0
Loss: 0.03819560259580612
Greedy
Action 0 - predicted reward: tensor([[-1.1939]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.2624]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2935.0
Loss: 0.02324983850121498
Action 0 - predicted reward: tensor([[-0.0672]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.3683]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9110.0
Loss: 0.00559667032212019
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9799]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0554]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6035.0
Loss: 60927.99609375
KL Divergence: 20.232799530029297
Action 0 - predicted reward: tensor([[0.9688]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0283]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5685.0
Loss: 49412.0234375
KL Divergence: 20.194164276123047
3499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9078]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4595.0
Loss: 0.03419390320777893
Action 0 - predicted reward: tensor([[0.0039]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2704]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5690.0
Loss: 0.05362560600042343
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1340]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.4267]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4365.0
Loss: 0.022225074470043182
Action 0 - predicted reward: tensor([[0.1283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6191]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5425.0
Loss: 0.0361810028553009
Greedy
Action 0 - predicted reward: tensor([[0.1639]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.2148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2970.0
Loss: 0.015921032056212425
Action 0 - predicted reward: tensor([[0.0302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.4908]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9400.0
Loss: 0.005281093064695597
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9647]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6125]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6075.0
Loss: 60339.2734375
KL Divergence: 19.642717361450195
Action 0 - predicted reward: tensor([[1.0501]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6418]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5685.0
Loss: 48263.5390625
KL Divergence: 19.63053321838379
3599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3048]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9946]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4635.0
Loss: 0.036837607622146606
Action 0 - predicted reward: tensor([[-0.3281]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7975]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5865.0
Loss: 0.0632748156785965
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2597]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4354]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4400.0
Loss: 0.022157521918416023
Action 0 - predicted reward: tensor([[0.1425]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8373]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5500.0
Loss: 0.03550587594509125
Greedy
Action 0 - predicted reward: tensor([[-0.1904]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9045]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3010.0
Loss: 0.015713026747107506
Action 0 - predicted reward: tensor([[-0.0296]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.2728]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 9675.0
Loss: 0.004938457626849413
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0090]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7481]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6075.0
Loss: 59432.74609375
KL Divergence: 19.108787536621094
Action 0 - predicted reward: tensor([[1.0390]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9078]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5730.0
Loss: 48607.0390625
KL Divergence: 19.0751953125
3699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.5425]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4775.0
Loss: 0.03412995859980583
Action 0 - predicted reward: tensor([[-0.3283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.3686]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5915.0
Loss: 0.06254842877388
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8863]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4470.0
Loss: 0.025421982631087303
Action 0 - predicted reward: tensor([[0.1030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8642]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5605.0
Loss: 0.03578248247504234
Greedy
Action 0 - predicted reward: tensor([[-0.3256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9519]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3045.0
Loss: 0.01896311715245247
Action 0 - predicted reward: tensor([[0.0286]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.5811]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9925.0
Loss: 0.004553069360554218
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.1038]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7136]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6080.0
Loss: 58191.5546875
KL Divergence: 18.589969635009766
Action 0 - predicted reward: tensor([[1.1216]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7671]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5735.0
Loss: 47752.0
KL Divergence: 18.56555938720703
3799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.7329]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4780.0
Loss: 0.032150719314813614
Action 0 - predicted reward: tensor([[0.0508]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0853]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5950.0
Loss: 0.060974255204200745
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1419]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.0396]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4510.0
Loss: 0.02802293747663498
Action 0 - predicted reward: tensor([[0.0821]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-73.4291]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5710.0
Loss: 0.03557343780994415
Greedy
Action 0 - predicted reward: tensor([[-1.0131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.8041]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3080.0
Loss: 0.02470357157289982
Action 0 - predicted reward: tensor([[0.0350]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.5844]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10180.0
Loss: 0.004098346922546625
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9877]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0387]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6185.0
Loss: 59817.30859375
KL Divergence: 18.103219985961914
Action 0 - predicted reward: tensor([[1.1458]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1488]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5740.0
Loss: 47179.63671875
KL Divergence: 18.074298858642578
3899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1414]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7850]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4850.0
Loss: 0.03331715986132622
Action 0 - predicted reward: tensor([[0.1421]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5270]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6080.0
Loss: 0.06136107072234154
Epsilon Greedy 1%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4625.0
Loss: 0.026371603831648827
Action 0 - predicted reward: tensor([[0.0684]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.7136]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5715.0
Loss: 0.03167557716369629
Greedy
Action 0 - predicted reward: tensor([[0.2852]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.5379]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3155.0
Loss: 0.025656336918473244
Action 0 - predicted reward: tensor([[0.0207]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.3181]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 10410.0
Loss: 0.0035606548190116882
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0346]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6448]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6220.0
Loss: 59511.69921875
KL Divergence: 17.63491439819336
Action 0 - predicted reward: tensor([[1.1581]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2297]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5745.0
Loss: 46505.42578125
KL Divergence: 17.598949432373047
3999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2816]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0444]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4955.0
Loss: 0.03336697444319725
Action 0 - predicted reward: tensor([[-0.1806]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0194]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6185.0
Loss: 0.05932268127799034
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1585]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1361]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4625.0
Loss: 0.02211158722639084
Action 0 - predicted reward: tensor([[0.0206]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.2734]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5750.0
Loss: 0.03099655918776989
Greedy
Action 0 - predicted reward: tensor([[0.2687]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9938]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3190.0
Loss: 0.030908100306987762
Action 0 - predicted reward: tensor([[-0.0877]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.1826]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 10635.0
Loss: 0.00290606077760458
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.1712]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0340]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6255.0
Loss: 59362.375
KL Divergence: 17.197509765625
Action 0 - predicted reward: tensor([[1.2286]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3037]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5765.0
Loss: 45418.96484375
KL Divergence: 17.163850784301758
4099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0670]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.3964]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5140.0
Loss: 0.03654418885707855
Action 0 - predicted reward: tensor([[-0.0583]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.8889]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6255.0
Loss: 0.048946563154459
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.9033]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4660.0
Loss: 0.02109137736260891
Action 0 - predicted reward: tensor([[-0.1611]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.7261]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5855.0
Loss: 0.037607546895742416
Greedy
Action 0 - predicted reward: tensor([[0.2209]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0463]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3225.0
Loss: 0.02125595696270466
Action 0 - predicted reward: tensor([[0.0900]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.8915]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10865.0
Loss: 0.0021509970538318157
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.1647]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2202]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6365.0
Loss: 58488.4375
KL Divergence: 16.787914276123047
Action 0 - predicted reward: tensor([[1.2178]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3058]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5845.0
Loss: 46007.7109375
KL Divergence: 16.77583122253418
4199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.1398]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5185.0
Loss: 0.03347538784146309
Action 0 - predicted reward: tensor([[0.1600]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0816]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6360.0
Loss: 0.052703581750392914
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0697]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2796]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4695.0
Loss: 0.026073265820741653
Action 0 - predicted reward: tensor([[0.0003]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9465]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5860.0
Loss: 0.03798018768429756
Greedy
Action 0 - predicted reward: tensor([[-0.0726]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2371]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3260.0
Loss: 0.016338499262928963
Action 0 - predicted reward: tensor([[0.0560]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.5032]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 11140.0
Loss: 0.0015355553478002548
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.2414]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3104]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6400.0
Loss: 55820.16796875
KL Divergence: 16.793739318847656
Action 0 - predicted reward: tensor([[1.2368]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2936]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5930.0
Loss: 44760.1640625
KL Divergence: 16.774974822998047
4299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0797]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-81.6092]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5255.0
Loss: 0.04287450760602951
Action 0 - predicted reward: tensor([[-0.0052]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.0912]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6480.0
Loss: 0.054454412311315536
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0725]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.7145]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4770.0
Loss: 0.03211982920765877
Action 0 - predicted reward: tensor([[0.0222]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5795]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6035.0
Loss: 0.04049772769212723
Greedy
Action 0 - predicted reward: tensor([[0.0819]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.2344]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3295.0
Loss: 0.0157045628875494
Action 0 - predicted reward: tensor([[-0.0112]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.3842]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 11400.0
Loss: 0.0009301555692218244
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.3102]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3742]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6400.0
Loss: 56174.65234375
KL Divergence: 16.791730880737305
Action 0 - predicted reward: tensor([[1.3531]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0527]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5985.0
Loss: 45866.2578125
KL Divergence: 16.77972412109375
4399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0259]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7674]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5430.0
Loss: 0.03551684319972992
Action 0 - predicted reward: tensor([[0.0802]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1879]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6525.0
Loss: 0.0526861697435379
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0489]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8786]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4815.0
Loss: 0.02957262098789215
Action 0 - predicted reward: tensor([[-0.2714]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0118]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6035.0
Loss: 0.03865261375904083
Greedy
Action 0 - predicted reward: tensor([[-0.8310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9565]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3295.0
Loss: 0.0146127725020051
Action 0 - predicted reward: tensor([[0.0067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.4711]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 11705.0
Loss: 0.0005658385925926268
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4107]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4889]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6440.0
Loss: 56519.328125
KL Divergence: 16.798480987548828
Action 0 - predicted reward: tensor([[1.3798]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0893]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6060.0
Loss: 46872.27734375
KL Divergence: 16.78647804260254
4499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0964]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.6792]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5550.0
Loss: 0.048627231270074844
Action 0 - predicted reward: tensor([[0.0791]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1301]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6605.0
Loss: 0.04900043085217476
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1324]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0093]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4850.0
Loss: 0.029465451836586
Action 0 - predicted reward: tensor([[0.0713]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1158]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6175.0
Loss: 0.048784997314214706
Greedy
Action 0 - predicted reward: tensor([[0.3711]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2748]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3295.0
Loss: 0.01413948368281126
Action 0 - predicted reward: tensor([[-0.0398]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.7397]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 12025.0
Loss: 0.0036923205479979515
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4661]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1932]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6440.0
Loss: 54407.8046875
KL Divergence: 16.80482292175293
Action 0 - predicted reward: tensor([[1.4597]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2358]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6060.0
Loss: 47232.06640625
KL Divergence: 16.788724899291992
4599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0472]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.8839]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5560.0
Loss: 0.04331303760409355
Action 0 - predicted reward: tensor([[0.0610]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.0002]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6710.0
Loss: 0.05599227547645569
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1117]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2007]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4885.0
Loss: 0.029040351510047913
Action 0 - predicted reward: tensor([[0.1831]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9796]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6245.0
Loss: 0.04763761907815933
Greedy
Action 0 - predicted reward: tensor([[-0.1829]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.2084]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3295.0
Loss: 0.014520587399601936
Action 0 - predicted reward: tensor([[0.0249]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.0904]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 12310.0
Loss: 0.003501208033412695
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.5028]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5701]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6520.0
Loss: 52361.09375
KL Divergence: 16.798372268676758
Action 0 - predicted reward: tensor([[1.5220]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5178]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6105.0
Loss: 47453.44921875
KL Divergence: 16.803359985351562
4699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0547]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0202]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5640.0
Loss: 0.04224001616239548
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6790.0
Loss: 0.05004472658038139
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0653]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.7349]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4960.0
Loss: 0.02583645097911358
Action 0 - predicted reward: tensor([[0.0019]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0683]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6250.0
Loss: 0.044903140515089035
Greedy
Action 0 - predicted reward: tensor([[-0.1262]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.3692]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3295.0
Loss: 0.010487828403711319
Action 0 - predicted reward: tensor([[0.0380]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.7896]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12555.0
Loss: 0.0036426775623112917
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.5038]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1126]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6590.0
Loss: 53322.09375
KL Divergence: 16.797517776489258
Action 0 - predicted reward: tensor([[1.4986]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5519]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6145.0
Loss: 46111.32421875
KL Divergence: 16.798789978027344
4799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1302]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.6722]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5655.0
Loss: 0.03753010556101799
Action 0 - predicted reward: tensor([[-0.0771]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.5115]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6865.0
Loss: 0.049008388072252274
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0919]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.0992]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5030.0
Loss: 0.028031274676322937
Action 0 - predicted reward: tensor([[-0.5374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0038]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6250.0
Loss: 0.0391552671790123
Greedy
Action 0 - predicted reward: tensor([[0.0923]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.2436]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3400.0
Loss: 0.013792311772704124
Action 0 - predicted reward: tensor([[-0.1246]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.1737]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12925.0
Loss: 0.01635344699025154
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.6555]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7271]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6660.0
Loss: 55139.578125
KL Divergence: 16.79952621459961
Action 0 - predicted reward: tensor([[1.6646]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3190]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6185.0
Loss: 46349.65625
KL Divergence: 16.81035804748535
4899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0981]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.8585]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5775.0
Loss: 0.04275628551840782
Action 0 - predicted reward: tensor([[0.1477]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.4864]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7055.0
Loss: 0.05164347589015961
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0804]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.6168]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5070.0
Loss: 0.022945908829569817
Action 0 - predicted reward: tensor([[0.7255]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9843]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6430.0
Loss: 0.04042670875787735
Greedy
Action 0 - predicted reward: tensor([[-0.0079]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.5869]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3400.0
Loss: 0.009407158941030502
Action 0 - predicted reward: tensor([[-0.2587]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.3296]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 13175.0
Loss: 0.015822818502783775
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.6137]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6942]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6700.0
Loss: 56098.7109375
KL Divergence: 16.80030632019043
Action 0 - predicted reward: tensor([[1.6935]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7717]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6220.0
Loss: 45765.29296875
KL Divergence: 16.817869186401367
4999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.8158]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5885.0
Loss: 0.04580719396471977
Action 0 - predicted reward: tensor([[0.2241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.9086]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7245.0
Loss: 0.04933229088783264
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0488]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.8097]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5180.0
Loss: 0.03245953470468521
Action 0 - predicted reward: tensor([[0.2383]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7181]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6570.0
Loss: 0.03513249009847641
Greedy
Action 0 - predicted reward: tensor([[0.0713]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.0548]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3435.0
Loss: 0.008276556618511677
Action 0 - predicted reward: tensor([[-0.1138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.5118]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13500.0
Loss: 0.01989654265344143
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7548]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8157]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6705.0
Loss: 52358.953125
KL Divergence: 16.819801330566406
Action 0 - predicted reward: tensor([[1.7055]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7699]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6270.0
Loss: 46698.59375
KL Divergence: 16.811906814575195
5099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.9058]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8663]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5955.0
Loss: 0.04263889789581299
Action 0 - predicted reward: tensor([[-0.1083]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2297]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7355.0
Loss: 0.05403660610318184
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2891]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.9543]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5250.0
Loss: 0.02411726489663124
Action 0 - predicted reward: tensor([[-0.0809]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.6654]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6605.0
Loss: 0.0305922981351614
Greedy
Action 0 - predicted reward: tensor([[0.4677]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2935]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3440.0
Loss: 0.007411573547869921
Action 0 - predicted reward: tensor([[-0.0698]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.9406]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 13810.0
Loss: 0.0158147644251585
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7679]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8288]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6710.0
Loss: 50163.9609375
KL Divergence: 16.831727981567383
Action 0 - predicted reward: tensor([[1.8259]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9105]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6270.0
Loss: 43711.265625
KL Divergence: 16.818044662475586
5199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0471]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.2613]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6080.0
Loss: 0.033485278487205505
Action 0 - predicted reward: tensor([[-0.2102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5745]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7395.0
Loss: 0.043165042996406555
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1822]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.5084]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5265.0
Loss: 0.018581833690404892
Action 0 - predicted reward: tensor([[-0.1323]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0229]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6640.0
Loss: 0.03032948076725006
Greedy
Action 0 - predicted reward: tensor([[0.0361]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.9837]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3440.0
Loss: 0.007451728917658329
Action 0 - predicted reward: tensor([[0.1034]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.3706]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14055.0
Loss: 0.011744408868253231
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.8553]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9298]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6745.0
Loss: 47149.83984375
KL Divergence: 16.831941604614258
Action 0 - predicted reward: tensor([[1.8223]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8991]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6420.0
Loss: 44631.99609375
KL Divergence: 16.82396697998047
5299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0211]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-55.4687]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6120.0
Loss: 0.03811333328485489
Action 0 - predicted reward: tensor([[0.1146]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6451]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7480.0
Loss: 0.04542037472128868
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1320]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5370.0
Loss: 0.023401079699397087
Action 0 - predicted reward: tensor([[0.9133]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0105]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6715.0
Loss: 0.034648679196834564
Greedy
Action 0 - predicted reward: tensor([[-4.3299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4701]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3475.0
Loss: 0.00866795890033245
Action 0 - predicted reward: tensor([[0.2119]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.3737]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 14300.0
Loss: 0.009499656967818737
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.8847]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9525]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6745.0
Loss: 46451.8203125
KL Divergence: 16.82555389404297
Action 0 - predicted reward: tensor([[1.8403]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9074]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6495.0
Loss: 43189.7734375
KL Divergence: 16.837018966674805
5399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1962]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.6882]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6160.0
Loss: 0.035468824207782745
Action 0 - predicted reward: tensor([[-0.2628]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.6903]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7625.0
Loss: 0.03897913172841072
Epsilon Greedy 1%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5415.0
Loss: 0.01864328607916832
Action 0 - predicted reward: tensor([[-0.0886]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-60.9309]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6785.0
Loss: 0.03495483845472336
Greedy
Action 0 - predicted reward: tensor([[0.2089]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.1463]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3510.0
Loss: 0.010972841642796993
Action 0 - predicted reward: tensor([[0.1401]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.6035]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 14560.0
Loss: 0.008895110338926315
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.9611]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3689]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6745.0
Loss: 45788.2421875
KL Divergence: 16.83158302307129
Action 0 - predicted reward: tensor([[1.9231]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9983]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6500.0
Loss: 40098.640625
KL Divergence: 16.840660095214844
5499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0447]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.7669]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6270.0
Loss: 0.04335774481296539
Action 0 - predicted reward: tensor([[0.1722]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4615]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7625.0
Loss: 0.03406691551208496
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0356]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.0966]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5455.0
Loss: 0.018437083810567856
Action 0 - predicted reward: tensor([[-0.0219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.4185]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6820.0
Loss: 0.033723391592502594
Greedy
Action 0 - predicted reward: tensor([[-2.8591]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4950]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3545.0
Loss: 0.011079736985266209
Action 0 - predicted reward: tensor([[-0.1252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.7096]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14780.0
Loss: 0.007651506457477808
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.9638]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1871]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6745.0
Loss: 45904.15625
KL Divergence: 16.819297790527344
Action 0 - predicted reward: tensor([[1.9816]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8249]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6505.0
Loss: 37790.76953125
KL Divergence: 16.845861434936523
5599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2486]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1381]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6415.0
Loss: 0.0420071966946125
Action 0 - predicted reward: tensor([[-0.7370]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.5868]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7670.0
Loss: 0.03786402568221092
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0724]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9355]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5455.0
Loss: 0.017938463017344475
Action 0 - predicted reward: tensor([[-0.2216]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9957]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6855.0
Loss: 0.03794148936867714
Greedy
Action 0 - predicted reward: tensor([[0.0978]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.7838]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3545.0
Loss: 0.010429158806800842
Action 0 - predicted reward: tensor([[-0.0167]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.8227]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15045.0
Loss: 0.011289017274975777
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.0041]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2591]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6745.0
Loss: 45994.67578125
KL Divergence: 16.824037551879883
Action 0 - predicted reward: tensor([[2.0226]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9391]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6575.0
Loss: 37084.015625
KL Divergence: 16.837696075439453
5699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4937]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5273]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6525.0
Loss: 0.034733179956674576
Action 0 - predicted reward: tensor([[0.0473]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.2766]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7745.0
Loss: 0.044474635273218155
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0036]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.3901]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5490.0
Loss: 0.0218521561473608
Action 0 - predicted reward: tensor([[0.0344]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.4708]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7030.0
Loss: 0.044065166264772415
Greedy
Action 0 - predicted reward: tensor([[0.0192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.7819]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3545.0
Loss: 0.010357100516557693
Action 0 - predicted reward: tensor([[-0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.3492]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 15310.0
Loss: 0.012062138877809048
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.0433]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1165]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6745.0
Loss: 46047.1171875
KL Divergence: 16.83135223388672
Action 0 - predicted reward: tensor([[2.0891]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1572]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6575.0
Loss: 36383.38671875
KL Divergence: 16.833110809326172
5799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0680]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.3681]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6560.0
Loss: 0.03297216072678566
Action 0 - predicted reward: tensor([[-0.1402]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.6052]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7820.0
Loss: 0.04468288645148277
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.4501]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5560.0
Loss: 0.02527271956205368
Action 0 - predicted reward: tensor([[0.7527]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1668]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7100.0
Loss: 0.03887098282575607
Greedy
Action 0 - predicted reward: tensor([[-2.3958]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0637]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3545.0
Loss: 0.01038912683725357
Action 0 - predicted reward: tensor([[0.0894]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.9639]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 15535.0
Loss: 0.010569281876087189
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.1096]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1812]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6745.0
Loss: 46119.109375
KL Divergence: 16.815750122070312
Action 0 - predicted reward: tensor([[2.1268]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1994]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6645.0
Loss: 37224.515625
KL Divergence: 16.845840454101562
5899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4039]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8600]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6640.0
Loss: 0.028312908485531807
Action 0 - predicted reward: tensor([[0.0996]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8448]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7855.0
Loss: 0.03319995477795601
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2889]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1910]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5665.0
Loss: 0.02288452722132206
Action 0 - predicted reward: tensor([[0.3657]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8247]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7170.0
Loss: 0.040636081248521805
Greedy
Action 0 - predicted reward: tensor([[-0.1612]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9367]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3545.0
Loss: 0.010245894081890583
Action 0 - predicted reward: tensor([[-0.1957]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.6582]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 15760.0
Loss: 0.01281217485666275
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.0704]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3018]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6745.0
Loss: 46200.125
KL Divergence: 16.814285278320312
Action 0 - predicted reward: tensor([[2.1740]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7446]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6680.0
Loss: 38084.5703125
KL Divergence: 16.849119186401367
5999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0818]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.9157]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6805.0
Loss: 0.029786137863993645
Action 0 - predicted reward: tensor([[0.2657]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1952]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7925.0
Loss: 0.029179472476243973
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0663]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0573]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5700.0
Loss: 0.01890377700328827
Action 0 - predicted reward: tensor([[-0.1629]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0653]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7275.0
Loss: 0.04265889525413513
Greedy
Action 0 - predicted reward: tensor([[-0.5368]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.7119]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3580.0
Loss: 0.010747147724032402
Action 0 - predicted reward: tensor([[0.0090]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.0310]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 15930.0
Loss: 0.01166816707700491
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2050]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7517]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6745.0
Loss: 46255.33203125
KL Divergence: 16.811359405517578
Action 0 - predicted reward: tensor([[2.1994]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2762]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6680.0
Loss: 37324.13671875
KL Divergence: 16.850095748901367
6099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3867]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9077]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6880.0
Loss: 0.02885577082633972
Action 0 - predicted reward: tensor([[0.1989]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8419]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8000.0
Loss: 0.025375869125127792
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.0468]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5740.0
Loss: 0.018735183402895927
Action 0 - predicted reward: tensor([[0.3630]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7313]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7275.0
Loss: 0.037429314106702805
Greedy
Action 0 - predicted reward: tensor([[3.9039]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7884]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3580.0
Loss: 0.00783098116517067
Action 0 - predicted reward: tensor([[-0.0615]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.5529]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16085.0
Loss: 0.010407352820038795
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.1470]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2162]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6850.0
Loss: 48688.140625
KL Divergence: 16.811717987060547
Action 0 - predicted reward: tensor([[2.1722]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2370]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6720.0
Loss: 38178.5859375
KL Divergence: 16.831485748291016
6199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1107]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0321]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6880.0
Loss: 0.026022011414170265
Action 0 - predicted reward: tensor([[-0.0888]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6551]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8145.0
Loss: 0.02862328104674816
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6643]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5860.0
Loss: 0.02224692888557911
Action 0 - predicted reward: tensor([[-0.1165]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.6191]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7345.0
Loss: 0.03247576579451561
Greedy
Action 0 - predicted reward: tensor([[-2.6155]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9298]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3615.0
Loss: 0.008471561595797539
Action 0 - predicted reward: tensor([[-0.0156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.4456]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 16260.0
Loss: 0.014715544879436493
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2211]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8913]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6850.0
Loss: 48708.27734375
KL Divergence: 16.816617965698242
Action 0 - predicted reward: tensor([[2.2022]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2742]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6730.0
Loss: 38184.48828125
KL Divergence: 16.834854125976562
6299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1149]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7070.0
Loss: 0.03564834222197533
Action 0 - predicted reward: tensor([[0.1037]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4113]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8290.0
Loss: 0.03268372640013695
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3451]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7759]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5935.0
Loss: 0.025231828913092613
Action 0 - predicted reward: tensor([[-0.3243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5352]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7345.0
Loss: 0.026403257623314857
Greedy
Action 0 - predicted reward: tensor([[0.0312]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.7188]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3615.0
Loss: 0.006551878526806831
Action 0 - predicted reward: tensor([[0.0126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.3888]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 16395.0
Loss: 0.015777932479977608
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.1908]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2547]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6885.0
Loss: 47140.2890625
KL Divergence: 16.81983757019043
Action 0 - predicted reward: tensor([[2.2459]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6412]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6765.0
Loss: 39020.69921875
KL Divergence: 16.84475326538086
6399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.7624]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2929]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7145.0
Loss: 0.026020843535661697
Action 0 - predicted reward: tensor([[-0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6193]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8360.0
Loss: 0.033122625201940536
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0505]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8401]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5935.0
Loss: 0.020919349044561386
Action 0 - predicted reward: tensor([[0.3040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9506]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7380.0
Loss: 0.028765968978405
Greedy
Action 0 - predicted reward: tensor([[-0.0494]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.4847]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3615.0
Loss: 0.006908902432769537
Action 0 - predicted reward: tensor([[0.2135]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.2921]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 16550.0
Loss: 0.013935517519712448
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2804]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3536]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6885.0
Loss: 44732.3984375
KL Divergence: 16.815038681030273
Action 0 - predicted reward: tensor([[2.2259]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8219]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6810.0
Loss: 39021.06640625
KL Divergence: 16.817312240600586
6499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1390]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.2765]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7225.0
Loss: 0.02804717980325222
Action 0 - predicted reward: tensor([[-0.2737]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2487]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8435.0
Loss: 0.029876647517085075
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1563]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2194]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5970.0
Loss: 0.022462675347924232
Action 0 - predicted reward: tensor([[-0.1871]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7450.0
Loss: 0.027130741626024246
Greedy
Action 0 - predicted reward: tensor([[0.0583]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.0336]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3615.0
Loss: 0.0069436607882380486
Action 0 - predicted reward: tensor([[-0.0523]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.9577]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16625.0
Loss: 0.013519030064344406
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2868]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3631]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6885.0
Loss: 44753.68359375
KL Divergence: 16.801115036010742
Action 0 - predicted reward: tensor([[2.2628]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6804]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6810.0
Loss: 38210.25
KL Divergence: 16.828229904174805
6599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1649]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.7872]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7375.0
Loss: 0.03165055811405182
Action 0 - predicted reward: tensor([[-0.0299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8772]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8475.0
Loss: 0.028661569580435753
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0506]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.2651]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6040.0
Loss: 0.023819396272301674
Action 0 - predicted reward: tensor([[0.0982]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9564]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7485.0
Loss: 0.02690727263689041
Greedy
Action 0 - predicted reward: tensor([[-0.1081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.2195]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3685.0
Loss: 0.016701839864253998
Action 0 - predicted reward: tensor([[-0.0172]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.3974]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16740.0
Loss: 0.012326490134000778
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2763]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8087]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6885.0
Loss: 43122.51953125
KL Divergence: 16.807214736938477
Action 0 - predicted reward: tensor([[2.2552]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0695]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6810.0
Loss: 37424.9453125
KL Divergence: 16.82976531982422
6699.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7415.0
Loss: 0.02367287129163742
Action 0 - predicted reward: tensor([[0.0353]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3690]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 8545.0
Loss: 0.03184950724244118
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4468]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1588]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6110.0
Loss: 0.025294532999396324
Action 0 - predicted reward: tensor([[0.0043]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.3127]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7565.0
Loss: 0.03484899550676346
Greedy
Action 0 - predicted reward: tensor([[-3.5013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8521]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3720.0
Loss: 0.019606834277510643
Action 0 - predicted reward: tensor([[-0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.3762]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16785.0
Loss: 0.007339965086430311
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2496]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3112]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6885.0
Loss: 42313.6015625
KL Divergence: 16.80478858947754
Action 0 - predicted reward: tensor([[2.2760]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3400]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6850.0
Loss: 38256.55859375
KL Divergence: 16.80942153930664
6799.
Epsilon Greedy 5%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7490.0
Loss: 0.02669256553053856
Action 0 - predicted reward: tensor([[0.0884]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8862]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8595.0
Loss: 0.024884570389986038
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1312]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8428]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6180.0
Loss: 0.029493652284145355
Action 0 - predicted reward: tensor([[0.0142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.4950]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7740.0
Loss: 0.032672543078660965
Greedy
Action 0 - predicted reward: tensor([[4.6319]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6713]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3755.0
Loss: 0.017750166356563568
Action 0 - predicted reward: tensor([[-0.0253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.5268]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16850.0
Loss: 0.00716861616820097
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3144]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6895]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6885.0
Loss: 40701.91015625
KL Divergence: 16.79892921447754
Action 0 - predicted reward: tensor([[2.3068]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3807]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6895.0
Loss: 39068.0625
KL Divergence: 16.792266845703125
6899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1589]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9679]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7565.0
Loss: 0.02522030845284462
Action 0 - predicted reward: tensor([[-0.2653]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7865]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8680.0
Loss: 0.025089958682656288
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4614]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8435]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6325.0
Loss: 0.028942184522747993
Action 0 - predicted reward: tensor([[0.1353]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.6542]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7880.0
Loss: 0.033069685101509094
Greedy
Action 0 - predicted reward: tensor([[0.2674]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8148]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3760.0
Loss: 0.0160487350076437
Action 0 - predicted reward: tensor([[0.0368]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.9604]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16880.0
Loss: 0.007116238120943308
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2457]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0004]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6885.0
Loss: 39083.171875
KL Divergence: 16.788198471069336
Action 0 - predicted reward: tensor([[2.2624]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3059]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6965.0
Loss: 40670.05859375
KL Divergence: 16.807453155517578
6999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0137]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.1848]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7675.0
Loss: 0.032916292548179626
Action 0 - predicted reward: tensor([[0.1810]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0958]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8690.0
Loss: 0.02421041578054428
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0429]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0251]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6430.0
Loss: 0.04187103360891342
Action 0 - predicted reward: tensor([[0.3243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4744]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8020.0
Loss: 0.03362113609910011
Greedy
Action 0 - predicted reward: tensor([[0.1845]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.6379]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3760.0
Loss: 0.012310571037232876
Action 0 - predicted reward: tensor([[-0.0127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.0810]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16965.0
Loss: 0.008647632785141468
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3372]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4213]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6885.0
Loss: 38260.43359375
KL Divergence: 16.777912139892578
Action 0 - predicted reward: tensor([[2.2716]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3338]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7000.0
Loss: 41495.8046875
KL Divergence: 16.79278564453125
7099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.4814]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7720.0
Loss: 0.02925938181579113
Action 0 - predicted reward: tensor([[0.0467]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0464]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8695.0
Loss: 0.024058425799012184
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.5922]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6505.0
Loss: 0.03870362043380737
Action 0 - predicted reward: tensor([[-0.0981]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0517]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8160.0
Loss: 0.04360492154955864
Greedy
Action 0 - predicted reward: tensor([[0.0424]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.7046]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3830.0
Loss: 0.016594432294368744
Action 0 - predicted reward: tensor([[0.0031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2528]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17005.0
Loss: 0.006901845335960388
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2453]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2973]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6920.0
Loss: 38239.0546875
KL Divergence: 16.764488220214844
Action 0 - predicted reward: tensor([[2.2625]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3260]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7075.0
Loss: 43100.38671875
KL Divergence: 16.788654327392578
7199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0923]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1296]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7835.0
Loss: 0.037823233753442764
Action 0 - predicted reward: tensor([[-0.0254]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.8355]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8740.0
Loss: 0.023933351039886475
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0862]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0700]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6575.0
Loss: 0.04330978915095329
Action 0 - predicted reward: tensor([[0.0697]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.1856]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8230.0
Loss: 0.042436711490154266
Greedy
Action 0 - predicted reward: tensor([[-5.7005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9430]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3900.0
Loss: 0.021206935867667198
Action 0 - predicted reward: tensor([[0.0256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0955]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17050.0
Loss: 0.006929852068424225
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3284]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5911]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6920.0
Loss: 36641.66796875
KL Divergence: 16.768911361694336
Action 0 - predicted reward: tensor([[2.2182]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1506]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7115.0
Loss: 43118.1171875
KL Divergence: 16.78541374206543
7299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0504]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.7847]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7875.0
Loss: 0.03891753777861595
Action 0 - predicted reward: tensor([[0.0598]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.7330]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8820.0
Loss: 0.02740219049155712
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0576]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.8632]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6645.0
Loss: 0.03728408366441727
Action 0 - predicted reward: tensor([[-0.1573]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.0093]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8300.0
Loss: 0.040468815714120865
Greedy
Action 0 - predicted reward: tensor([[-8.4620]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0985]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3900.0
Loss: 0.018737321719527245
Action 0 - predicted reward: tensor([[0.0062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.9094]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17095.0
Loss: 0.00695922551676631
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3376]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3931]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6955.0
Loss: 35847.1171875
KL Divergence: 16.753263473510742
Action 0 - predicted reward: tensor([[2.2901]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3390]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7135.0
Loss: 43106.08203125
KL Divergence: 16.7786922454834
7399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1341]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1821]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7920.0
Loss: 0.044053174555301666
Action 0 - predicted reward: tensor([[-0.1151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6771]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8965.0
Loss: 0.03157200291752815
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0099]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8080]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6715.0
Loss: 0.034986723214387894
Action 0 - predicted reward: tensor([[-0.0192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9727]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8475.0
Loss: 0.03642444685101509
Greedy
Action 0 - predicted reward: tensor([[-12.6281]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7023]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3900.0
Loss: 0.016846664249897003
Action 0 - predicted reward: tensor([[0.0967]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4516]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17175.0
Loss: 0.008596801199018955
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3825]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4406]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6955.0
Loss: 35861.74609375
KL Divergence: 16.753490447998047
Action 0 - predicted reward: tensor([[2.2705]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7151]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7140.0
Loss: 43109.55859375
KL Divergence: 16.774873733520508
7499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1834]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.1386]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8010.0
Loss: 0.04995524883270264
Action 0 - predicted reward: tensor([[-0.0004]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8288]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9180.0
Loss: 0.033153798431158066
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0129]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5004]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6760.0
Loss: 0.032292790710926056
Action 0 - predicted reward: tensor([[-0.0001]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.5152]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8510.0
Loss: 0.0288797989487648
Greedy
Action 0 - predicted reward: tensor([[0.1011]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.4354]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3935.0
Loss: 0.01947639137506485
Action 0 - predicted reward: tensor([[-0.2182]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.0624]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17205.0
Loss: 0.0070210834965109825
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3470]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3954]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6955.0
Loss: 34246.50390625
KL Divergence: 16.733556747436523
Action 0 - predicted reward: tensor([[2.2773]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3336]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7180.0
Loss: 43900.45703125
KL Divergence: 16.774770736694336
7599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0922]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1944]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8100.0
Loss: 0.05032844841480255
Action 0 - predicted reward: tensor([[-0.0408]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.0730]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9295.0
Loss: 0.038482215255498886
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2495]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-85.4641]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6760.0
Loss: 0.031019197776913643
Action 0 - predicted reward: tensor([[-0.1031]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.1288]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8585.0
Loss: 0.02918005920946598
Greedy
Action 0 - predicted reward: tensor([[0.0992]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.5590]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4005.0
Loss: 0.01782524213194847
Action 0 - predicted reward: tensor([[-0.0540]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0026]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17270.0
Loss: 0.007618081755936146
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3150]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7731]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6955.0
Loss: 34227.05078125
KL Divergence: 16.7252254486084
Action 0 - predicted reward: tensor([[2.2645]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9625]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7180.0
Loss: 43903.75
KL Divergence: 16.75279426574707
7699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2611]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.9370]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8135.0
Loss: 0.04340062662959099
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 9410.0
Loss: 0.03854506090283394
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0269]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.0467]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6760.0
Loss: 0.031206121668219566
Action 0 - predicted reward: tensor([[0.7061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9955]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8660.0
Loss: 0.0312295351177454
Greedy
Action 0 - predicted reward: tensor([[-0.3032]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.2130]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4075.0
Loss: 0.02323637343943119
Action 0 - predicted reward: tensor([[-0.0337]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9127]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17320.0
Loss: 0.008074120618402958
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3474]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6609]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6990.0
Loss: 35053.2265625
KL Divergence: 16.71396827697754
Action 0 - predicted reward: tensor([[2.2929]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5949]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7185.0
Loss: 43101.44140625
KL Divergence: 16.771568298339844
7799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0038]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2129]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8245.0
Loss: 0.04059124365448952
Action 0 - predicted reward: tensor([[0.0800]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9569]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9490.0
Loss: 0.03539212793111801
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4676]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9780]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6830.0
Loss: 0.03839806094765663
Action 0 - predicted reward: tensor([[-0.2529]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3456]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8765.0
Loss: 0.0399971604347229
Greedy
Action 0 - predicted reward: tensor([[-0.0573]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.7086]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4075.0
Loss: 0.019240692257881165
Action 0 - predicted reward: tensor([[-0.1210]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.3786]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17385.0
Loss: 0.007717490196228027
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.2832]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3390]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6990.0
Loss: 35053.30859375
KL Divergence: 16.697309494018555
Action 0 - predicted reward: tensor([[2.2306]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4423]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7225.0
Loss: 43887.67578125
KL Divergence: 16.7625675201416
7899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0605]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.4454]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8300.0
Loss: 0.03966217860579491
Action 0 - predicted reward: tensor([[0.0587]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8193]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9560.0
Loss: 0.030648011714220047
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0440]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.6114]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6865.0
Loss: 0.031241845339536667
Action 0 - predicted reward: tensor([[-0.0668]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.4937]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8800.0
Loss: 0.03552388399839401
Greedy
Action 0 - predicted reward: tensor([[-5.2614]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9052]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4080.0
Loss: 0.0179235078394413
Action 0 - predicted reward: tensor([[0.0204]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9974]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17410.0
Loss: 0.007023530080914497
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3402]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4065]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7060.0
Loss: 34234.50390625
KL Divergence: 16.71612548828125
Action 0 - predicted reward: tensor([[2.2125]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2784]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7260.0
Loss: 44682.73046875
KL Divergence: 16.75147247314453
7999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0524]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0344]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8320.0
Loss: 0.0371054969727993
Action 0 - predicted reward: tensor([[0.0048]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.1353]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9640.0
Loss: 0.030324267223477364
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0389]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.7248]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6970.0
Loss: 0.0357915535569191
Action 0 - predicted reward: tensor([[0.2710]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0340]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8840.0
Loss: 0.034785155206918716
Greedy
Action 0 - predicted reward: tensor([[-8.3781]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8716]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4120.0
Loss: 0.01754789613187313
Action 0 - predicted reward: tensor([[-0.0522]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.7408]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17465.0
Loss: 0.01248378399759531
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3386]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4162]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7130.0
Loss: 35037.296875
KL Divergence: 16.712509155273438
Action 0 - predicted reward: tensor([[2.2613]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3096]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7335.0
Loss: 46306.08203125
KL Divergence: 16.756694793701172
8099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0356]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1042]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8430.0
Loss: 0.033662427216768265
Action 0 - predicted reward: tensor([[0.1122]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4265]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9675.0
Loss: 0.03441884368658066
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0712]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.1057]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6970.0
Loss: 0.030369717627763748
Action 0 - predicted reward: tensor([[0.0581]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.7804]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8945.0
Loss: 0.03565846011042595
Greedy
Action 0 - predicted reward: tensor([[-3.2680]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5487]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4225.0
Loss: 0.02564905397593975
Action 0 - predicted reward: tensor([[-0.0374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9838]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17575.0
Loss: 0.012983071617782116
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3479]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4132]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7130.0
Loss: 34230.0546875
KL Divergence: 16.711917877197266
Action 0 - predicted reward: tensor([[2.2216]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2920]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7370.0
Loss: 47111.41015625
KL Divergence: 16.749160766601562
8199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1703]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8580.0
Loss: 0.031023098155856133
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9720.0
Loss: 0.03668617457151413
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1108]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9049]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6970.0
Loss: 0.027713829651474953
Action 0 - predicted reward: tensor([[0.0498]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.7059]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9055.0
Loss: 0.0427238903939724
Greedy
Action 0 - predicted reward: tensor([[-8.0730]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0669]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4265.0
Loss: 0.019978653639554977
Action 0 - predicted reward: tensor([[0.0061]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.1007]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17715.0
Loss: 0.016048040241003036
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4123]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4771]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7130.0
Loss: 31815.412109375
KL Divergence: 16.705875396728516
Action 0 - predicted reward: tensor([[2.2291]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2850]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7405.0
Loss: 46300.59375
KL Divergence: 16.740585327148438
8299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.8803]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8730.0
Loss: 0.03250841796398163
Action 0 - predicted reward: tensor([[-0.0104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4533]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9800.0
Loss: 0.040930718183517456
Epsilon Greedy 1%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 7040.0
Loss: 0.030521845445036888
Action 0 - predicted reward: tensor([[-0.1113]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.2821]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9090.0
Loss: 0.04005904123187065
Greedy
Action 0 - predicted reward: tensor([[-11.6803]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6123]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4300.0
Loss: 0.019601773470640182
Action 0 - predicted reward: tensor([[0.0771]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6164]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17775.0
Loss: 0.014034733176231384
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3461]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6102]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7130.0
Loss: 31018.20703125
KL Divergence: 16.697484970092773
Action 0 - predicted reward: tensor([[2.1713]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2303]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7450.0
Loss: 45496.4765625
KL Divergence: 16.73993492126465
8399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1498]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2925]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8805.0
Loss: 0.03190072625875473
Action 0 - predicted reward: tensor([[0.0547]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0074]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9835.0
Loss: 0.03831317275762558
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.3742]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7075.0
Loss: 0.029253847897052765
Action 0 - predicted reward: tensor([[0.6437]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0526]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9095.0
Loss: 0.03674882650375366
Greedy
Action 0 - predicted reward: tensor([[0.7397]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3056]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4305.0
Loss: 0.014750778675079346
Action 0 - predicted reward: tensor([[-0.0198]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.5303]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17810.0
Loss: 0.012385494075715542
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3942]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2468]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7130.0
Loss: 31014.27734375
KL Divergence: 16.69558334350586
Action 0 - predicted reward: tensor([[2.2333]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0023]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7455.0
Loss: 44676.078125
KL Divergence: 16.7220401763916
8499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1725]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.1803]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8840.0
Loss: 0.03162229433655739
Action 0 - predicted reward: tensor([[-0.0360]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9365]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9950.0
Loss: 0.037199657410383224
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2274]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7075.0
Loss: 0.026676366105675697
Action 0 - predicted reward: tensor([[0.2223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9760]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9095.0
Loss: 0.03304072469472885
Greedy
Action 0 - predicted reward: tensor([[-0.1548]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.9834]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4340.0
Loss: 0.014638300985097885
Action 0 - predicted reward: tensor([[-0.1327]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0516]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 17870.0
Loss: 0.011933231726288795
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3724]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8749]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7165.0
Loss: 31013.26171875
KL Divergence: 16.68218994140625
Action 0 - predicted reward: tensor([[2.3253]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5153]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7490.0
Loss: 43916.94921875
KL Divergence: 16.736650466918945
8599.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8930.0
Loss: 0.024699945002794266
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 9990.0
Loss: 0.034576401114463806
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0354]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3491]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7110.0
Loss: 0.026666656136512756
Action 0 - predicted reward: tensor([[1.0311]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0973]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9165.0
Loss: 0.032820798456668854
Greedy
Action 0 - predicted reward: tensor([[0.0363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.2319]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4375.0
Loss: 0.011553123593330383
Action 0 - predicted reward: tensor([[0.0750]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.8421]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 17885.0
Loss: 0.008313250727951527
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3806]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6791]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7165.0
Loss: 31014.8984375
KL Divergence: 16.670312881469727
Action 0 - predicted reward: tensor([[2.2386]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6217]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7495.0
Loss: 43884.8515625
KL Divergence: 16.71995735168457
8699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0432]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.2124]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9045.0
Loss: 0.027634097263216972
Action 0 - predicted reward: tensor([[0.0760]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1487]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9995.0
Loss: 0.031548887491226196
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0390]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9173]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7145.0
Loss: 0.02082405984401703
Action 0 - predicted reward: tensor([[0.1815]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0818]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9200.0
Loss: 0.03316020220518112
Greedy
Action 0 - predicted reward: tensor([[0.1492]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.3544]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4410.0
Loss: 0.011479482054710388
Action 0 - predicted reward: tensor([[-0.0387]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0647]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18005.0
Loss: 0.013164718635380268
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3581]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4115]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7235.0
Loss: 31004.2421875
KL Divergence: 16.65962791442871
Action 0 - predicted reward: tensor([[2.2771]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3461]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7495.0
Loss: 43122.6875
KL Divergence: 16.724451065063477
8799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0888]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.4728]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9055.0
Loss: 0.027435259893536568
Action 0 - predicted reward: tensor([[0.1329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1331]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10100.0
Loss: 0.03031759150326252
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.8670]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7145.0
Loss: 0.016817957162857056
Action 0 - predicted reward: tensor([[-0.0145]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.1249]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9235.0
Loss: 0.029931524768471718
Greedy
Action 0 - predicted reward: tensor([[0.0436]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5808]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4410.0
Loss: 0.010636425577104092
Action 0 - predicted reward: tensor([[-0.0277]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.4903]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18050.0
Loss: 0.010122212581336498
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4095]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8657]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7235.0
Loss: 29389.26953125
KL Divergence: 16.666973114013672
Action 0 - predicted reward: tensor([[2.3148]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3914]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7600.0
Loss: 44735.046875
KL Divergence: 16.721582412719727
8899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1410]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9197]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9095.0
Loss: 0.030597416684031487
Action 0 - predicted reward: tensor([[0.0559]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0129]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10230.0
Loss: 0.03539292514324188
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2191]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2064]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7180.0
Loss: 0.009954754263162613
Action 0 - predicted reward: tensor([[-0.0788]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.6370]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9310.0
Loss: 0.02612117864191532
Greedy
Action 0 - predicted reward: tensor([[-0.2074]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.3152]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4520.0
Loss: 0.019964853301644325
Action 0 - predicted reward: tensor([[0.0795]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2207]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18065.0
Loss: 0.008222570642828941
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4267]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4879]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7235.0
Loss: 27783.34765625
KL Divergence: 16.656661987304688
Action 0 - predicted reward: tensor([[2.3438]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7034]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7600.0
Loss: 43913.2578125
KL Divergence: 16.739498138427734
8999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0134]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0911]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9140.0
Loss: 0.03538012132048607
Action 0 - predicted reward: tensor([[0.0846]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0442]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10340.0
Loss: 0.03204610198736191
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0899]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0151]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7215.0
Loss: 0.007207681890577078
Action 0 - predicted reward: tensor([[-0.0105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.4329]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9310.0
Loss: 0.02600008435547352
Greedy
Action 0 - predicted reward: tensor([[-0.0328]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.8108]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4520.0
Loss: 0.014689471572637558
Action 0 - predicted reward: tensor([[0.0397]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4079]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18150.0
Loss: 0.01770707406103611
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3953]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4594]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7305.0
Loss: 28590.283203125
KL Divergence: 16.64803695678711
Action 0 - predicted reward: tensor([[2.2813]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5264]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7600.0
Loss: 43106.859375
KL Divergence: 16.707836151123047
9099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0261]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-54.3208]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9245.0
Loss: 0.03920406848192215
Action 0 - predicted reward: tensor([[-0.0044]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0693]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10380.0
Loss: 0.030062930658459663
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0808]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0322]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7255.0
Loss: 0.010179267264902592
Action 0 - predicted reward: tensor([[0.0187]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.4709]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9310.0
Loss: 0.0259028859436512
Greedy
Action 0 - predicted reward: tensor([[0.0536]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.8309]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4560.0
Loss: 0.018528036773204803
Action 0 - predicted reward: tensor([[-0.0699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.1330]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18200.0
Loss: 0.015296509489417076
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3806]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6505]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7305.0
Loss: 28587.236328125
KL Divergence: 16.652870178222656
Action 0 - predicted reward: tensor([[2.2495]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1647]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7600.0
Loss: 42321.7578125
KL Divergence: 16.696739196777344
9199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.9801]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0074]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9285.0
Loss: 0.031953126192092896
Action 0 - predicted reward: tensor([[-0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7216]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10385.0
Loss: 0.029918666929006577
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.1007]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7360.0
Loss: 0.011841895990073681
Action 0 - predicted reward: tensor([[-0.0562]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.0548]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9380.0
Loss: 0.02806258574128151
Greedy
Action 0 - predicted reward: tensor([[-6.4230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8637]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4560.0
Loss: 0.014448647387325764
Action 0 - predicted reward: tensor([[0.0589]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.1351]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18275.0
Loss: 0.013857416808605194
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4252]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7145]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7340.0
Loss: 29393.9921875
KL Divergence: 16.64535140991211
Action 0 - predicted reward: tensor([[2.3236]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3839]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7600.0
Loss: 42310.9296875
KL Divergence: 16.692378997802734
9299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.2902]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9365.0
Loss: 0.03178979083895683
Action 0 - predicted reward: tensor([[0.0304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.2468]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10465.0
Loss: 0.028408240526914597
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0090]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.4290]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7465.0
Loss: 0.011791149154305458
Action 0 - predicted reward: tensor([[-0.0551]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.3766]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9380.0
Loss: 0.026545587927103043
Greedy
Action 0 - predicted reward: tensor([[-3.9271]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0070]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4560.0
Loss: 0.013824363239109516
Action 0 - predicted reward: tensor([[-0.0754]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.7871]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18315.0
Loss: 0.016360078006982803
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3573]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6795]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7340.0
Loss: 28591.46484375
KL Divergence: 16.634628295898438
Action 0 - predicted reward: tensor([[2.3112]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2413]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7600.0
Loss: 39071.1015625
KL Divergence: 16.68155288696289
9399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0082]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6207]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9375.0
Loss: 0.030533775687217712
Action 0 - predicted reward: tensor([[0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.7472]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10610.0
Loss: 0.039703965187072754
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0392]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.9292]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7500.0
Loss: 0.010495172813534737
Action 0 - predicted reward: tensor([[-0.2287]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3324]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9415.0
Loss: 0.026197202503681183
Greedy
Action 0 - predicted reward: tensor([[-13.8922]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0155]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4630.0
Loss: 0.01782851852476597
Action 0 - predicted reward: tensor([[-0.2022]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9390]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18335.0
Loss: 0.012495550327003002
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4244]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4763]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7375.0
Loss: 29399.041015625
KL Divergence: 16.633264541625977
Action 0 - predicted reward: tensor([[2.2939]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3396]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7605.0
Loss: 37469.578125
KL Divergence: 16.660337448120117
9499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2806]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0750]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9450.0
Loss: 0.027769772335886955
Action 0 - predicted reward: tensor([[-0.0538]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9550]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10755.0
Loss: 0.033116500824689865
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.3362]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7605.0
Loss: 0.01994539052248001
Action 0 - predicted reward: tensor([[-0.0492]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.3305]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9455.0
Loss: 0.026790175586938858
Greedy
Action 0 - predicted reward: tensor([[-0.1904]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.3511]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4630.0
Loss: 0.010864583775401115
Action 0 - predicted reward: tensor([[-0.1092]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1991]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18450.0
Loss: 0.013903981074690819
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3908]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9268]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7375.0
Loss: 29400.052734375
KL Divergence: 16.628385543823242
Action 0 - predicted reward: tensor([[2.3243]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3817]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7620.0
Loss: 37483.0390625
KL Divergence: 16.665374755859375
9599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0402]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9331]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9600.0
Loss: 0.03921080380678177
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10860.0
Loss: 0.032429832965135574
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0161]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.8225]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7640.0
Loss: 0.014337238855659962
Action 0 - predicted reward: tensor([[0.0012]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.1318]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9490.0
Loss: 0.03059268929064274
Greedy
Action 0 - predicted reward: tensor([[-2.2438]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8599]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4670.0
Loss: 0.013259615749120712
Action 0 - predicted reward: tensor([[-0.0663]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.5103]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18505.0
Loss: 0.010427147150039673
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3583]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8295]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7375.0
Loss: 29400.5078125
KL Divergence: 16.602750778198242
Action 0 - predicted reward: tensor([[2.3223]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5874]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7655.0
Loss: 38289.04296875
KL Divergence: 16.666311264038086
9699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0832]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9640.0
Loss: 0.0315554141998291
Action 0 - predicted reward: tensor([[0.0891]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.5478]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10900.0
Loss: 0.026813361793756485
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0392]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9300]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7755.0
Loss: 0.02008771151304245
Action 0 - predicted reward: tensor([[0.1804]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9750]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9500.0
Loss: 0.02931543067097664
Greedy
Action 0 - predicted reward: tensor([[-0.0392]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.4624]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4670.0
Loss: 0.010872804559767246
Action 0 - predicted reward: tensor([[0.0527]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.4639]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18575.0
Loss: 0.01343201007694006
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4245]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4859]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7375.0
Loss: 29406.03515625
KL Divergence: 16.604761123657227
Action 0 - predicted reward: tensor([[2.3167]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8749]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7690.0
Loss: 37448.14453125
KL Divergence: 16.65496826171875
9799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3429]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9720.0
Loss: 0.03391711413860321
Action 0 - predicted reward: tensor([[-0.0255]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.2856]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11115.0
Loss: 0.03558313474059105
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0407]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7825.0
Loss: 0.014713897369801998
Action 0 - predicted reward: tensor([[-0.0840]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0774]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9535.0
Loss: 0.02617625892162323
Greedy
Action 0 - predicted reward: tensor([[-7.0253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0267]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4705.0
Loss: 0.012024794705212116
Action 0 - predicted reward: tensor([[0.0279]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8469]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18650.0
Loss: 0.01588338240981102
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4165]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4831]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7375.0
Loss: 29404.45703125
KL Divergence: 16.595739364624023
Action 0 - predicted reward: tensor([[2.2965]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3393]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7690.0
Loss: 37476.96484375
KL Divergence: 16.65452003479004
9899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0587]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0919]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9755.0
Loss: 0.030604755505919456
Action 0 - predicted reward: tensor([[0.0403]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5262]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11155.0
Loss: 0.04010435938835144
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1139]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.0020]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7875.0
Loss: 0.019041117280721664
Action 0 - predicted reward: tensor([[0.0039]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.9368]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9540.0
Loss: 0.0258213821798563
Greedy
Action 0 - predicted reward: tensor([[-8.9007]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6101]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4705.0
Loss: 0.011303910054266453
Action 0 - predicted reward: tensor([[0.0030]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.6579]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 18765.0
Loss: 0.01643229089677334
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3800]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0448]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7375.0
Loss: 29403.4609375
KL Divergence: 16.587730407714844
Action 0 - predicted reward: tensor([[2.3655]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4099]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7725.0
Loss: 36655.80078125
KL Divergence: 16.674060821533203
9999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.3533]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8854]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9790.0
Loss: 0.029834464192390442
Action 0 - predicted reward: tensor([[-0.0849]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9902]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11280.0
Loss: 0.03898147866129875
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1047]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7071]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7910.0
Loss: 0.017931688576936722
Action 0 - predicted reward: tensor([[-0.1066]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8919]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9610.0
Loss: 0.028569020330905914
Greedy
Action 0 - predicted reward: tensor([[0.0363]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.3673]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4740.0
Loss: 0.010470692999660969
Action 0 - predicted reward: tensor([[-0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9625]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 18840.0
Loss: 0.013650503940880299
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3962]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7085]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7375.0
Loss: 29404.619140625
KL Divergence: 16.57737159729004
Action 0 - predicted reward: tensor([[2.3333]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4011]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7725.0
Loss: 35855.5
KL Divergence: 16.66750717163086
10099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0704]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8610]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9805.0
Loss: 0.029337048530578613
Action 0 - predicted reward: tensor([[-0.0148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.0686]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11355.0
Loss: 0.03388053551316261
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0789]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1511]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7950.0
Loss: 0.01897665485739708
Action 0 - predicted reward: tensor([[-0.0089]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.8770]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9650.0
Loss: 0.027771200984716415
Greedy
Action 0 - predicted reward: tensor([[-0.0240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6749]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4740.0
Loss: 0.010461033321917057
Action 0 - predicted reward: tensor([[-0.0452]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.7341]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19015.0
Loss: 0.01082390546798706
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.3964]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4695]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7410.0
Loss: 30205.876953125
KL Divergence: 16.5822811126709
Action 0 - predicted reward: tensor([[2.3337]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9232]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7730.0
Loss: 35856.97265625
KL Divergence: 16.667591094970703
10199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1357]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2627]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9920.0
Loss: 0.03871132433414459
Action 0 - predicted reward: tensor([[-0.0178]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7672]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11395.0
Loss: 0.030390901491045952
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.5435]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8277]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7985.0
Loss: 0.016561321914196014
Action 0 - predicted reward: tensor([[0.1304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0655]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9655.0
Loss: 0.02681989036500454
Greedy
Action 0 - predicted reward: tensor([[-0.0282]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.3895]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4845.0
Loss: 0.014574464410543442
Action 0 - predicted reward: tensor([[-0.1838]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8044]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19030.0
Loss: 0.008228452876210213
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4167]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9207]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7410.0
Loss: 27776.36328125
KL Divergence: 16.58448600769043
Action 0 - predicted reward: tensor([[2.3603]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3973]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7775.0
Loss: 35852.26171875
KL Divergence: 16.651199340820312
10299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.4840]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9955.0
Loss: 0.03617566078901291
Action 0 - predicted reward: tensor([[-0.0145]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7434]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11470.0
Loss: 0.02905597910284996
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1895]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2041]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8020.0
Loss: 0.02129148691892624
Action 0 - predicted reward: tensor([[-0.1253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9504]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9690.0
Loss: 0.02333471179008484
Greedy
Action 0 - predicted reward: tensor([[-5.6586]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9734]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4845.0
Loss: 0.010755735449492931
Action 0 - predicted reward: tensor([[-0.0918]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.8396]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19075.0
Loss: 0.013042937032878399
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4163]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0198]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7445.0
Loss: 28575.400390625
KL Divergence: 16.58194351196289
Action 0 - predicted reward: tensor([[2.3066]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3811]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7845.0
Loss: 37460.35546875
KL Divergence: 16.663822174072266
10399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0663]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9307]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9995.0
Loss: 0.03719789534807205
Action 0 - predicted reward: tensor([[0.0221]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.4972]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11545.0
Loss: 0.02255120500922203
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1323]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-49.0796]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8020.0
Loss: 0.019885387271642685
Action 0 - predicted reward: tensor([[-0.0374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6466]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9725.0
Loss: 0.026763178408145905
Greedy
Action 0 - predicted reward: tensor([[-0.0050]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.9232]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4845.0
Loss: 0.01003993023186922
Action 0 - predicted reward: tensor([[0.2431]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0302]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19080.0
Loss: 0.008152131922543049
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4245]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6846]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7445.0
Loss: 27778.69921875
KL Divergence: 16.5902099609375
Action 0 - predicted reward: tensor([[2.2927]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4608]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7885.0
Loss: 37452.64453125
KL Divergence: 16.653953552246094
10499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0565]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9690]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10105.0
Loss: 0.03313906490802765
Action 0 - predicted reward: tensor([[0.0067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2366]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11600.0
Loss: 0.02589253894984722
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3398]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9869]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8055.0
Loss: 0.01913214847445488
Action 0 - predicted reward: tensor([[-0.0100]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.8246]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9760.0
Loss: 0.026467040181159973
Greedy
Action 0 - predicted reward: tensor([[0.0373]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.6911]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4880.0
Loss: 0.01324748620390892
Action 0 - predicted reward: tensor([[0.0386]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.1830]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19080.0
Loss: 0.007726909127086401
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4132]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1922]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7445.0
Loss: 27763.880859375
KL Divergence: 16.587387084960938
Action 0 - predicted reward: tensor([[2.3025]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8839]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7885.0
Loss: 36635.234375
KL Divergence: 16.654157638549805
10599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.0902]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10250.0
Loss: 0.026665620505809784
Action 0 - predicted reward: tensor([[0.2437]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9975]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11610.0
Loss: 0.01863195188343525
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0062]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.2814]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8090.0
Loss: 0.02222062647342682
Action 0 - predicted reward: tensor([[-0.1049]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0254]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9830.0
Loss: 0.031793028116226196
Greedy
Action 0 - predicted reward: tensor([[0.0071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.8223]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4915.0
Loss: 0.01239787321537733
Action 0 - predicted reward: tensor([[0.1264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-66.9355]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19090.0
Loss: 0.007399644237011671
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4315]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4932]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7445.0
Loss: 27774.1171875
KL Divergence: 16.584409713745117
Action 0 - predicted reward: tensor([[2.2819]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1549]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7890.0
Loss: 36619.08203125
KL Divergence: 16.6474552154541
10699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0275]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9082]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10290.0
Loss: 0.019954631105065346
Action 0 - predicted reward: tensor([[-0.0304]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-62.2235]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11650.0
Loss: 0.017763594165444374
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.6102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1112]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8125.0
Loss: 0.021734220907092094
Action 0 - predicted reward: tensor([[0.0036]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9810]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9835.0
Loss: 0.027200303971767426
Greedy
Action 0 - predicted reward: tensor([[-17.7109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6431]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4985.0
Loss: 0.015576938167214394
Action 0 - predicted reward: tensor([[-0.1521]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.3144]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19125.0
Loss: 0.01210376899689436
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4168]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8148]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7445.0
Loss: 27774.060546875
KL Divergence: 16.583255767822266
Action 0 - predicted reward: tensor([[2.2915]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5190]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7895.0
Loss: 36635.3203125
KL Divergence: 16.658519744873047
10799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0683]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9015]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10340.0
Loss: 0.01705360785126686
Action 0 - predicted reward: tensor([[0.1613]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1529]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11730.0
Loss: 0.018407659605145454
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1569]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0124]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8165.0
Loss: 0.02121453732252121
Action 0 - predicted reward: tensor([[0.9830]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0124]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9905.0
Loss: 0.027914265170693398
Greedy
Action 0 - predicted reward: tensor([[0.0566]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.3535]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5020.0
Loss: 0.014622033573687077
Action 0 - predicted reward: tensor([[0.2237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0385]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19125.0
Loss: 0.010787371546030045
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4180]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9592]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7445.0
Loss: 27772.36328125
KL Divergence: 16.58744239807129
Action 0 - predicted reward: tensor([[2.3229]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3852]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7900.0
Loss: 35829.98828125
KL Divergence: 16.64019203186035
10899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1572]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-31.4252]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10380.0
Loss: 0.019029835239052773
Action 0 - predicted reward: tensor([[-0.0215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.3291]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11780.0
Loss: 0.021506959572434425
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3315]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9178]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8275.0
Loss: 0.026092322543263435
Action 0 - predicted reward: tensor([[0.3766]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9368]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9940.0
Loss: 0.03108190931379795
Greedy
Action 0 - predicted reward: tensor([[0.2174]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.1052]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5060.0
Loss: 0.012958075851202011
Action 0 - predicted reward: tensor([[0.0013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5341]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19130.0
Loss: 0.00737756909802556
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4550]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5071]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7445.0
Loss: 27767.583984375
KL Divergence: 16.58146095275879
Action 0 - predicted reward: tensor([[2.3276]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3951]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7900.0
Loss: 35034.859375
KL Divergence: 16.65809440612793
10999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.9922]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10420.0
Loss: 0.017993047833442688
Action 0 - predicted reward: tensor([[-0.0552]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.8071]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11890.0
Loss: 0.028047144412994385
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1408]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-66.9200]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8275.0
Loss: 0.02025950513780117
Action 0 - predicted reward: tensor([[-0.0314]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0563]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9975.0
Loss: 0.027479326352477074
Greedy
Action 0 - predicted reward: tensor([[-6.9148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8611]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5100.0
Loss: 0.013734830543398857
Action 0 - predicted reward: tensor([[0.1192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.5779]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19130.0
Loss: 0.007198157254606485
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4232]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7104]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7445.0
Loss: 27777.443359375
KL Divergence: 16.585105895996094
Action 0 - predicted reward: tensor([[2.3178]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4812]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7900.0
Loss: 33406.33203125
KL Divergence: 16.659934997558594
11099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0197]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8260]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10530.0
Loss: 0.016167692840099335
Action 0 - predicted reward: tensor([[0.1837]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0152]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12015.0
Loss: 0.025807194411754608
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0640]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.8599]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8380.0
Loss: 0.02530275657773018
Action 0 - predicted reward: tensor([[0.0265]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.6013]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9975.0
Loss: 0.026576906442642212
Greedy
Action 0 - predicted reward: tensor([[-0.1700]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8397]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5135.0
Loss: 0.014622335322201252
Action 0 - predicted reward: tensor([[0.0764]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.3235]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19170.0
Loss: 0.008174421265721321
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4386]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5088]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7445.0
Loss: 27780.087890625
KL Divergence: 16.57732582092285
Action 0 - predicted reward: tensor([[2.3231]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3905]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7905.0
Loss: 32595.68359375
KL Divergence: 16.656949996948242
11199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0558]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.2696]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10650.0
Loss: 0.02203284204006195
Action 0 - predicted reward: tensor([[0.0879]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2980]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12160.0
Loss: 0.031281497329473495
Epsilon Greedy 1%
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 8495.0
Loss: 0.025366390123963356
Action 0 - predicted reward: tensor([[1.6192]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9839]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9975.0
Loss: 0.026408348232507706
Greedy
Action 0 - predicted reward: tensor([[-0.2557]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.1956]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5140.0
Loss: 0.013939240016043186
Action 0 - predicted reward: tensor([[0.0366]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.5567]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19205.0
Loss: 0.007083808537572622
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4430]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8892]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7445.0
Loss: 26941.09375
KL Divergence: 16.57897186279297
Action 0 - predicted reward: tensor([[2.3515]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4137]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7910.0
Loss: 30964.3359375
KL Divergence: 16.66307830810547
11299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1148]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.1598]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10755.0
Loss: 0.02289678528904915
Action 0 - predicted reward: tensor([[0.1536]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4265]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12310.0
Loss: 0.03912213444709778
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1568]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7814]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8500.0
Loss: 0.02454412542283535
Action 0 - predicted reward: tensor([[0.0240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9216]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9975.0
Loss: 0.025923557579517365
Greedy
Action 0 - predicted reward: tensor([[-0.8883]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.0544]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5175.0
Loss: 0.013629092834889889
Action 0 - predicted reward: tensor([[-0.1029]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9933]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19240.0
Loss: 0.006996110547333956
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4375]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5072]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7445.0
Loss: 26967.818359375
KL Divergence: 16.587177276611328
Action 0 - predicted reward: tensor([[2.3375]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7868]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7915.0
Loss: 30177.080078125
KL Divergence: 16.661069869995117
11399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0647]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.0238]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10865.0
Loss: 0.02286493591964245
Action 0 - predicted reward: tensor([[-0.0117]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.1054]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12415.0
Loss: 0.03699292615056038
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.8283]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9501]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8570.0
Loss: 0.028059301897883415
Action 0 - predicted reward: tensor([[-0.3569]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0300]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10010.0
Loss: 0.026085596531629562
Greedy
Action 0 - predicted reward: tensor([[-7.7337]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2538]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5185.0
Loss: 0.013479293324053288
Action 0 - predicted reward: tensor([[0.0066]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.8734]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19275.0
Loss: 0.011130673810839653
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4336]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0019]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7445.0
Loss: 26149.578125
KL Divergence: 16.57655906677246
Action 0 - predicted reward: tensor([[2.3590]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4146]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7915.0
Loss: 30179.51953125
KL Divergence: 16.66263771057129
11499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2776]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9762]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10945.0
Loss: 0.021668381989002228
Action 0 - predicted reward: tensor([[-0.0503]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.2475]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12530.0
Loss: 0.04667946696281433
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8993]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8605.0
Loss: 0.025432361289858818
Action 0 - predicted reward: tensor([[0.0046]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.7989]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10080.0
Loss: 0.023524388670921326
Greedy
Action 0 - predicted reward: tensor([[0.7143]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8169]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5225.0
Loss: 0.012539586052298546
Action 0 - predicted reward: tensor([[-0.2076]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.2551]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19330.0
Loss: 0.010833298787474632
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4215]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4975]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7445.0
Loss: 26152.123046875
KL Divergence: 16.572877883911133
Action 0 - predicted reward: tensor([[2.3532]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4078]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7925.0
Loss: 30175.390625
KL Divergence: 16.66173553466797
11599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0466]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.7186]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11050.0
Loss: 0.024074997752904892
Action 0 - predicted reward: tensor([[0.0380]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2990]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12605.0
Loss: 0.044369883835315704
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0661]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.2004]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8610.0
Loss: 0.02391301654279232
Action 0 - predicted reward: tensor([[-0.1329]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9470]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10085.0
Loss: 0.020237036049365997
Greedy
Action 0 - predicted reward: tensor([[-0.1163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.4581]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5260.0
Loss: 0.012814514338970184
Action 0 - predicted reward: tensor([[0.2893]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9306]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19405.0
Loss: 0.016196781769394875
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4338]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4964]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7445.0
Loss: 26151.673828125
KL Divergence: 16.568206787109375
Action 0 - predicted reward: tensor([[2.3539]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4091]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7925.0
Loss: 29365.328125
KL Divergence: 16.660289764404297
11699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0232]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-67.2858]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11135.0
Loss: 0.02377130277454853
Action 0 - predicted reward: tensor([[-0.0884]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.5968]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12720.0
Loss: 0.04804399237036705
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[1.4171]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8885]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8645.0
Loss: 0.025959419086575508
Action 0 - predicted reward: tensor([[-0.0093]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.6745]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10120.0
Loss: 0.023230789229273796
Greedy
Action 0 - predicted reward: tensor([[0.2857]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.7760]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5260.0
Loss: 0.012033005245029926
Action 0 - predicted reward: tensor([[0.1646]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8971]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19440.0
Loss: 0.015619237907230854
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4472]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5108]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7445.0
Loss: 26155.009765625
KL Divergence: 16.577808380126953
Action 0 - predicted reward: tensor([[2.3351]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6299]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7930.0
Loss: 29365.166015625
KL Divergence: 16.653833389282227
11799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1738]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2576]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11275.0
Loss: 0.026638086885213852
Action 0 - predicted reward: tensor([[-0.0280]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8847]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12790.0
Loss: 0.0470595546066761
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0542]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.8383]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8680.0
Loss: 0.027357952669262886
Action 0 - predicted reward: tensor([[-0.1005]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6009]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10125.0
Loss: 0.016620058566331863
Greedy
Action 0 - predicted reward: tensor([[1.5878]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0242]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5260.0
Loss: 0.011437828652560711
Action 0 - predicted reward: tensor([[0.0440]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.9547]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19440.0
Loss: 0.01557970978319645
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4604]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5127]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7445.0
Loss: 25342.923828125
KL Divergence: 16.579591751098633
Action 0 - predicted reward: tensor([[2.3339]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3077]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7975.0
Loss: 30172.58203125
KL Divergence: 16.65202522277832
11899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.1598]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11435.0
Loss: 0.02248186059296131
Action 0 - predicted reward: tensor([[0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.7599]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12915.0
Loss: 0.0525834858417511
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0163]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.6892]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8715.0
Loss: 0.03029189258813858
Action 0 - predicted reward: tensor([[0.0600]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.8131]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10130.0
Loss: 0.016543161123991013
Greedy
Action 0 - predicted reward: tensor([[-4.9141]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8651]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5335.0
Loss: 0.014625729992985725
Action 0 - predicted reward: tensor([[0.3443]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0004]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19440.0
Loss: 0.01522133406251669
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4584]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5130]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7445.0
Loss: 25343.806640625
KL Divergence: 16.579792022705078
Action 0 - predicted reward: tensor([[2.3270]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4865]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7975.0
Loss: 29357.1875
KL Divergence: 16.657424926757812
11999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.9383]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11515.0
Loss: 0.01957985945045948
Action 0 - predicted reward: tensor([[0.0267]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.5721]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12955.0
Loss: 0.0511881448328495
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4392]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.7772]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8785.0
Loss: 0.025115979835391045
Action 0 - predicted reward: tensor([[0.1807]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9802]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10130.0
Loss: 0.01647212542593479
Greedy
Action 0 - predicted reward: tensor([[-5.9081]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9012]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5380.0
Loss: 0.012493816204369068
Action 0 - predicted reward: tensor([[0.0633]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.5587]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19475.0
Loss: 0.015620085410773754
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4446]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8759]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7445.0
Loss: 23721.62890625
KL Divergence: 16.580629348754883
Action 0 - predicted reward: tensor([[2.3584]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3951]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8015.0
Loss: 29355.10546875
KL Divergence: 16.642986297607422
12099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1600]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1110]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 11585.0
Loss: 0.01858925074338913
Action 0 - predicted reward: tensor([[-0.0779]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-21.7209]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13005.0
Loss: 0.049464091658592224
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1574]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.6005]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8790.0
Loss: 0.021825047209858894
Action 0 - predicted reward: tensor([[1.7385]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9937]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10130.0
Loss: 0.016065359115600586
Greedy
Action 0 - predicted reward: tensor([[0.0422]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.5935]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5415.0
Loss: 0.014682166278362274
Action 0 - predicted reward: tensor([[0.9761]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0355]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19520.0
Loss: 0.019768819212913513
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4598]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5242]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7450.0
Loss: 22096.916015625
KL Divergence: 16.57798194885254
Action 0 - predicted reward: tensor([[2.3392]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4015]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8020.0
Loss: 27717.775390625
KL Divergence: 16.646177291870117
12199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1991]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.2530]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11695.0
Loss: 0.024921312928199768
Action 0 - predicted reward: tensor([[0.0571]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.5132]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13150.0
Loss: 0.055092375725507736
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0739]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.9274]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8830.0
Loss: 0.02074231393635273
Action 0 - predicted reward: tensor([[-0.1240]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0104]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10170.0
Loss: 0.0171652939170599
Greedy
Action 0 - predicted reward: tensor([[0.0169]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.5624]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5485.0
Loss: 0.010933807119727135
Action 0 - predicted reward: tensor([[-0.0292]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-43.3754]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19555.0
Loss: 0.016347143799066544
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4463]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6452]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7450.0
Loss: 22098.095703125
KL Divergence: 16.58292007446289
Action 0 - predicted reward: tensor([[2.3187]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.1145]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8100.0
Loss: 28536.90234375
KL Divergence: 16.644943237304688
12299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0222]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.4239]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11805.0
Loss: 0.02419777400791645
Action 0 - predicted reward: tensor([[-0.0497]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4768]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13195.0
Loss: 0.0524628721177578
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4684]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.1135]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8865.0
Loss: 0.02300035022199154
Action 0 - predicted reward: tensor([[-0.1320]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9864]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10170.0
Loss: 0.014412406831979752
Greedy
Action 0 - predicted reward: tensor([[-6.2196]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2478]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5520.0
Loss: 0.010944070294499397
Action 0 - predicted reward: tensor([[0.5135]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9812]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19555.0
Loss: 0.015884237363934517
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4707]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5309]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7450.0
Loss: 22098.072265625
KL Divergence: 16.588083267211914
Action 0 - predicted reward: tensor([[2.3155]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4684]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8135.0
Loss: 28527.791015625
KL Divergence: 16.640649795532227
12399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1716]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5188]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12020.0
Loss: 0.03179692476987839
Action 0 - predicted reward: tensor([[0.0832]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.2335]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13240.0
Loss: 0.05304736644029617
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0319]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.4056]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8865.0
Loss: 0.020254485309123993
Action 0 - predicted reward: tensor([[0.0080]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8271]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10205.0
Loss: 0.009983422234654427
Greedy
Action 0 - predicted reward: tensor([[-0.1346]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9920]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5555.0
Loss: 0.012155559845268726
Action 0 - predicted reward: tensor([[-0.0605]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.7911]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19555.0
Loss: 0.015288558788597584
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4618]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5308]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7450.0
Loss: 22098.955078125
KL Divergence: 16.57550048828125
Action 0 - predicted reward: tensor([[2.3282]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2681]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8140.0
Loss: 27739.302734375
KL Divergence: 16.648365020751953
12499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0826]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5786]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12065.0
Loss: 0.02829919010400772
Action 0 - predicted reward: tensor([[-0.0105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.1085]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13420.0
Loss: 0.05043812096118927
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0722]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.8452]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8900.0
Loss: 0.020878314971923828
Action 0 - predicted reward: tensor([[0.0143]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.4334]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10205.0
Loss: 0.009835433214902878
Greedy
Action 0 - predicted reward: tensor([[-4.4748]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9266]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5555.0
Loss: 0.011440983973443508
Action 0 - predicted reward: tensor([[0.6051]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0184]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19565.0
Loss: 0.015376749448478222
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.5043]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5315]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7485.0
Loss: 22097.51953125
KL Divergence: 16.572444915771484
Action 0 - predicted reward: tensor([[2.3232]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9261]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8180.0
Loss: 27726.703125
KL Divergence: 16.65296173095703
12599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0230]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-35.8217]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12180.0
Loss: 0.02633759193122387
Action 0 - predicted reward: tensor([[-0.1384]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.1277]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13525.0
Loss: 0.05236470699310303
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3689]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1504]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8935.0
Loss: 0.02012270875275135
Action 0 - predicted reward: tensor([[0.7271]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9753]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10210.0
Loss: 0.009766027331352234
Greedy
Action 0 - predicted reward: tensor([[0.2587]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.7478]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5555.0
Loss: 0.010970896109938622
Action 0 - predicted reward: tensor([[-0.0059]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.2346]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19605.0
Loss: 0.015045993030071259
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4559]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7726]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7485.0
Loss: 22081.6484375
KL Divergence: 16.588056564331055
Action 0 - predicted reward: tensor([[2.3242]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0967]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8180.0
Loss: 27734.83984375
KL Divergence: 16.65572166442871
12699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.4310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.8122]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12295.0
Loss: 0.030023999512195587
Action 0 - predicted reward: tensor([[0.0318]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.2549]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13565.0
Loss: 0.05052170902490616
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4475]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8955]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8935.0
Loss: 0.020096303895115852
Action 0 - predicted reward: tensor([[-0.0350]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9794]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 10245.0
Loss: 0.009767514653503895
Greedy
Action 0 - predicted reward: tensor([[-5.3535]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9490]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5555.0
Loss: 0.01073913648724556
Action 0 - predicted reward: tensor([[0.2058]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9625]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19610.0
Loss: 0.011552958749234676
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4657]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.5363]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7485.0
Loss: 22096.302734375
KL Divergence: 16.582372665405273
Action 0 - predicted reward: tensor([[2.3131]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9490]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8185.0
Loss: 27727.998046875
KL Divergence: 16.653568267822266
12799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0507]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0158]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 12300.0
Loss: 0.02788175828754902
Action 0 - predicted reward: tensor([[0.0234]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9564]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 13565.0
Loss: 0.04872448742389679
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0268]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-9.4965]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8935.0
Loss: 0.017300451174378395
Action 0 - predicted reward: tensor([[0.0252]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.7280]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10245.0
Loss: 0.009710406884551048
Greedy
Action 0 - predicted reward: tensor([[-1.7157]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9881]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5555.0
Loss: 0.010744473896920681
Action 0 - predicted reward: tensor([[0.1116]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.7641]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 19610.0
Loss: 0.011329635046422482
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4777]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7519]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7485.0
Loss: 20461.490234375
KL Divergence: 16.580257415771484
Action 0 - predicted reward: tensor([[2.3063]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3826]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8185.0
Loss: 27722.556640625
KL Divergence: 16.662342071533203
12899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0523]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8776]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12415.0
Loss: 0.023247841745615005
Action 0 - predicted reward: tensor([[0.0273]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.4538]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13635.0
Loss: 0.0527101494371891
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4465]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0176]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8970.0
Loss: 0.016907237470149994
Action 0 - predicted reward: tensor([[-0.0110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-64.1713]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 10245.0
Loss: 0.009634463116526604
Greedy
Action 0 - predicted reward: tensor([[-0.1017]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.7986]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5555.0
Loss: 0.01061999425292015
Action 0 - predicted reward: tensor([[0.2176]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9672]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 19645.0
Loss: 0.01187663059681654
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.4814]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6621]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7485.0
Loss: 20475.908203125
KL Divergence: 16.577655792236328
Action 0 - predicted reward: tensor([[2.3400]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.4160]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8185.0
