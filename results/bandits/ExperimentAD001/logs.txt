Use GPU: False
1.0.1.post2
99.
Action 0 - predicted reward: tensor([[-7.6523]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.0039]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 335.0
1. Loss: 0.3776724934577942
Action 0 - predicted reward: tensor([[-0.6614]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.5817]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 300.0
1. Loss: 0.1999388486146927
Action 0 - predicted reward: tensor([[-0.0056]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0052]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 240.0
1. Loss: 0.0039165327325463295
Action 0 - predicted reward: tensor([[-0.6013]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6990]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 400.0
1. Loss: 179149.234375
199.
Action 0 - predicted reward: tensor([[-1.0846]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.9218]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 725.0
3. Loss: 0.41478779911994934
Action 0 - predicted reward: tensor([[0.0553]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.6538]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 590.0
3. Loss: 0.028615297749638557
Action 0 - predicted reward: tensor([[0.3832]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.4536]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 590.0
3. Loss: 0.01062121894210577
Action 0 - predicted reward: tensor([[-2.5643]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-2.9457]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 920.0
3. Loss: 216186.125
299.
Action 0 - predicted reward: tensor([[-0.0436]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7842]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 765.0
4. Loss: 0.05562557280063629
Action 0 - predicted reward: tensor([[-1.1641]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.1038]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 905.0
4. Loss: 0.011282265186309814
Action 0 - predicted reward: tensor([[-0.8969]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.4902]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 930.0
4. Loss: 0.003129329765215516
Action 0 - predicted reward: tensor([[-0.6129]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.9578]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1320.0
4. Loss: 182607.796875
399.
Action 0 - predicted reward: tensor([[1.3250]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.2929]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 835.0
6. Loss: 0.010064929723739624
Action 0 - predicted reward: tensor([[1.0479]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2071]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1185.0
6. Loss: 0.0015031874645501375
Action 0 - predicted reward: tensor([[0.4472]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.9381]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1150.0
6. Loss: 0.04624170437455177
Action 0 - predicted reward: tensor([[0.1275]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1507]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1775.0
6. Loss: 176772.84375
499.
Action 0 - predicted reward: tensor([[4.0719]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5907]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 940.0
7. Loss: 0.01900811493396759
Action 0 - predicted reward: tensor([[-0.6845]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.8019]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1445.0
7. Loss: 0.002607539761811495
Action 0 - predicted reward: tensor([[1.9451]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.9542]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1170.0
7. Loss: 0.01228572241961956
Action 0 - predicted reward: tensor([[-0.7018]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1557]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2195.0
7. Loss: 183122.609375
599.
Action 0 - predicted reward: tensor([[-0.9787]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6363]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1085.0
9. Loss: 0.025963345542550087
Action 0 - predicted reward: tensor([[-0.5412]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.3971]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1760.0
9. Loss: 0.061899445950984955
Action 0 - predicted reward: tensor([[-0.1127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-8.8017]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1215.0
9. Loss: 0.0025608264841139317
Action 0 - predicted reward: tensor([[-1.6668]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.3918]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2440.0
9. Loss: 166890.171875
699.
Action 0 - predicted reward: tensor([[3.8616]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[7.0010]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1205.0
10. Loss: 0.006764532532542944
Action 0 - predicted reward: tensor([[0.2320]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.3977]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2005.0
10. Loss: 0.010619979351758957
Action 0 - predicted reward: tensor([[-1.2561]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0833]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1325.0
10. Loss: 0.016574745997786522
Action 0 - predicted reward: tensor([[-1.4946]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.7000]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2745.0
10. Loss: 157430.921875
799.
Action 0 - predicted reward: tensor([[1.5833]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.7865]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1285.0
12. Loss: 0.019924834370613098
Action 0 - predicted reward: tensor([[-0.8658]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.6866]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2245.0
12. Loss: 0.012498737312853336
Action 0 - predicted reward: tensor([[0.0753]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.6739]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1425.0
12. Loss: 0.037414222955703735
Action 0 - predicted reward: tensor([[-0.9831]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0640]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3070.0
12. Loss: 153350.828125
899.
Action 0 - predicted reward: tensor([[1.6233]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.4710]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1485.0
14. Loss: 0.019607774913311005
Action 0 - predicted reward: tensor([[-0.2400]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.9703]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2450.0
14. Loss: 0.0026849762070924044
Action 0 - predicted reward: tensor([[-1.5197]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7401]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1545.0
14. Loss: 0.0015326929278671741
Action 0 - predicted reward: tensor([[-1.0425]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1299]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3505.0
14. Loss: 158792.046875
999.
Action 0 - predicted reward: tensor([[-1.3983]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5686]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1535.0
15. Loss: 0.016706576570868492
Action 0 - predicted reward: tensor([[-1.6041]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.9319]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2600.0
15. Loss: 0.0004884684458374977
Action 0 - predicted reward: tensor([[-0.4455]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4270]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1585.0
15. Loss: 0.0050064679235219955
Action 0 - predicted reward: tensor([[-0.4029]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5019]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3890.0
15. Loss: 157897.5625
1099.
Action 0 - predicted reward: tensor([[0.6010]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8031]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1620.0
17. Loss: 0.01611192524433136
Action 0 - predicted reward: tensor([[0.3370]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.0749]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2795.0
17. Loss: 0.0029826865065842867
Action 0 - predicted reward: tensor([[0.2989]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.9344]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1665.0
17. Loss: 0.0015656009782105684
Action 0 - predicted reward: tensor([[-1.0249]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1759]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4290.0
17. Loss: 159549.4375
1199.
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1665.0
18. Loss: 0.023093195632100105
Action 0 - predicted reward: tensor([[1.5617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.8625]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2925.0
18. Loss: 0.012404827401041985
Action 0 - predicted reward: tensor([[-0.2897]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.0703]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1790.0
18. Loss: 0.00023748290550429374
Action 0 - predicted reward: tensor([[-0.9421]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8998]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4620.0
18. Loss: 157728.484375
1299.
Action 0 - predicted reward: tensor([[-0.0994]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1684]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1850.0
20. Loss: 0.024864766746759415
Action 0 - predicted reward: tensor([[-0.2485]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.1172]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3070.0
20. Loss: 0.015243787318468094
Action 0 - predicted reward: tensor([[-0.1118]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8968]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1805.0
20. Loss: 0.0007967468118295074
Action 0 - predicted reward: tensor([[-0.7936]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8217]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4900.0
20. Loss: 153810.890625
1399.
Action 0 - predicted reward: tensor([[-0.2904]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9485]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1890.0
21. Loss: 0.021209385246038437
Action 0 - predicted reward: tensor([[0.2314]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.5025]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3110.0
21. Loss: 0.010820508003234863
Action 0 - predicted reward: tensor([[-0.3228]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.6160]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1885.0
21. Loss: 0.005488865543156862
Action 0 - predicted reward: tensor([[-1.0064]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0370]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5185.0
21. Loss: 151784.15625
1499.
Action 0 - predicted reward: tensor([[0.2241]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9332]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1930.0
23. Loss: 0.021208371967077255
Action 0 - predicted reward: tensor([[-0.1896]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.2022]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3195.0
23. Loss: 0.009203111752867699
Action 0 - predicted reward: tensor([[-0.0981]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6032]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1925.0
23. Loss: 0.0041411928832530975
Action 0 - predicted reward: tensor([[-0.7560]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.9738]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5460.0
23. Loss: 145744.703125
1599.
Action 0 - predicted reward: tensor([[0.3055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8758]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1940.0
24. Loss: 0.01722703129053116
Action 0 - predicted reward: tensor([[-0.2102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9533]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3245.0
24. Loss: 0.02064819633960724
Action 0 - predicted reward: tensor([[-0.0632]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.8019]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1970.0
24. Loss: 0.0006318952655419707
Action 0 - predicted reward: tensor([[-0.6061]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6059]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5685.0
24. Loss: 143239.0625
1699.
Action 0 - predicted reward: tensor([[0.3916]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2073]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2055.0
26. Loss: 0.01631316915154457
Action 0 - predicted reward: tensor([[-0.2261]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.9897]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3345.0
26. Loss: 0.008825583383440971
Action 0 - predicted reward: tensor([[0.0084]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-22.6662]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2110.0
26. Loss: 0.000230844336329028
Action 0 - predicted reward: tensor([[-0.5467]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5409]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5850.0
26. Loss: 137345.359375
1799.
Action 0 - predicted reward: tensor([[-0.2387]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.8596]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2175.0
28. Loss: 0.037127092480659485
Action 0 - predicted reward: tensor([[-0.0117]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7000]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3435.0
28. Loss: 0.008098917081952095
Action 0 - predicted reward: tensor([[-0.0901]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4083]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2225.0
28. Loss: 0.014984305948019028
Action 0 - predicted reward: tensor([[-0.4480]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4481]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6070.0
28. Loss: 135113.453125
1899.
Action 0 - predicted reward: tensor([[0.1219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.5777]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2225.0
29. Loss: 0.021578796207904816
Action 0 - predicted reward: tensor([[0.0276]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4719]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3450.0
29. Loss: 0.007174555212259293
Action 0 - predicted reward: tensor([[-0.0142]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9705]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2270.0
29. Loss: 2.3084989152266644e-05
Action 0 - predicted reward: tensor([[-0.3710]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3711]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6215.0
29. Loss: 130492.03125
1999.
Action 0 - predicted reward: tensor([[-0.3144]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0824]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2400.0
31. Loss: 0.026474030688405037
Action 0 - predicted reward: tensor([[0.1016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.6785]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3465.0
31. Loss: 0.008010043762624264
Action 0 - predicted reward: tensor([[0.1259]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.5757]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2345.0
31. Loss: 0.006846532225608826
Action 0 - predicted reward: tensor([[-0.5826]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5908]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6370.0
31. Loss: 127499.140625
