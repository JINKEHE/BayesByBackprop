Use GPU: False
1.0.1.post2
99.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4491]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4823]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 250.0
Loss: 0.19391201436519623
Action 0 - predicted reward: tensor([[-0.4470]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4712]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 330.0
Loss: 0.9741479754447937
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.5072]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5438]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 315.0
Loss: 0.601035475730896
Action 0 - predicted reward: tensor([[-0.3438]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3587]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 480.0
Loss: 2.667081832885742
Greedy
Action 0 - predicted reward: tensor([[-0.6708]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7292]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 290.0
Loss: 0.3236464560031891
Action 0 - predicted reward: tensor([[-2.4245]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.7962]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 415.0
Loss: 1.46608304977417
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.6143]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.6187]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 385.0
Loss: 350496.75
KL Divergence: 711.6668701171875
Action 0 - predicted reward: tensor([[-1.8459]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.8589]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 450.0
Loss: 385670.15625
KL Divergence: 714.0657958984375
199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4532]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.5261]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 535.0
Loss: 0.1940290629863739
Action 0 - predicted reward: tensor([[0.1980]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1630]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 600.0
Loss: 0.3870132565498352
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1428]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0677]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 560.0
Loss: 0.7589290738105774
Action 0 - predicted reward: tensor([[-0.8112]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.4287]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 740.0
Loss: 1.4133660793304443
Greedy
Action 0 - predicted reward: tensor([[-0.2981]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3964]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 535.0
Loss: 0.09105327725410461
Action 0 - predicted reward: tensor([[0.4951]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.7611]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 675.0
Loss: 0.2589212954044342
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.1687]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.1691]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 655.0
Loss: 353055.8125
KL Divergence: 353.43975830078125
Action 0 - predicted reward: tensor([[-0.9992]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0019]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 745.0
Loss: 363540.46875
KL Divergence: 353.3501281738281
299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2444]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4529]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 825.0
Loss: 0.21695521473884583
Action 0 - predicted reward: tensor([[0.4260]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1020]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 930.0
Loss: 0.26862743496894836
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3613]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1147]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 785.0
Loss: 0.14048974215984344
Action 0 - predicted reward: tensor([[1.1469]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.1387]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1060.0
Loss: 0.23057743906974792
Greedy
Action 0 - predicted reward: tensor([[0.0523]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0265]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 780.0
Loss: 0.05893075466156006
Action 0 - predicted reward: tensor([[0.5976]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4728]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 925.0
Loss: 0.020035283640027046
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.0628]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.0664]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1225.0
Loss: 550688.375
KL Divergence: 235.26263427734375
Action 0 - predicted reward: tensor([[-1.6325]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.6312]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1305.0
Loss: 529861.5625
KL Divergence: 235.23692321777344
399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.8325]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.8340]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1135.0
Loss: 0.3091432452201843
Action 0 - predicted reward: tensor([[0.3980]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0486]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1195.0
Loss: 0.13777725398540497
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1729]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1685]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1035.0
Loss: 0.05034333094954491
Action 0 - predicted reward: tensor([[-1.8841]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.3678]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1160.0
Loss: 0.10785294324159622
Greedy
Action 0 - predicted reward: tensor([[0.1144]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0120]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1050.0
Loss: 0.04278590530157089
Action 0 - predicted reward: tensor([[-0.0340]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.2776]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1190.0
Loss: 0.009282485581934452
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.8167]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.8166]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1950.0
Loss: 718486.75
KL Divergence: 176.4255828857422
Action 0 - predicted reward: tensor([[-1.6853]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.6867]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1770.0
Loss: 570384.5
KL Divergence: 175.94154357910156
499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5430]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1639]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1385.0
Loss: 0.10087087750434875
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1405.0
Loss: 0.08292122185230255
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.5121]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4413]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1355.0
Loss: 0.08720526844263077
Action 0 - predicted reward: tensor([[-0.3839]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.3450]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1350.0
Loss: 0.12612156569957733
Greedy
Action 0 - predicted reward: tensor([[-0.0674]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.4179]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1285.0
Loss: 0.03176509588956833
Action 0 - predicted reward: tensor([[-0.0526]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1984]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1525.0
Loss: 0.08902300149202347
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.9192]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.9199]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2525.0
Loss: 768679.8125
KL Divergence: 141.2572784423828
Action 0 - predicted reward: tensor([[-1.5840]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.5903]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2185.0
Loss: 585671.75
KL Divergence: 140.17616271972656
599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2281]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1904]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1595.0
Loss: 0.023377470672130585
Action 0 - predicted reward: tensor([[-0.3159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.0310]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 1865.0
Loss: 0.20050597190856934
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2940]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.1683]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1635.0
Loss: 0.1501552015542984
Action 0 - predicted reward: tensor([[-0.5281]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.4863]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1610.0
Loss: 0.20546011626720428
Greedy
Action 0 - predicted reward: tensor([[0.0195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2303]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1570.0
Loss: 0.023576272651553154
Action 0 - predicted reward: tensor([[-0.1546]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.7065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1910.0
Loss: 0.2335537225008011
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.8638]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.8677]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2890.0
Loss: 731754.75
KL Divergence: 117.18799591064453
Action 0 - predicted reward: tensor([[-1.5086]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.5087]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2620.0
Loss: 601135.25
KL Divergence: 116.4910659790039
699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.8214]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1870.0
Loss: 0.007714009843766689
Action 0 - predicted reward: tensor([[0.0700]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5414]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2070.0
Loss: 0.08060318231582642
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.3025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.4480]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 1980.0
Loss: 0.18568508327007294
Action 0 - predicted reward: tensor([[0.5875]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6866]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1910.0
Loss: 0.25733938813209534
Greedy
Action 0 - predicted reward: tensor([[0.0255]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0547]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 1825.0
Loss: 0.01634390465915203
Action 0 - predicted reward: tensor([[-0.0367]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.6057]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2140.0
Loss: 0.10995730012655258
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.5647]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.5673]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3230.0
Loss: 698092.375
KL Divergence: 100.25572204589844
Action 0 - predicted reward: tensor([[-1.5250]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.5223]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3145.0
Loss: 652898.1875
KL Divergence: 99.86103057861328
799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0390]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.9921]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2120.0
Loss: 0.06934235244989395
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2370.0
Loss: 0.08847787231206894
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0433]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-5.1946]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2205.0
Loss: 0.13872021436691284
Action 0 - predicted reward: tensor([[-0.5212]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.2503]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 1990.0
Loss: 0.08481580018997192
Greedy
Action 0 - predicted reward: tensor([[0.0390]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0664]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2145.0
Loss: 0.009198561310768127
Action 0 - predicted reward: tensor([[-0.0766]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.3704]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2365.0
Loss: 0.08022797852754593
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.4732]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.4842]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3465.0
Loss: 629501.4375
KL Divergence: 87.50141906738281
Action 0 - predicted reward: tensor([[-1.2607]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.2681]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3275.0
Loss: 570230.8125
KL Divergence: 87.02410888671875
899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5566]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.1869]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2330.0
Loss: 0.08564461767673492
Action 0 - predicted reward: tensor([[-0.6478]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.2715]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2530.0
Loss: 0.04102734848856926
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2844]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2470.0
Loss: 0.1893947273492813
Action 0 - predicted reward: tensor([[-0.0170]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-38.1639]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2085.0
Loss: 0.04702632129192352
Greedy
Action 0 - predicted reward: tensor([[0.0015]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.8073]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2415.0
Loss: 0.0035207124892622232
Action 0 - predicted reward: tensor([[0.2025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.4650]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2610.0
Loss: 0.07578985393047333
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.2940]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.2968]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3710.0
Loss: 602816.3125
KL Divergence: 77.48483276367188
Action 0 - predicted reward: tensor([[-0.9005]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8965]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3460.0
Loss: 547468.9375
KL Divergence: 77.11743927001953
999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2102]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.0496]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2505.0
Loss: 0.0638267993927002
Action 0 - predicted reward: tensor([[0.5248]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.2928]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2640.0
Loss: 0.04274562746286392
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.4994]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.5061]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2510.0
Loss: 0.043699461966753006
Action 0 - predicted reward: tensor([[-0.2200]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.8238]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2240.0
Loss: 0.036052949726581573
Greedy
Action 0 - predicted reward: tensor([[-0.0374]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.5639]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2720.0
Loss: 0.020469961687922478
Action 0 - predicted reward: tensor([[-0.0026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2425]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2820.0
Loss: 0.09132921695709229
Bayes by Backprop
Action 0 - predicted reward: tensor([[-1.1127]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-1.1109]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3915.0
Loss: 543987.1875
KL Divergence: 69.59087371826172
Action 0 - predicted reward: tensor([[-0.8490]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8491]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 3670.0
Loss: 505536.4375
KL Divergence: 69.5670166015625
1099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5288]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.5250]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2610.0
Loss: 0.02374999411404133
Action 0 - predicted reward: tensor([[0.0404]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2866]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2795.0
Loss: 0.0873001292347908
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2224]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8439]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2660.0
Loss: 0.06772059202194214
Action 0 - predicted reward: tensor([[-0.3109]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.7540]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2325.0
Loss: 0.052213940769433975
Greedy
Action 0 - predicted reward: tensor([[0.0773]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0538]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3010.0
Loss: 0.01840272732079029
Action 0 - predicted reward: tensor([[-0.2105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.5954]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2950.0
Loss: 0.04981705918908119
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.8929]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.8957]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4100.0
Loss: 503764.25
KL Divergence: 63.415733337402344
Action 0 - predicted reward: tensor([[-0.5300]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5188]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3800.0
Loss: 483468.84375
KL Divergence: 63.26972198486328
1199.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2785.0
Loss: 0.07131849229335785
Action 0 - predicted reward: tensor([[0.1209]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2522]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3020.0
Loss: 0.11966215819120407
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2731]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.4829]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 2670.0
Loss: 0.020207129418849945
Action 0 - predicted reward: tensor([[0.5126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4719]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2500.0
Loss: 0.030971301719546318
Greedy
Action 0 - predicted reward: tensor([[-0.1312]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1835]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3280.0
Loss: 0.01718352548778057
Action 0 - predicted reward: tensor([[0.2006]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1782]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3035.0
Loss: 0.015909774228930473
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.5467]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.5472]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4190.0
Loss: 464439.3125
KL Divergence: 58.179290771484375
Action 0 - predicted reward: tensor([[-0.2366]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2318]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3875.0
Loss: 447532.59375
KL Divergence: 58.05638122558594
1299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.5771]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.9986]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 2905.0
Loss: 0.08543290942907333
Action 0 - predicted reward: tensor([[1.7501]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.8691]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3380.0
Loss: 0.9323334097862244
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1912]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7010]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2785.0
Loss: 0.00812026858329773
Action 0 - predicted reward: tensor([[-1.0159]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.8174]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2590.0
Loss: 0.035652026534080505
Greedy
Action 0 - predicted reward: tensor([[0.0048]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1205]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3540.0
Loss: 0.01593857631087303
Action 0 - predicted reward: tensor([[0.1206]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.0193]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3165.0
Loss: 0.017111601307988167
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.4291]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.4278]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4250.0
Loss: 441198.5625
KL Divergence: 53.87939453125
Action 0 - predicted reward: tensor([[-0.0377]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2360]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3915.0
Loss: 416192.9375
KL Divergence: 53.698280334472656
1399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2709]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.2040]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2985.0
Loss: 0.031898077577352524
Action 0 - predicted reward: tensor([[-0.6417]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.0614]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3490.0
Loss: 0.43065783381462097
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1418]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.1903]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2900.0
Loss: 0.011019405908882618
Action 0 - predicted reward: tensor([[-1.2281]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.4910]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2750.0
Loss: 0.03946542367339134
Greedy
Action 0 - predicted reward: tensor([[0.1746]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.9421]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3800.0
Loss: 0.015249378979206085
Action 0 - predicted reward: tensor([[0.1515]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.1844]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3255.0
Loss: 0.0270578321069479
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.1644]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.3733]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4280.0
Loss: 415966.53125
KL Divergence: 50.0693473815918
Action 0 - predicted reward: tensor([[0.1463]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1592]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3965.0
Loss: 401030.53125
KL Divergence: 49.930809020996094
1499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4167]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.2143]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3005.0
Loss: 0.004850451368838549
Action 0 - predicted reward: tensor([[-0.1793]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5376]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3535.0
Loss: 0.14779694378376007
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3297]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2983]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2985.0
Loss: 0.0009731752215884626
Action 0 - predicted reward: tensor([[0.0150]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.6907]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 2770.0
Loss: 0.02679828554391861
Greedy
Action 0 - predicted reward: tensor([[-0.1294]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1974]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4100.0
Loss: 0.04070796072483063
Action 0 - predicted reward: tensor([[-0.2392]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.6494]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3370.0
Loss: 0.024013232439756393
Bayes by Backprop
Action 0 - predicted reward: tensor([[-0.3025]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.2972]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4425.0
Loss: 407613.65625
KL Divergence: 46.86620330810547
Action 0 - predicted reward: tensor([[0.2960]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0787]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3975.0
Loss: 373363.125
KL Divergence: 46.60900115966797
1599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2065]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.4694]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3135.0
Loss: 0.0027037784457206726
Action 0 - predicted reward: tensor([[0.3281]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4096]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3585.0
Loss: 0.055222783237695694
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1511]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.9172]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3060.0
Loss: 0.01232810690999031
Action 0 - predicted reward: tensor([[-0.3898]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5136]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2845.0
Loss: 0.04133347421884537
Greedy
Action 0 - predicted reward: tensor([[0.0237]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0289]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4390.0
Loss: 0.03865971788764
Action 0 - predicted reward: tensor([[-0.0126]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0619]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3490.0
Loss: 0.03352311998605728
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.2193]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2301]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4470.0
Loss: 395332.4375
KL Divergence: 43.952701568603516
Action 0 - predicted reward: tensor([[0.4720]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2214]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4020.0
Loss: 365276.5625
KL Divergence: 43.795738220214844
1699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1444]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.0199]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3240.0
Loss: 0.01776314340531826
Action 0 - predicted reward: tensor([[-0.0440]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8674]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3975.0
Loss: 0.0952073484659195
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1336]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.9226]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3205.0
Loss: 0.02100760117173195
Action 0 - predicted reward: tensor([[0.2343]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8238]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 2965.0
Loss: 0.0301530584692955
Greedy
Action 0 - predicted reward: tensor([[0.1919]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0007]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4610.0
Loss: 0.04609614610671997
Action 0 - predicted reward: tensor([[-0.0538]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.4856]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3560.0
Loss: 0.03281104192137718
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.0477]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0660]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4480.0
Loss: 372336.53125
KL Divergence: 41.48176193237305
Action 0 - predicted reward: tensor([[0.4344]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.0558]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4055.0
Loss: 353398.40625
KL Divergence: 41.30100631713867
1799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1194]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8289]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3390.0
Loss: 0.02129986323416233
Action 0 - predicted reward: tensor([[-0.0622]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2025]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4115.0
Loss: 0.09204152226448059
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0499]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.2965]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3350.0
Loss: 0.032531239092350006
Action 0 - predicted reward: tensor([[-0.1096]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.4739]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3005.0
Loss: 0.03144114837050438
Greedy
Action 0 - predicted reward: tensor([[-0.2644]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.2826]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4860.0
Loss: 0.05782901868224144
Action 0 - predicted reward: tensor([[-0.0681]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2374]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3670.0
Loss: 0.03159191831946373
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.3501]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3705]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4560.0
Loss: 361460.375
KL Divergence: 39.34348678588867
Action 0 - predicted reward: tensor([[0.5032]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.1852]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4060.0
Loss: 338065.34375
KL Divergence: 39.07951354980469
1899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0359]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.9951]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3560.0
Loss: 0.0077866679057478905
Action 0 - predicted reward: tensor([[0.0067]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.2573]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4340.0
Loss: 0.11292766034603119
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0876]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0381]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3430.0
Loss: 0.029522011056542397
Action 0 - predicted reward: tensor([[0.1967]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9900]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3085.0
Loss: 0.0381692498922348
Greedy
Action 0 - predicted reward: tensor([[0.0449]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.3002]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5045.0
Loss: 0.059399981051683426
Action 0 - predicted reward: tensor([[0.0465]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.8682]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3820.0
Loss: 0.04674241691827774
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4274]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3457]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4640.0
Loss: 359126.5
KL Divergence: 37.26642990112305
Action 0 - predicted reward: tensor([[0.7984]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8495]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4060.0
Loss: 320820.125
KL Divergence: 37.065673828125
1999.
Epsilon Greedy 5%
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3655.0
Loss: 0.0012518485309556127
Action 0 - predicted reward: tensor([[1.3775]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.3775]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4795.0
Loss: 1.0307368040084839
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2490]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8045]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3440.0
Loss: 0.031327784061431885
Action 0 - predicted reward: tensor([[-0.2312]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0628]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3165.0
Loss: 0.05083003640174866
Greedy
Action 0 - predicted reward: tensor([[0.0445]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[6.1370]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5200.0
Loss: 0.04025987163186073
Action 0 - predicted reward: tensor([[0.2054]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-63.7049]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3970.0
Loss: 0.029864318668842316
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.3026]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[-0.0380]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4820.0
Loss: 364386.9375
KL Divergence: 35.52777099609375
Action 0 - predicted reward: tensor([[0.9652]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8933]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4060.0
Loss: 310614.34375
KL Divergence: 35.2741813659668
2099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0254]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-41.0989]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3725.0
Loss: 0.0010103428503498435
Action 0 - predicted reward: tensor([[0.4164]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.4164]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5005.0
Loss: 0.9720768928527832
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1174]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.4801]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3480.0
Loss: 0.020270662382245064
Action 0 - predicted reward: tensor([[0.1316]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.0498]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3175.0
Loss: 0.036654431372880936
Greedy
Action 0 - predicted reward: tensor([[-0.1105]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0338]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5350.0
Loss: 0.019421078264713287
Action 0 - predicted reward: tensor([[-0.4091]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[2.2121]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 4005.0
Loss: 0.03511068597435951
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4133]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4643]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5040.0
Loss: 374786.78125
KL Divergence: 33.87232208251953
Action 0 - predicted reward: tensor([[0.9757]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6322]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4095.0
Loss: 302247.65625
KL Divergence: 33.67009353637695
2199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1120]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.2344]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3920.0
Loss: 0.04146508872509003
Action 0 - predicted reward: tensor([[0.3873]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3873]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5235.0
Loss: 0.9556499719619751
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1256]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7356]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3570.0
Loss: 0.011056572198867798
Action 0 - predicted reward: tensor([[0.1426]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5353]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3195.0
Loss: 0.03296839818358421
Greedy
Action 0 - predicted reward: tensor([[-0.1435]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.3807]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5540.0
Loss: 0.03578735515475273
Action 0 - predicted reward: tensor([[-0.0064]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9689]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4145.0
Loss: 0.04130483791232109
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4981]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5195]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5115.0
Loss: 369747.59375
KL Divergence: 32.4432373046875
Action 0 - predicted reward: tensor([[0.9276]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9600]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4200.0
Loss: 300930.0
KL Divergence: 32.155643463134766
2299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0025]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.4845]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4180.0
Loss: 0.026275789365172386
Action 0 - predicted reward: tensor([[0.3792]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3792]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 5530.0
Loss: 0.8857593536376953
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1131]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.9986]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3675.0
Loss: 0.018734648823738098
Action 0 - predicted reward: tensor([[0.0631]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.4770]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3275.0
Loss: 0.03904816135764122
Greedy
Action 0 - predicted reward: tensor([[0.2638]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.4373]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5620.0
Loss: 0.022832674905657768
Action 0 - predicted reward: tensor([[0.0026]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.9738]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4220.0
Loss: 0.036578163504600525
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.4603]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.3687]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5150.0
Loss: 360424.65625
KL Divergence: 31.09308433532715
Action 0 - predicted reward: tensor([[0.9713]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0034]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4235.0
Loss: 296066.65625
KL Divergence: 30.826229095458984
2399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0161]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0392]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4255.0
Loss: 0.014543560333549976
Action 0 - predicted reward: tensor([[0.3629]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3629]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5825.0
Loss: 0.8477545380592346
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0892]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-3.2146]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3745.0
Loss: 0.016529174521565437
Action 0 - predicted reward: tensor([[-1.4264]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.0470]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3405.0
Loss: 0.054795365780591965
Greedy
Action 0 - predicted reward: tensor([[0.2531]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.5880]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5625.0
Loss: 0.013400446623563766
Action 0 - predicted reward: tensor([[-0.4818]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.6412]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4360.0
Loss: 0.03221598640084267
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6589]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6954]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5395.0
Loss: 367516.46875
KL Divergence: 29.897111892700195
Action 0 - predicted reward: tensor([[0.8617]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6524]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4305.0
Loss: 291392.875
KL Divergence: 29.626949310302734
2499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1294]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.3390]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4365.0
Loss: 0.0340065062046051
Action 0 - predicted reward: tensor([[0.3448]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3448]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6085.0
Loss: 0.8272714614868164
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1691]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0723]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3890.0
Loss: 0.024660030379891396
Action 0 - predicted reward: tensor([[0.0622]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.5959]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3525.0
Loss: 0.04427272826433182
Greedy
Action 0 - predicted reward: tensor([[-0.0896]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.8804]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5660.0
Loss: 0.011448211036622524
Action 0 - predicted reward: tensor([[0.3035]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7075]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4430.0
Loss: 0.04439789056777954
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.5365]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5340]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5435.0
Loss: 364256.0625
KL Divergence: 28.734481811523438
Action 0 - predicted reward: tensor([[0.9831]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7141]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4340.0
Loss: 286813.0625
KL Divergence: 28.48516273498535
2599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.8155]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.3065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4480.0
Loss: 0.03315895050764084
Action 0 - predicted reward: tensor([[0.3368]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3368]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6345.0
Loss: 0.7904157042503357
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2202]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.5198]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4030.0
Loss: 0.038441020995378494
Action 0 - predicted reward: tensor([[0.0070]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.7426]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3605.0
Loss: 0.046707574278116226
Greedy
Action 0 - predicted reward: tensor([[-0.2435]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.5617]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5725.0
Loss: 0.014164839871227741
Action 0 - predicted reward: tensor([[0.1971]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.8808]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4500.0
Loss: 0.03528670221567154
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6683]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7111]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5730.0
Loss: 373049.3125
KL Divergence: 27.66517448425293
Action 0 - predicted reward: tensor([[0.9620]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0197]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4375.0
Loss: 280897.65625
KL Divergence: 27.438720703125
2699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4982]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.2456]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4520.0
Loss: 0.040385469794273376
Action 0 - predicted reward: tensor([[0.3127]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.3127]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6675.0
Loss: 0.8007539510726929
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1993]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.8864]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4145.0
Loss: 0.027145829051733017
Action 0 - predicted reward: tensor([[0.0694]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.2118]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3680.0
Loss: 0.0463872104883194
Greedy
Action 0 - predicted reward: tensor([[-0.2849]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-50.1510]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5780.0
Loss: 0.012585829012095928
Action 0 - predicted reward: tensor([[0.0663]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6276]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4505.0
Loss: 0.025085048750042915
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6861]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7207]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5805.0
Loss: 372875.09375
KL Divergence: 26.695741653442383
Action 0 - predicted reward: tensor([[1.1057]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5978]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4375.0
Loss: 272747.125
KL Divergence: 26.48696517944336
2799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1646]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0486]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4765.0
Loss: 0.04697990417480469
Action 0 - predicted reward: tensor([[0.2853]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2853]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6945.0
Loss: 0.7528935670852661
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0607]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7436]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4155.0
Loss: 0.02043377421796322
Action 0 - predicted reward: tensor([[0.0901]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0853]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 3790.0
Loss: 0.047247935086488724
Greedy
Action 0 - predicted reward: tensor([[0.1243]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.4987]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5795.0
Loss: 0.007919302210211754
Action 0 - predicted reward: tensor([[0.0745]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.7250]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4575.0
Loss: 0.03032034821808338
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7229]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7906]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5980.0
Loss: 373621.5
KL Divergence: 25.83121109008789
Action 0 - predicted reward: tensor([[1.0923]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1537]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4375.0
Loss: 264315.3125
KL Divergence: 25.58439826965332
2899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4223]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.4455]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4885.0
Loss: 0.12476090341806412
Action 0 - predicted reward: tensor([[0.2459]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2459]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7285.0
Loss: 0.7584494352340698
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0215]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2481]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4300.0
Loss: 0.042164966464042664
Action 0 - predicted reward: tensor([[0.0563]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.3478]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3915.0
Loss: 0.057786013931035995
Greedy
Action 0 - predicted reward: tensor([[0.3767]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5919]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5825.0
Loss: 0.006188482977449894
Action 0 - predicted reward: tensor([[-0.1953]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.2292]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4620.0
Loss: 0.03360345587134361
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.6231]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6648]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6295.0
Loss: 391926.5625
KL Divergence: 24.97690773010254
Action 0 - predicted reward: tensor([[1.2806]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3217]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4445.0
Loss: 261560.734375
KL Divergence: 24.733917236328125
2999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0515]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0839]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4940.0
Loss: 0.03303425759077072
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7585.0
Loss: 0.726366400718689
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1017]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9511]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4480.0
Loss: 0.05710624158382416
Action 0 - predicted reward: tensor([[-0.0567]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.7014]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 3960.0
Loss: 0.051630325615406036
Greedy
Action 0 - predicted reward: tensor([[-0.0414]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5376]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5920.0
Loss: 0.008880591951310635
Action 0 - predicted reward: tensor([[-0.0691]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-32.7865]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4655.0
Loss: 0.039128340780735016
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7204]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7587]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6330.0
Loss: 383246.34375
KL Divergence: 24.19937515258789
Action 0 - predicted reward: tensor([[1.3172]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3881]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4550.0
Loss: 264265.3125
KL Divergence: 23.93474769592285
3099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0276]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0631]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4950.0
Loss: 0.029041284695267677
Action 0 - predicted reward: tensor([[0.2205]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2205]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 7795.0
Loss: 0.7268970608711243
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0940]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6113]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4590.0
Loss: 0.05122281610965729
Action 0 - predicted reward: tensor([[-0.9351]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.6533]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4045.0
Loss: 0.0550825297832489
Greedy
Action 0 - predicted reward: tensor([[-0.0918]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3495]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5965.0
Loss: 0.017958957701921463
Action 0 - predicted reward: tensor([[-4.8971]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.3145]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4730.0
Loss: 0.04795506224036217
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8171]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8704]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6365.0
Loss: 372008.4375
KL Divergence: 23.48446273803711
Action 0 - predicted reward: tensor([[1.3232]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0842]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4585.0
Loss: 259434.328125
KL Divergence: 23.224586486816406
3199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0753]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.2137]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5040.0
Loss: 0.03601909801363945
Action 0 - predicted reward: tensor([[0.1948]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1948]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8115.0
Loss: 0.6913298964500427
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3574]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6295]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4700.0
Loss: 0.054198168218135834
Action 0 - predicted reward: tensor([[-0.0326]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0180]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4115.0
Loss: 0.05766868218779564
Greedy
Action 0 - predicted reward: tensor([[-0.6328]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.1208]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6065.0
Loss: 0.017628418281674385
Action 0 - predicted reward: tensor([[-0.4447]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.6914]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4810.0
Loss: 0.04225744307041168
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7811]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.8133]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6365.0
Loss: 366847.71875
KL Divergence: 22.791643142700195
Action 0 - predicted reward: tensor([[1.4555]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1279]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4655.0
Loss: 259975.359375
KL Divergence: 22.5052433013916
3299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0450]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.0209]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5165.0
Loss: 0.035955801606178284
Action 0 - predicted reward: tensor([[0.1985]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1985]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8385.0
Loss: 0.671329140663147
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1149]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2122]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4805.0
Loss: 0.04324023798108101
Action 0 - predicted reward: tensor([[-0.1491]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0321]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4160.0
Loss: 0.05479072779417038
Greedy
Action 0 - predicted reward: tensor([[-0.0660]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-26.7868]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6110.0
Loss: 0.014932829886674881
Action 0 - predicted reward: tensor([[0.0154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7067]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4920.0
Loss: 0.0463782399892807
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.7900]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7935]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6470.0
Loss: 361438.28125
KL Divergence: 22.165727615356445
Action 0 - predicted reward: tensor([[1.4227]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4797]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4690.0
Loss: 253671.3125
KL Divergence: 21.863903045654297
3399.
Epsilon Greedy 5%
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5305.0
Loss: 0.051063619554042816
Action 0 - predicted reward: tensor([[0.2007]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2007]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 8660.0
Loss: 0.6526129245758057
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0701]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.6501]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4945.0
Loss: 0.04648151993751526
Action 0 - predicted reward: tensor([[-0.0690]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-65.9386]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4205.0
Loss: 0.0584680438041687
Greedy
Action 0 - predicted reward: tensor([[0.1778]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7986]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6225.0
Loss: 0.01658627949655056
Action 0 - predicted reward: tensor([[-0.0568]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-14.1796]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4925.0
Loss: 0.04046699404716492
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8951]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5853]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6505.0
Loss: 356405.71875
KL Divergence: 21.566776275634766
Action 0 - predicted reward: tensor([[1.4783]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5465]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4760.0
Loss: 255046.703125
KL Divergence: 21.2486515045166
3499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2219]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.9598]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5380.0
Loss: 0.041251566261053085
Action 0 - predicted reward: tensor([[0.1891]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1891]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8915.0
Loss: 0.6388154029846191
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0785]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[1.8331]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 5090.0
Loss: 0.06234574317932129
Action 0 - predicted reward: tensor([[-1.9835]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-52.4651]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4355.0
Loss: 0.06086910888552666
Greedy
Action 0 - predicted reward: tensor([[0.2296]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.4907]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6275.0
Loss: 0.014880464412271976
Action 0 - predicted reward: tensor([[0.2168]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9882]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4970.0
Loss: 0.04481898248195648
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.1100]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.5270]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6610.0
Loss: 356442.6875
KL Divergence: 21.012271881103516
Action 0 - predicted reward: tensor([[1.1169]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1681]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4760.0
Loss: 247725.4375
KL Divergence: 20.668615341186523
3599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0154]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0621]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5590.0
Loss: 0.06268133223056793
Action 0 - predicted reward: tensor([[0.1960]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1960]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9220.0
Loss: 0.6214658617973328
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.4195]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9411]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5205.0
Loss: 0.04477381333708763
Action 0 - predicted reward: tensor([[0.1834]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.8971]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4465.0
Loss: 0.062033187597990036
Greedy
Action 0 - predicted reward: tensor([[0.0734]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.9487]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6290.0
Loss: 0.010064815171062946
Action 0 - predicted reward: tensor([[-0.0515]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9109]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5005.0
Loss: 0.04633282497525215
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0261]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0813]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6685.0
Loss: 353486.40625
KL Divergence: 20.465429306030273
Action 0 - predicted reward: tensor([[1.2615]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3101]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4795.0
Loss: 246086.828125
KL Divergence: 20.121458053588867
3699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0609]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0669]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5600.0
Loss: 0.04894934967160225
Action 0 - predicted reward: tensor([[0.1746]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1746]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 9445.0
Loss: 0.6059693098068237
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0636]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0525]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5380.0
Loss: 0.051910992711782455
Action 0 - predicted reward: tensor([[-0.0466]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.3773]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 4480.0
Loss: 0.05619475618004799
Greedy
Action 0 - predicted reward: tensor([[-0.0921]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1290]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6330.0
Loss: 0.009513499215245247
Action 0 - predicted reward: tensor([[0.3760]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0739]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5015.0
Loss: 0.03923354670405388
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8983]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.2294]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6860.0
Loss: 355673.09375
KL Divergence: 19.954561233520508
Action 0 - predicted reward: tensor([[1.5484]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6122]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4865.0
Loss: 245105.234375
KL Divergence: 19.636306762695312
3799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1975]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.5467]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5680.0
Loss: 0.04899454489350319
Action 0 - predicted reward: tensor([[0.1555]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1555]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 9705.0
Loss: 0.5937368273735046
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0869]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-37.2307]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5460.0
Loss: 0.04868990555405617
Action 0 - predicted reward: tensor([[0.1352]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.8044]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4525.0
Loss: 0.05454253777861595
Greedy
Action 0 - predicted reward: tensor([[-0.0699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9998]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6410.0
Loss: 0.014818905852735043
Action 0 - predicted reward: tensor([[0.0970]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2095]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5020.0
Loss: 0.03633318468928337
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.8847]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9404]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6930.0
Loss: 351461.34375
KL Divergence: 19.498394012451172
Action 0 - predicted reward: tensor([[1.6372]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5972]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4935.0
Loss: 243234.96875
KL Divergence: 19.135868072509766
3899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0530]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.5278]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5765.0
Loss: 0.05012653023004532
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 9980.0
Loss: 0.5790821313858032
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.2002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.9536]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5540.0
Loss: 0.03857465833425522
Action 0 - predicted reward: tensor([[-0.1632]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8386]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4605.0
Loss: 0.05375475808978081
Greedy
Action 0 - predicted reward: tensor([[-0.0599]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8894]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6490.0
Loss: 0.01171157043427229
Action 0 - predicted reward: tensor([[-0.3046]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9990]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5025.0
Loss: 0.03500884771347046
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9452]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.4096]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7035.0
Loss: 351756.78125
KL Divergence: 19.043193817138672
Action 0 - predicted reward: tensor([[1.3043]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3617]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5040.0
Loss: 246342.03125
KL Divergence: 18.671131134033203
3999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1122]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-25.3012]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5915.0
Loss: 0.060653675347566605
Action 0 - predicted reward: tensor([[0.1596]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1596]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 10245.0
Loss: 0.5793185830116272
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1160]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.3587]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5680.0
Loss: 0.046010833233594894
Action 0 - predicted reward: tensor([[-0.0299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-15.5185]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4605.0
Loss: 0.05150160565972328
Greedy
Action 0 - predicted reward: tensor([[0.0554]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0076]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6535.0
Loss: 0.012510058470070362
Action 0 - predicted reward: tensor([[0.2822]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.3152]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5030.0
Loss: 0.033374905586242676
Bayes by Backprop
Action 0 - predicted reward: tensor([[0.9548]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6453]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7105.0
Loss: 347392.46875
KL Divergence: 18.629125595092773
Action 0 - predicted reward: tensor([[1.3029]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3034]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5215.0
Loss: 251155.46875
KL Divergence: 18.227170944213867
4099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1193]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7547]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 6130.0
Loss: 0.07581211626529694
Action 0 - predicted reward: tensor([[0.1609]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1609]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 10520.0
Loss: 0.5467363595962524
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3535]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2100]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5700.0
Loss: 0.03621983155608177
Action 0 - predicted reward: tensor([[0.0023]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.8554]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4610.0
Loss: 0.04947774112224579
Greedy
Action 0 - predicted reward: tensor([[0.2274]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.9636]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6550.0
Loss: 0.010460798628628254
Action 0 - predicted reward: tensor([[0.1750]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.5974]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5075.0
Loss: 0.030666010454297066
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.3196]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.7686]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7175.0
Loss: 344032.1875
KL Divergence: 18.24303436279297
Action 0 - predicted reward: tensor([[1.4625]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3053]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5355.0
Loss: 253895.625
KL Divergence: 17.829565048217773
4199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0111]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.8699]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6145.0
Loss: 0.07389666885137558
Action 0 - predicted reward: tensor([[0.1745]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1745]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 10785.0
Loss: 0.5313497185707092
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0839]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-58.5466]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5855.0
Loss: 0.03623684495687485
Action 0 - predicted reward: tensor([[0.0112]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-20.0184]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4715.0
Loss: 0.048741526901721954
Greedy
Action 0 - predicted reward: tensor([[-0.2104]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9389]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6660.0
Loss: 0.018018465489149094
Action 0 - predicted reward: tensor([[0.1763]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-59.5248]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5115.0
Loss: 0.03002946265041828
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.2505]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2949]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7180.0
Loss: 334126.3125
KL Divergence: 18.303932189941406
Action 0 - predicted reward: tensor([[1.5108]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4900]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5355.0
Loss: 241810.71875
KL Divergence: 17.871124267578125
4299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.6299]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1348]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6220.0
Loss: 0.06316786259412766
Action 0 - predicted reward: tensor([[0.1701]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1701]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11055.0
Loss: 0.5312356352806091
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1007]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.9884]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5950.0
Loss: 0.036707986146211624
Action 0 - predicted reward: tensor([[0.0409]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9636]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4795.0
Loss: 0.047617822885513306
Greedy
Action 0 - predicted reward: tensor([[0.0002]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2543]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6695.0
Loss: 0.01639905571937561
Action 0 - predicted reward: tensor([[0.0856]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.1239]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5185.0
Loss: 0.03479126840829849
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.0163]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9195]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7325.0
Loss: 338891.21875
KL Divergence: 18.36616325378418
Action 0 - predicted reward: tensor([[1.4365]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.9497]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5425.0
Loss: 242538.5
KL Divergence: 17.900461196899414
4399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.3291]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-56.1418]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6230.0
Loss: 0.054830051958560944
Action 0 - predicted reward: tensor([[0.1802]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1802]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 11305.0
Loss: 0.5265280604362488
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0669]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9942]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6060.0
Loss: 0.050489865243434906
Action 0 - predicted reward: tensor([[0.0507]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-11.2969]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4800.0
Loss: 0.04603751376271248
Greedy
Action 0 - predicted reward: tensor([[0.0969]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1292]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6740.0
Loss: 0.015169735066592693
Action 0 - predicted reward: tensor([[-0.0894]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8395]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5195.0
Loss: 0.02929169125854969
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.2648]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3109]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7500.0
Loss: 322504.375
KL Divergence: 18.41392707824707
Action 0 - predicted reward: tensor([[1.6004]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6536]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5495.0
Loss: 219644.8125
KL Divergence: 17.94973373413086
4499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0305]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2729]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6305.0
Loss: 0.05390363559126854
Action 0 - predicted reward: tensor([[0.1791]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1791]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 11525.0
Loss: 0.5263731479644775
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2355]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2644]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6105.0
Loss: 0.04811333492398262
Action 0 - predicted reward: tensor([[0.0040]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9735]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4880.0
Loss: 0.044378187507390976
Greedy
Action 0 - predicted reward: tensor([[-0.6183]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6614]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6745.0
Loss: 0.012903798371553421
Action 0 - predicted reward: tensor([[-0.0402]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0845]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5235.0
Loss: 0.03134758770465851
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4684]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3480]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7610.0
Loss: 296148.75
KL Divergence: 18.462995529174805
Action 0 - predicted reward: tensor([[1.7528]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8243]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5530.0
Loss: 207290.3125
KL Divergence: 17.986303329467773
4599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2699]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9588]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6450.0
Loss: 0.051337502896785736
Action 0 - predicted reward: tensor([[0.1693]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1693]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 11760.0
Loss: 0.5203073620796204
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0551]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6805]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6150.0
Loss: 0.04330233484506607
Action 0 - predicted reward: tensor([[0.0856]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.1005]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4950.0
Loss: 0.04130895435810089
Greedy
Action 0 - predicted reward: tensor([[0.1175]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0924]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6795.0
Loss: 0.012123974040150642
Action 0 - predicted reward: tensor([[-0.0130]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1935]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5375.0
Loss: 0.03834092617034912
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4522]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5010]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7715.0
Loss: 277829.59375
KL Divergence: 18.518096923828125
Action 0 - predicted reward: tensor([[1.7064]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7851]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5600.0
Loss: 192613.71875
KL Divergence: 18.012521743774414
4699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0865]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.3787]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6490.0
Loss: 0.052383001893758774
Action 0 - predicted reward: tensor([[0.1938]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1938]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12060.0
Loss: 0.4794411361217499
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1414]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0740]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6200.0
Loss: 0.03691611811518669
Action 0 - predicted reward: tensor([[-0.0593]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9594]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 4990.0
Loss: 0.04160943254828453
Greedy
Action 0 - predicted reward: tensor([[0.1024]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1082]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6875.0
Loss: 0.01622602343559265
Action 0 - predicted reward: tensor([[-0.1640]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-47.1003]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5375.0
Loss: 0.0361301563680172
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7656]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8307]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7785.0
Loss: 269802.5
KL Divergence: 18.573162078857422
Action 0 - predicted reward: tensor([[1.9096]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3487]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5635.0
Loss: 177716.859375
KL Divergence: 18.026073455810547
4799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.4482]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-83.1158]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6740.0
Loss: 0.05960800498723984
Action 0 - predicted reward: tensor([[0.1822]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1822]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12310.0
Loss: 0.45185336470603943
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0191]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.4041]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6320.0
Loss: 0.038983892649412155
Action 0 - predicted reward: tensor([[-0.1013]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.4857]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 4995.0
Loss: 0.03695400059223175
Greedy
Action 0 - predicted reward: tensor([[0.5838]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.1750]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6910.0
Loss: 0.015021905303001404
Action 0 - predicted reward: tensor([[-0.1446]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0599]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5410.0
Loss: 0.03525248169898987
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4063]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0266]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7890.0
Loss: 266181.1875
KL Divergence: 18.604049682617188
Action 0 - predicted reward: tensor([[2.0821]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7861]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5635.0
Loss: 156322.515625
KL Divergence: 18.046588897705078
4899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1316]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-2.7695]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6775.0
Loss: 0.050797294825315475
Action 0 - predicted reward: tensor([[0.1796]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1796]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 12580.0
Loss: 0.4141753017902374
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.1750]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.4450]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6465.0
Loss: 0.05207504332065582
Action 0 - predicted reward: tensor([[-0.0496]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1517]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5140.0
Loss: 0.04685964807868004
Greedy
Action 0 - predicted reward: tensor([[-0.0091]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9498]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6945.0
Loss: 0.012140611186623573
Action 0 - predicted reward: tensor([[-0.0493]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.5721]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5445.0
Loss: 0.03935863822698593
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.4579]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5288]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8030.0
Loss: 270748.84375
KL Divergence: 18.66681480407715
Action 0 - predicted reward: tensor([[2.2109]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2715]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5670.0
Loss: 158646.921875
KL Divergence: 18.073923110961914
4999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0339]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-27.9710]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6880.0
Loss: 0.05332241952419281
Action 0 - predicted reward: tensor([[0.1555]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.1555]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 12845.0
Loss: 0.3909381628036499
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0527]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-46.7221]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6540.0
Loss: 0.04325340688228607
Action 0 - predicted reward: tensor([[0.0576]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-16.2646]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5150.0
Loss: 0.037178054451942444
Greedy
Action 0 - predicted reward: tensor([[-0.0617]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.5065]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6955.0
Loss: 0.01081003900617361
Action 0 - predicted reward: tensor([[0.0481]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0775]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5515.0
Loss: 0.042197756469249725
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.3254]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[0.6883]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8100.0
Loss: 267403.34375
KL Divergence: 18.669801712036133
Action 0 - predicted reward: tensor([[2.1933]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2527]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5675.0
Loss: 154437.71875
KL Divergence: 18.07094955444336
5099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2151]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0000]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6920.0
Loss: 0.04887104779481888
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 13150.0
Loss: 0.3816086947917938
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.3531]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2192]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6660.0
Loss: 0.045021284371614456
Action 0 - predicted reward: tensor([[0.1345]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0374]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5155.0
Loss: 0.033356908708810806
Greedy
Action 0 - predicted reward: tensor([[0.0021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.4546]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6990.0
Loss: 0.011604980565607548
Action 0 - predicted reward: tensor([[0.0261]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0634]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5515.0
Loss: 0.0367627888917923
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.5690]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6202]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8105.0
Loss: 267536.1875
KL Divergence: 18.735836029052734
Action 0 - predicted reward: tensor([[2.2642]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2531]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5675.0
Loss: 150116.21875
KL Divergence: 18.07378387451172
5199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0121]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9073]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7030.0
Loss: 0.053344838321208954
Action 0 - predicted reward: tensor([[0.0759]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0759]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 13390.0
Loss: 0.3580378293991089
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1295]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-51.5971]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6710.0
Loss: 0.03923317790031433
Action 0 - predicted reward: tensor([[0.4479]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9959]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5205.0
Loss: 0.029639430344104767
Greedy
Action 0 - predicted reward: tensor([[0.0178]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-19.7989]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7035.0
Loss: 0.012306672520935535
Action 0 - predicted reward: tensor([[0.0235]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-36.7359]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5550.0
Loss: 0.03908776491880417
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.5873]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6472]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8140.0
Loss: 270190.5
KL Divergence: 18.7722110748291
Action 0 - predicted reward: tensor([[2.2386]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2442]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 5710.0
Loss: 150209.796875
KL Divergence: 18.077688217163086
5299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0251]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-61.7852]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7105.0
Loss: 0.05110021308064461
Action 0 - predicted reward: tensor([[0.0599]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0599]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 13680.0
Loss: 0.3299185633659363
Epsilon Greedy 1%
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 6865.0
Loss: 0.03767715021967888
Action 0 - predicted reward: tensor([[0.1366]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-7.2360]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5255.0
Loss: 0.028396891430020332
Greedy
Action 0 - predicted reward: tensor([[-0.1128]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.7446]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7120.0
Loss: 0.016243595629930496
Action 0 - predicted reward: tensor([[-0.0436]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.0880]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5620.0
Loss: 0.046876244246959686
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7311]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7736]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8180.0
Loss: 272555.09375
KL Divergence: 18.79837417602539
Action 0 - predicted reward: tensor([[1.9340]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7444]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5745.0
Loss: 152504.765625
KL Divergence: 18.065410614013672
5399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0370]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.9319]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7365.0
Loss: 0.06564537435770035
Action 0 - predicted reward: tensor([[0.0704]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.0704]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 13930.0
Loss: 0.27335694432258606
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0156]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1380]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6935.0
Loss: 0.03366228938102722
Action 0 - predicted reward: tensor([[0.3330]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9585]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5335.0
Loss: 0.025739476084709167
Greedy
Action 0 - predicted reward: tensor([[-0.1869]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9848]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7200.0
Loss: 0.021157318726181984
Action 0 - predicted reward: tensor([[0.0649]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9616]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5725.0
Loss: 0.04438672959804535
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7029]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5160]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8230.0
Loss: 274751.25
KL Divergence: 18.81346893310547
Action 0 - predicted reward: tensor([[2.2159]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6954]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5850.0
Loss: 158957.71875
KL Divergence: 18.069679260253906
5499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2153]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.7431]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7460.0
Loss: 0.06274594366550446
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 14240.0
Loss: 0.264022558927536
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2386]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0341]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6985.0
Loss: 0.03245903179049492
Action 0 - predicted reward: tensor([[-0.5204]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9864]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5380.0
Loss: 0.026209821924567223
Greedy
Action 0 - predicted reward: tensor([[-0.3197]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.5203]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7240.0
Loss: 0.019719185307621956
Action 0 - predicted reward: tensor([[0.3951]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.1555]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of -35.0.
Cumulative regret is 5830.0
Loss: 0.04375911131501198
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.6049]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6597]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8305.0
Loss: 278995.9375
KL Divergence: 18.829177856445312
Action 0 - predicted reward: tensor([[2.1960]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4189]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5920.0
Loss: 161196.078125
KL Divergence: 18.04732894897461
5599.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.1495]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6992]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7540.0
Loss: 0.05582522973418236
Action 0 - predicted reward: tensor([[-0.0194]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0194]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14475.0
Loss: 0.25448137521743774
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.1693]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[0.2396]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7055.0
Loss: 0.03769013658165932
Action 0 - predicted reward: tensor([[-0.0547]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-42.6754]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5400.0
Loss: 0.02579825557768345
Greedy
Action 0 - predicted reward: tensor([[-0.0589]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-30.1202]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7250.0
Loss: 0.01494461391121149
Action 0 - predicted reward: tensor([[0.0138]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.0811]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5900.0
Loss: 0.04069250822067261
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7574]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.0772]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8340.0
Loss: 274618.8125
KL Divergence: 18.856307983398438
Action 0 - predicted reward: tensor([[2.2782]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3502]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6025.0
Loss: 167830.859375
KL Divergence: 18.076641082763672
5699.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.0333]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.5287]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7695.0
Loss: 0.05260240659117699
Action 0 - predicted reward: tensor([[-0.0766]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0766]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 14735.0
Loss: 0.235454261302948
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0047]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-12.5671]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7095.0
Loss: 0.030050786212086678
Action 0 - predicted reward: tensor([[0.2575]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1442]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5455.0
Loss: 0.021643836051225662
Greedy
Action 0 - predicted reward: tensor([[0.0597]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-57.3649]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7295.0
Loss: 0.01398348156362772
Action 0 - predicted reward: tensor([[0.1763]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0701]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5970.0
Loss: 0.03877020254731178
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.6238]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6884]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8340.0
Loss: 272675.21875
KL Divergence: 18.89706802368164
Action 0 - predicted reward: tensor([[2.1887]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.9076]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6165.0
Loss: 174343.078125
KL Divergence: 18.075353622436523
5799.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.7016]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9210]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7735.0
Loss: 0.046051833778619766
Action 0 - predicted reward: tensor([[-0.0472]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0472]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 14990.0
Loss: 0.18824896216392517
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.2289]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-33.5109]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7145.0
Loss: 0.02502405270934105
Action 0 - predicted reward: tensor([[0.0118]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9903]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5515.0
Loss: 0.018012428656220436
Greedy
Action 0 - predicted reward: tensor([[0.1441]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8218]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7340.0
Loss: 0.017777658998966217
Action 0 - predicted reward: tensor([[-0.1110]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.7275]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6080.0
Loss: 0.03759410232305527
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.5103]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.1481]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8410.0
Loss: 276810.125
KL Divergence: 18.913311004638672
Action 0 - predicted reward: tensor([[2.1576]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2319]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6200.0
Loss: 174274.9375
KL Divergence: 18.08187484741211
5899.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2145]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.7402]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7815.0
Loss: 0.05162718892097473
Action 0 - predicted reward: tensor([[-0.0769]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0769]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 15245.0
Loss: 0.15960055589675903
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0571]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6815]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7250.0
Loss: 0.034046657383441925
Action 0 - predicted reward: tensor([[0.1253]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8628]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5520.0
Loss: 0.01756966859102249
Greedy
Action 0 - predicted reward: tensor([[0.0575]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-34.2301]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7380.0
Loss: 0.01767031103372574
Action 0 - predicted reward: tensor([[-0.0444]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-29.6363]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6115.0
Loss: 0.04032507166266441
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.6227]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.6711]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8445.0
Loss: 274829.1875
KL Divergence: 18.949081420898438
Action 0 - predicted reward: tensor([[2.3011]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3759]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6235.0
Loss: 176485.015625
KL Divergence: 18.105337142944336
5999.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.2071]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-18.8254]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7940.0
Loss: 0.04097282141447067
Action 0 - predicted reward: tensor([[-0.1310]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.1310]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 15635.0
Loss: 0.14945943653583527
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0500]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.4214]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7360.0
Loss: 0.03153811767697334
Action 0 - predicted reward: tensor([[0.1754]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0409]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5525.0
Loss: 0.017252454534173012
Greedy
Action 0 - predicted reward: tensor([[0.3284]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0032]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7420.0
Loss: 0.019107511267066002
Action 0 - predicted reward: tensor([[0.0774]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-40.7889]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6115.0
Loss: 0.03597420081496239
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7724]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.8216]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8445.0
Loss: 266268.21875
KL Divergence: 18.974153518676758
Action 0 - predicted reward: tensor([[2.3965]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.5612]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6235.0
Loss: 176580.125
KL Divergence: 18.112947463989258
6099.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0791]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.1328]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8060.0
Loss: 0.04159543290734291
Action 0 - predicted reward: tensor([[-0.0954]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0954]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 15900.0
Loss: 0.10984727740287781
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0942]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0223]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7435.0
Loss: 0.035010986030101776
Action 0 - predicted reward: tensor([[0.1470]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-6.9183]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5575.0
Loss: 0.01723354123532772
Greedy
Action 0 - predicted reward: tensor([[-0.0372]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-4.7864]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7470.0
Loss: 0.023694003000855446
Action 0 - predicted reward: tensor([[0.0740]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[3.9805]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6150.0
Loss: 0.03583133965730667
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.0033]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.4770]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8550.0
Loss: 266276.875
KL Divergence: 19.00174331665039
Action 0 - predicted reward: tensor([[2.2124]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7719]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6235.0
Loss: 176404.609375
KL Divergence: 18.098358154296875
6199.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1850]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-13.6585]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8170.0
Loss: 0.048113852739334106
Action 0 - predicted reward: tensor([[-0.0937]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0937]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16155.0
Loss: 0.11004263162612915
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0805]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-10.2821]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7480.0
Loss: 0.03229594975709915
Action 0 - predicted reward: tensor([[0.1297]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9830]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5615.0
Loss: 0.021488701924681664
Greedy
Action 0 - predicted reward: tensor([[0.1042]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-23.2594]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7520.0
Loss: 0.021310249343514442
Action 0 - predicted reward: tensor([[0.0929]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-17.1631]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6290.0
Loss: 0.03480156883597374
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.9669]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0237]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8550.0
Loss: 253465.296875
KL Divergence: 19.03245735168457
Action 0 - predicted reward: tensor([[2.3173]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.3753]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6270.0
Loss: 176443.234375
KL Divergence: 18.090932846069336
6299.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.1482]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-24.3813]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8210.0
Loss: 0.04402414709329605
Action 0 - predicted reward: tensor([[-0.0945]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0945]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 16415.0
Loss: 0.11001723259687424
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0021]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-28.0387]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7520.0
Loss: 0.02647753246128559
Action 0 - predicted reward: tensor([[-0.0373]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-39.3030]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 5685.0
Loss: 0.01815052703022957
Greedy
Action 0 - predicted reward: tensor([[0.0059]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-45.4248]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7560.0
Loss: 0.01696600392460823
Action 0 - predicted reward: tensor([[-0.0282]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-48.2757]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6325.0
Loss: 0.037109676748514175
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7537]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.3405]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8585.0
Loss: 251473.359375
KL Divergence: 19.04610824584961
Action 0 - predicted reward: tensor([[2.1938]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.7221]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6340.0
Loss: 174355.765625
KL Divergence: 18.098766326904297
6399.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[-0.0471]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-44.4411]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8405.0
Loss: 0.05003679171204567
Action 0 - predicted reward: tensor([[-0.0934]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0934]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 16715.0
Loss: 0.11014409363269806
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[-0.0088]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.2555]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7630.0
Loss: 0.03756042569875717
Action 0 - predicted reward: tensor([[0.2046]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.8739]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5720.0
Loss: 0.01807839423418045
Greedy
Action 0 - predicted reward: tensor([[-0.0114]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-53.1118]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 7600.0
Loss: 0.017184173688292503
Action 0 - predicted reward: tensor([[0.1184]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[5.0327]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6360.0
Loss: 0.029081767424941063
Bayes by Backprop
Action 0 - predicted reward: tensor([[2.0128]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.0847]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 8655.0
Loss: 251394.671875
KL Divergence: 19.077590942382812
Action 0 - predicted reward: tensor([[2.1831]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2493]], grad_fn=<DivBackward0>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6375.0
Loss: 172209.859375
KL Divergence: 18.090164184570312
6499.
Epsilon Greedy 5%
Action 0 - predicted reward: tensor([[0.2802]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-1.5134]], grad_fn=<AddmmBackward>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8475.0
Loss: 0.05068599805235863
Action 0 - predicted reward: tensor([[-0.0915]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[-0.0915]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose pass and got a reward of 0.
Cumulative regret is 16965.0
Loss: 0.11023879796266556
Epsilon Greedy 1%
Action 0 - predicted reward: tensor([[0.0834]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9855]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7645.0
Loss: 0.03426225110888481
Action 0 - predicted reward: tensor([[0.0336]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9480]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 5790.0
Loss: 0.017528589814901352
Greedy
Action 0 - predicted reward: tensor([[0.9055]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.9223]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 7650.0
Loss: 0.017986273393034935
Action 0 - predicted reward: tensor([[0.3598]], grad_fn=<AddmmBackward>)
Action 1 - predicted reward: tensor([[4.6724]], grad_fn=<AddmmBackward>)
The mushroom was edible. The agent chose eat and got a reward of 5.0.
Cumulative regret is 6360.0
Loss: 0.024880221113562584
Bayes by Backprop
Action 0 - predicted reward: tensor([[1.7210]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[1.2944]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 8690.0
Loss: 240662.484375
KL Divergence: 19.064271926879883
Action 0 - predicted reward: tensor([[2.2525]], grad_fn=<DivBackward0>)
Action 1 - predicted reward: tensor([[2.2448]], grad_fn=<DivBackward0>)
The mushroom was poisonous. The agent chose pass and got a reward of 0.
Cumulative regret is 6445.0
Loss: 174226.046875
KL Divergence: 18.12552261352539
