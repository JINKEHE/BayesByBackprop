{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bandits.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "BvFtMlt2C-i3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import queue\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.utils.data\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch import distributions as dist\n",
        "\n",
        "from core import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-z4HOgOaLdHH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 117 + 1\n",
        "SAMPLE_COUNT = 2\n",
        "AGENT_MEMORY_LEN = 4096\n",
        "\n",
        "EDIBLE_REWARD = 5.0\n",
        "POISONOUS_REWARD = -35.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QwqcuVGaFbJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mushroom_dataset = pd.read_csv('mushrooms.csv')\n",
        "train_labels = mushroom_dataset['class']\n",
        "train_labels = train_labels.replace(['p', 'e'],\n",
        "                                    [POISONOUS_REWARD, EDIBLE_REWARD])\n",
        "# the features contain missing values (marked as ?)\n",
        "# these are treated as a different class atm\n",
        "train_features = pd.get_dummies(mushroom_dataset.drop(['class'], axis=1))\n",
        "train_features = torch.tensor(train_features.values, dtype=torch.float)\n",
        "train_labels = torch.tensor(train_labels.values)\n",
        "trainset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
        "                                          shuffle=True, num_workers=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c8FQFeRhJ6Ys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.value_estimates = BayesianNN(\n",
        "        CONTEXT_SIZE, [100, 100, 1],\n",
        "        [ActivationType.RELU, ActivationType.RELU, ActivationType.NONE])\n",
        "    self.optimizer = optim.Adam(self.value_estimates.parameters(), lr=0.01)\n",
        "  \n",
        "    self.past_plays_context = queue.Queue(maxsize=AGENT_MEMORY_LEN)\n",
        "    self.past_plays_action = queue.Queue(AGENT_MEMORY_LEN)\n",
        "    self.past_plays_reward = queue.Queue(AGENT_MEMORY_LEN)\n",
        "  \n",
        "  def collected_data_count(self):\n",
        "    return self.past_plays_context.qsize()\n",
        "  \n",
        "  def select_action(self, context, logs=False):\n",
        "    self.value_estimates.train()\n",
        "    max_reward = POISONOUS_REWARD - 1\n",
        "    argmax_action = -1\n",
        "    for action in range(2):\n",
        "      expected_reward = 0\n",
        "      for i in range(SAMPLE_COUNT):\n",
        "        context_and_action = torch.cat(\n",
        "            [context, torch.tensor([action], dtype=torch.float)])\n",
        "        expected_reward += self.value_estimates(context_and_action)\n",
        "      expected_reward /= SAMPLE_COUNT\n",
        "      if logs:\n",
        "        print('Action {} - predicted reward: {}'.format(\n",
        "            action, expected_reward))\n",
        "      if expected_reward > max_reward:\n",
        "        max_reward = expected_reward\n",
        "        argmax_action = action\n",
        "    return argmax_action\n",
        "  \n",
        "  def update_memory(self, context, action, reward):\n",
        "    self.past_plays_context.put(context)\n",
        "    self.past_plays_action.put(action)\n",
        "    self.past_plays_reward.put(reward)\n",
        "    if self.past_plays_context.full():\n",
        "      self.past_plays_context.get()\n",
        "      self.past_plays_action.get()\n",
        "      self.past_plays_reward.get()\n",
        "    \n",
        "  \n",
        "  def update_variational_posterior(self, context, action, reward, logs=False):\n",
        "    features = []\n",
        "    for context, action in zip(self.past_plays_context, self.past_plays_action):\n",
        "      features.append(torch.cat(\n",
        "          [context, torch.tensor([action], dtype=torch.float)]).unsqueeze(0))\n",
        "    features = torch.cat(features)\n",
        "    \n",
        "    rewards = torch.tensor(self.past_plays_reward, dtype=torch.float)\n",
        "    \n",
        "    past_plays_set = torch.utils.data.TensorDataset(features, rewards)\n",
        "    past_plays_loader = torch.utils.data.DataLoader(\n",
        "        past_plays_set, batch_size=64, shuffle=True, num_workers=1)\n",
        "    \n",
        "    for i, data in enumerate(past_plays_loader):\n",
        "      inputs, labels = data\n",
        "      # zero the parameter gradients\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      loss, _, _ = self.value_estimates.cost_function(\n",
        "          inputs, labels, num_samples=2, num_batches=len(trainloader))\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      \n",
        "      if logs:\n",
        "        print('{}. Loss: {}'.format(i, loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JX5fKGIsMUZP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Environment(object):\n",
        "  \n",
        "  def __init__(self, agent, dataloader):\n",
        "    self.agent = agent\n",
        "    self.dataloader = dataloader\n",
        "    self.cumulative_regret = 0\n",
        "  \n",
        "  def play_round(self, logs=False):\n",
        "    context, eat_reward = next(iter(self.dataloader))\n",
        "    selected_action = agent.select_action(context[0], logs)\n",
        "    if selected_action == 0: #not eat\n",
        "      if eat_reward == EDIBLE_REWARD:\n",
        "        if logs:\n",
        "          print('Mushroom is edible; agent chose to pass.')\n",
        "        self.cumulative_regret += abs(EDIBLE_REWARD)\n",
        "      else:\n",
        "        if logs:\n",
        "          print('Mushroom is poisonous; agent chose to pass.')\n",
        "      reward = 0\n",
        "    else: #eat\n",
        "      if eat_reward == POISONOUS_REWARD:\n",
        "        self.cumulative_regret += abs(POISONOUS_REWARD)\n",
        "        if logs:\n",
        "          print('Mushroom is poisonous; agent chose to eat.')\n",
        "      else:\n",
        "        if logs:\n",
        "          print('Mushroom is edible; agent chose to eat.')\n",
        "      reward = eat_reward\n",
        "    if logs:\n",
        "      print('Cumulative regret is {}'.format(self.cumulative_regret))\n",
        "    agent.update_memory(context[0], selected_action, reward)\n",
        "    if agent.collected_data_count() >= AGENT_MEMORY_LEN:\n",
        "      agent.update_variational_posterior(context[0], selected_action, reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QRXWxtmYYnHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent = Agent()\n",
        "env = Environment(agent, trainloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pgv2F8Loa7_s",
        "colab_type": "code",
        "outputId": "c1ccbb3c-787c-4a53-9c53-422eddce967f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1046
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(100000):\n",
        "  if i == AGENT_MEMORY_LEN:\n",
        "    print('Started training')\n",
        "  if (i+1) % 100 == 0:\n",
        "    print(i)\n",
        "    env.play_round(logs=True)\n",
        "  else:\n",
        "    env.play_round()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99\n",
            "Action 0 - predicted reward: tensor([16.1083], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([23.0726], grad_fn=<DivBackward0>)\n",
            "Mushroom is edible; agent chose to eat.\n",
            "Cumulative regret is 1195.0\n",
            "199\n",
            "Action 0 - predicted reward: tensor([22.5331], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([18.3910], grad_fn=<DivBackward0>)\n",
            "Mushroom is edible; agent chose to pass.\n",
            "Cumulative regret is 2265.0\n",
            "299\n",
            "Action 0 - predicted reward: tensor([20.4924], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([21.5987], grad_fn=<DivBackward0>)\n",
            "Mushroom is poisonous; agent chose to eat.\n",
            "Cumulative regret is 3165.0\n",
            "399\n",
            "Action 0 - predicted reward: tensor([14.2115], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([19.1123], grad_fn=<DivBackward0>)\n",
            "Mushroom is poisonous; agent chose to eat.\n",
            "Cumulative regret is 4480.0\n",
            "499\n",
            "Action 0 - predicted reward: tensor([15.9786], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([20.6412], grad_fn=<DivBackward0>)\n",
            "Mushroom is poisonous; agent chose to eat.\n",
            "Cumulative regret is 5815.0\n",
            "599\n",
            "Action 0 - predicted reward: tensor([15.0560], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([15.5593], grad_fn=<DivBackward0>)\n",
            "Mushroom is edible; agent chose to eat.\n",
            "Cumulative regret is 7125.0\n",
            "699\n",
            "Action 0 - predicted reward: tensor([14.5937], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([23.4613], grad_fn=<DivBackward0>)\n",
            "Mushroom is edible; agent chose to eat.\n",
            "Cumulative regret is 8040.0\n",
            "799\n",
            "Action 0 - predicted reward: tensor([24.6856], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([26.3248], grad_fn=<DivBackward0>)\n",
            "Mushroom is poisonous; agent chose to eat.\n",
            "Cumulative regret is 9200.0\n",
            "899\n",
            "Action 0 - predicted reward: tensor([13.9724], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([16.7090], grad_fn=<DivBackward0>)\n",
            "Mushroom is edible; agent chose to eat.\n",
            "Cumulative regret is 10580.0\n",
            "999\n",
            "Action 0 - predicted reward: tensor([19.8187], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([23.4598], grad_fn=<DivBackward0>)\n",
            "Mushroom is edible; agent chose to eat.\n",
            "Cumulative regret is 11535.0\n",
            "1099\n",
            "Action 0 - predicted reward: tensor([8.8157], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([13.7818], grad_fn=<DivBackward0>)\n",
            "Mushroom is edible; agent chose to eat.\n",
            "Cumulative regret is 12505.0\n",
            "1199\n",
            "Action 0 - predicted reward: tensor([15.2785], grad_fn=<DivBackward0>)\n",
            "Action 1 - predicted reward: tensor([17.9445], grad_fn=<DivBackward0>)\n",
            "Mushroom is poisonous; agent chose to eat.\n",
            "Cumulative regret is 13690.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OWlmEVs5uzXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}